{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajZQZPTAL35r"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 12.123\n",
    "#    Data feature augmentation\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH8Ro002LsPZ"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 1960,
     "status": "ok",
     "timestamp": 1754146051730,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "dcPcE495rMxg",
    "outputId": "dd65db78-1115-4758-ccb8-07320fa98e52"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "n_clust = 300\n",
    "blur    = 1\n",
    "\n",
    "A = [ 1,1 ]\n",
    "B = [ 5,1 ]\n",
    "C = [ 4,3 ]\n",
    "\n",
    "a = [ A[0]+np.random.randn(n_clust)*blur, A[1]+np.random.randn(n_clust)*blur ]\n",
    "b = [ B[0]+np.random.randn(n_clust)*blur, B[1]+np.random.randn(n_clust)*blur ]\n",
    "c = [ C[0]+np.random.randn(n_clust)*blur, C[1]+np.random.randn(n_clust)*blur ]\n",
    "\n",
    "# True labels\n",
    "labels_np = np.hstack(( np.zeros((n_clust)),\n",
    "                        np.ones( (n_clust)),\n",
    "                        np.ones( (n_clust))+1 ))\n",
    "\n",
    "# Concatanate into a matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data   = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).long()\n",
    "\n",
    "# Plotting (with distance from origin)\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig = plt.figure(figsize=(phi*6,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(0.2,0.9,len(np.unique(labels))))\n",
    "\n",
    "for i in range(len(data)):\n",
    "  plt.plot([0,data[i,0]],[0,data[i,1]],color=cmaps[labels[i]],alpha=.2)\n",
    "\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'s',color=cmaps[0],alpha=.5)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'o',color=cmaps[1],alpha=.5)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'^',color=cmaps[2],alpha=.5)\n",
    "\n",
    "plt.grid(color=[.9,.9,.9])\n",
    "plt.title('Some data')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "plt.savefig('figure25_data_feature_augmentation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure25_data_feature_augmentation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULwXj0-gvuHV"
   },
   "outputs": [],
   "source": [
    "# %% A note\n",
    "\n",
    "# The native data only have 2 dimensions (x,y), here we are going to use the\n",
    "# distance from origin as a new feature and see if it helps the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1754146063266,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "hZrHK3D-wToW",
    "outputId": "d27d86ae-8436-4926-d577-5b2fb757cc3c"
   },
   "outputs": [],
   "source": [
    "# %% Compute Euclidian distance of datapoints from origin\n",
    "\n",
    "# Distance\n",
    "dist2origin = torch.sqrt( data[:,0]**2 + data[:,1]**2 )\n",
    "\n",
    "# Plotting\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig = plt.figure(figsize=(phi*6,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(0.2,0.9,len(np.unique(labels))))\n",
    "\n",
    "for i,lab in enumerate(np.unique(labels)):\n",
    "    idx = labels==lab\n",
    "    plt.plot(labels[labels==lab]+torch.randn(n_clust)/10,dist2origin[idx],'o',color=cmaps[i])\n",
    "\n",
    "plt.xticks([0,1,2],labels=['Cluster 1','Cluster 2','Cluster 3'])\n",
    "plt.ylabel('Euclidean distance (a.u.)')\n",
    "plt.title('Distance from origin')\n",
    "\n",
    "plt.savefig('figure26_data_feature_augmentation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure26_data_feature_augmentation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1754146076646,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "GOYR8SZjwT79",
    "outputId": "4a2cb69a-7908-498d-d5dc-ed38b8a17ed5"
   },
   "outputs": [],
   "source": [
    "# %% Add new feature to data set\n",
    "\n",
    "data_aug = torch.cat( (data,dist2origin.view(len(data),1)),axis=1 )\n",
    "\n",
    "print(data.shape)\n",
    "print(data_aug.shape)\n",
    "print()\n",
    "\n",
    "print(data)\n",
    "print(data_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "repaky7cwUBW"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Split data with scikitlearn\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(data_aug,labels,train_size=0.9)\n",
    "\n",
    "# Convert to PyTorch Datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert to dataloader object\n",
    "batch_size   = 15\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last= True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSW7Nze23z5v"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "def gen_model(use_extra_feature=False):\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Input layer (flexible use of extra feature)\n",
    "            if use_extra_feature:\n",
    "                self.input = nn.Linear(3,8)\n",
    "            else:\n",
    "                self.input = nn.Linear(2,8)\n",
    "\n",
    "            # Hidden layer\n",
    "            self.hid = nn.Linear(8,8)\n",
    "\n",
    "            # Output layer\n",
    "            self.output = nn.Linear(8,3)\n",
    "\n",
    "        def forward(self,x):\n",
    "\n",
    "            #print(x.shape) # comment out during training\n",
    "            if not use_extra_feature:\n",
    "                x = x[:,:2]\n",
    "            #print(x.shape) # comment out during training\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Model instance, loss function, and optimiser\n",
    "    ANN       = model()\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.001)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1754146084529,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "u5nQLT5r3zxq",
    "outputId": "d80466cb-a5aa-4522-b197-e9d1e2776091"
   },
   "outputs": [],
   "source": [
    "# %% Test model\n",
    "\n",
    "print('Using augmented data set :')\n",
    "ANN = gen_model(use_extra_feature=True)[0]\n",
    "ANN(next(iter(train_loader))[0]);\n",
    "\n",
    "print('\\nNot using augmented data set :')\n",
    "ANN = gen_model(use_extra_feature=False)[0]\n",
    "ANN(next(iter(train_loader))[0]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rARiwf1T3zvG"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model(use_extra_feature=False):\n",
    "\n",
    "    # Number of epochs and model instance\n",
    "    num_epochs = 200\n",
    "    ANN,loss_function,optimizer = gen_model(use_extra_feature)\n",
    "\n",
    "    # Preallocate variables\n",
    "    losses    = torch.zeros(num_epochs)\n",
    "    train_acc = torch.zeros(num_epochs)\n",
    "    test_acc  = torch.zeros(num_epochs)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Batches loop\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward prop and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_function(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute loss and accuracy for this batch\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_acc.append(100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item())\n",
    "\n",
    "        # Compute loss and accuracy for the epoch\n",
    "        losses[epoch_i]    = np.mean(batch_loss)\n",
    "        train_acc[epoch_i] = np.mean(batch_acc)\n",
    "\n",
    "        # Test accuracy (switch to evaluation mode and then back to training\n",
    "        # mode to save up computation)\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "\n",
    "        test_acc[epoch_i] = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item()\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "498YhGXm3zsc"
   },
   "outputs": [],
   "source": [
    "# %% Function to plot the results\n",
    "\n",
    "def plot_results():\n",
    "\n",
    "    # Accuracy over entire data set (train + test)\n",
    "    yHat  = ANN(data_aug)\n",
    "    preds = torch.argmax(yHat,axis=1)\n",
    "    acc   = (preds==labels).float()\n",
    "\n",
    "    # Accuracy by cluster\n",
    "    acc_by_clust = np.zeros(len(np.unique(labels)))\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        acc_by_clust[i] = 100*torch.mean(acc[labels==i])\n",
    "\n",
    "    # Plotting\n",
    "    phi = (1 + np.sqrt(5)) / 2\n",
    "    fig,ax = plt.subplots(2,2,figsize=(phi*6,6))\n",
    "\n",
    "    # plot the loss function\n",
    "    ax[0,0].plot(losses)\n",
    "    ax[0,0].set_ylabel('Loss')\n",
    "    ax[0,0].set_xlabel('epoch')\n",
    "    ax[0,0].set_title('Losses')\n",
    "\n",
    "    # plot the accuracy functions\n",
    "    ax[0,1].plot(train_acc,label='Train')\n",
    "    ax[0,1].plot(test_acc,label='Test')\n",
    "    ax[0,1].set_ylabel('Accuracy (%)')\n",
    "    ax[0,1].set_xlabel('Epoch')\n",
    "    ax[0,1].set_title(f'Accuracy ({test_acc[-1].item():.2f}%)')\n",
    "    ax[0,1].legend()\n",
    "\n",
    "    # plot overall accuracy by group\n",
    "    ax[1,0].bar(range(3),acc_by_clust)\n",
    "    ax[1,0].set_ylim([np.min(acc_by_clust)-5,np.max(acc_by_clust)+5])\n",
    "    ax[1,0].set_xticks([0,1,2])\n",
    "    ax[1,0].set_xlabel('Group')\n",
    "    ax[1,0].set_ylabel('Accuracy (%)')\n",
    "    ax[1,0].set_title('Accuracy by group')\n",
    "\n",
    "    # scatterplot of correct and incorrect labeled data\n",
    "    color_shapes = [ 's','o','^' ]\n",
    "    cmaps = plt.cm.plasma(np.linspace(0.2,0.9,len(np.unique(labels))))\n",
    "    for i in range(3):\n",
    "        # plot all data points\n",
    "        ax[1,1].plot(data_aug[labels==i,0],data_aug[labels==i,1],\n",
    "                     color_shapes[i],color=cmaps[i],alpha=.3,label=f'Clust. {i}')\n",
    "\n",
    "        # cross-out the incorrect ones\n",
    "        idxErr = (acc==0) & (labels==i)\n",
    "        ax[1,1].plot(data_aug[idxErr,0],data_aug[idxErr,1],'rx',alpha=0.6)\n",
    "\n",
    "    ax[1,1].set_title('All groups')\n",
    "    ax[1,1].set_xlabel('Dimension 1')\n",
    "    ax[1,1].set_ylabel('Dimension 2')\n",
    "    ax[1,1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('figure27_data_feature_augmentation.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    files.download('figure27_data_feature_augmentation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 40053,
     "status": "ok",
     "timestamp": 1754146380764,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "IwC03edk84Pi",
    "outputId": "4373d0c6-03e6-4960-c2db-7aed90516f7f"
   },
   "outputs": [],
   "source": [
    "# %% Test model without augmented data\n",
    "\n",
    "train_acc,test_acc,losses,ANN = train_model(False)\n",
    "print('Final accuracy: %.2f%%' %test_acc[-1].item())\n",
    "plot_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "executionInfo": {
     "elapsed": 38209,
     "status": "ok",
     "timestamp": 1754146454965,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "PaDS2dsZ84Lu",
    "outputId": "5cec7251-1670-4ce5-feeb-dd4439613003"
   },
   "outputs": [],
   "source": [
    "# %% Test model with augmented data\n",
    "\n",
    "train_acc,test_acc,losses,ANN = train_model(True)\n",
    "print('Final accuracy: %.2f%%' %test_acc[-1].item())\n",
    "plot_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1754143620084,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "vkpQ0cFcCIPU",
    "outputId": "5ffd8af4-62c4-4640-9ae1-1d034310ee20"
   },
   "outputs": [],
   "source": [
    "# %% Run parametric experiment\n",
    "\n",
    "# Run the model many times and carry out a t-test on the result distributions to\n",
    "# check whether the augmented data do make a difference; if no difference,\n",
    "# prefer the simpler model\n",
    "\n",
    "# Spoiler alert : the feature we added here is a linear mixture\n",
    "\n",
    "# Run\n",
    "repetitions       = 10\n",
    "acc_not_augmented = np.zeros(repetitions)\n",
    "acc_augmented     = np.zeros(repetitions)\n",
    "\n",
    "for i in range(repetitions):\n",
    "\n",
    "    acc_not_augmented[i] = train_model(False)[1][-1]\n",
    "    acc_augmented[i]     = train_model(True)[1][-1]\n",
    "\n",
    "print( np.round(np.vstack((acc_not_augmented,acc_augmented)).T,2) )\n",
    "\n",
    "# Stats\n",
    "t,p = stats.ttest_ind(acc_not_augmented,acc_augmented)\n",
    "print('\\nT-test for independent samples :')\n",
    "print(f't = {t:.2f}, p = {p:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSMdmxIjwXUX"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Add code to the \"distance-to-origin\" plot (top of the script) so the color and shape of the dots matches those\n",
    "#    used in the previous qwerties plot. Also, change the colors of the bars in the barplots to match the qwerties.\n",
    "\n",
    "# Ended up already implementing this part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRoPMEaPwXLl"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    If you increase the learning rate, or increase the number of epochs, or use Adam instead of SGD, you'll find that\n",
    "#    both datasets lead to equally good -- and high -- performance. Based on the graph of the data, do you think it's\n",
    "#    even possible to reach 100% accuracy? What does this tell you about ceiling effects in DL?\n",
    "\n",
    "# Even just switching to Adam improves the learning rate dramatically, so that\n",
    "# the performance reaches ceil very quickly (~85/87%); however, even adding more\n",
    "# epochs - in case there was some hidden local minimun - doesn't really improves\n",
    "# the performance, and by looking at the data plot (see also above), it seems\n",
    "# quite obvious that the model will never be able to classify the regions of\n",
    "# overlap with an 100% accuracy,  at least not only based on x and y coordinates\n",
    "# and Euclidian distance (and even if it happened, it would be a lucky chance\n",
    "# every now and then).\n",
    "# To prove this conceptual point, one can try to make the data more segregated\n",
    "# in themselves, for example by reducing he blur. Now even the basic model, with\n",
    "# augmented or non-augmented data, SGD and 200 iterations, performes nearly\n",
    "# perfectly (beside the same stubborn point).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM4D9FX+bDD4ZAgITEPZpya",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
