{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le5MsLuthBqv"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 8.68\n",
    "#    Cross validation on regression\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty5MFy8KhV0e"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1744018497084,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "WIXVtTckkooG",
    "outputId": "03f72342-ae81-409e-c8c6-fa6fb441a8d7"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "n = 100\n",
    "x = torch.randn(n,1)\n",
    "y = x + torch.randn(n,1)\n",
    "\n",
    "plt.plot(x,y,'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Some correlated data')\n",
    "\n",
    "plt.savefig('figure11_cross_validation_regression.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure11_cross_validation_regression.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odsap2UDlLJY"
   },
   "outputs": [],
   "source": [
    "# %% Model\n",
    "\n",
    "ANNreg = nn.Sequential(\n",
    "            nn.Linear(1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1,1)\n",
    "            )\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "learn_rate = 0.05\n",
    "optimizer  = torch.optim.SGD(ANNreg.parameters(),lr=learn_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1744018740461,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "tjNQ1x0VlxAe",
    "outputId": "e9ca128e-260c-4442-d147-387bcc332a03"
   },
   "outputs": [],
   "source": [
    "# %% Select data for training\n",
    "\n",
    "# Random indices, inizialise False vector, select samples to true\n",
    "train_prop = int(len(x)*.8)\n",
    "train_idx  = np.random.choice(range(n),train_prop,replace=False)\n",
    "train_bool = np.zeros(n,dtype=bool)\n",
    "train_bool[train_idx] = True\n",
    "\n",
    "# Show sizes\n",
    "print(f'Training data: {x[train_bool].shape}')\n",
    "print(f'Test data: {x[~train_bool].shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsBTH14QnGIG"
   },
   "outputs": [],
   "source": [
    "# %% Train model\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epochs_i in range(num_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    yHat = ANNreg(x[train_bool])\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fun(yHat,y[train_bool])\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1743970467179,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "gyW3lDgOoVy2",
    "outputId": "d90080fd-ea3f-4509-bd42-c88bec21d5ad"
   },
   "outputs": [],
   "source": [
    "# %% Report losses\n",
    "\n",
    "# Model pass on test data\n",
    "pred_test = ANNreg(x[~train_bool])\n",
    "test_loss = (pred_test-y[~train_bool]).pow(2).mean()\n",
    "\n",
    "print(f'Final train loss: {loss.detach():.2f}')\n",
    "print(f'Final test loss: {test_loss.detach():.2f}')\n",
    "\n",
    "pred_test = pred_test.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1743969285172,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "gLLK1jk_pQ0i",
    "outputId": "13e3285e-7d14-4e19-95d6-6cbe0b7f779e"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "# Final pass on train data\n",
    "pred_train = ANNreg(x[train_bool]).detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.plot(x,y,'o',label='All data')\n",
    "plt.plot(x[train_bool],pred_train,'s',label='Training predictions')\n",
    "plt.plot(x[~train_bool],pred_test,'^',label='Test predictions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Model performance on training and test data')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure12_cross_validation_regression.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure12_cross_validation_regression.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILFUNHKhsywl"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    The train/test split is currently hard-coded to be 80/20 (note the number \"80\"). This is bad coding style, because\n",
    "#    if you change the number of datapoints from N=100 to N=10000, then we're still only training on 80 samples and testing\n",
    "#    on 10000-80=9920 samples. Change how the variable trainBool is created so that it always trains on 80% of the data,\n",
    "#    regardless of the dataset size.\n",
    "\n",
    "# Easy-peasy fix:\n",
    "\n",
    "train_prop = int(len(x)*.8)\n",
    "train_idx  = np.random.choice(range(n),train_prop,replace=False)\n",
    "train_bool = np.zeros(n,dtype=bool)\n",
    "train_bool[train_idx] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1744018526245,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "DELbHGmbs1-7",
    "outputId": "b0b37cbd-13f9-4fc1-d0a6-d209b10c6c60"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Re-write this code to use scikitlearn and/or DataLoader instead of manually separating the data into train/test.\n",
    "\n",
    "partition = [.8,.2]\n",
    "train_x,test_x,train_y,test_y = train_test_split(x,y,train_size=partition[0])\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epochs_i in range(num_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    yHat = ANNreg(train_x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fun(yHat,train_y)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "pred_test = ANNreg(test_x)\n",
    "test_loss = (pred_test-test_y).pow(2).mean()\n",
    "print(f'Final train loss: {loss.detach():.2f}')\n",
    "print(f'Final test loss: {test_loss.detach():.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Fo5p48s2bU"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    Do we really need 500 epochs to train the model? To find out, add code to the training loop to compute the MSEloss\n",
    "#    for the train and test data on each iteration during training. Then plot the train and test error as a function of\n",
    "#    training epoch. What is your evaluation of an appropriate amount of training for this model/dataset?\n",
    "\n",
    "# Not 100% sure I got this right, I basically computed the model for the test set at each iteration, for\n",
    "# the purpose of exploring how well it fits those data, even though normally we would pass the test set\n",
    "# through the model only for a final pass. Now..  Assuming I got this right, 100-150 iterations is quite\n",
    "# enough to get to a stable loss pattern (Fig. 1), but it's also true that sometimes the test set loss\n",
    "# bounces back to higher values (Fig. 2). Is this a \"signature\" of overfitting in the training set ? And if\n",
    "# yes, what is the most likely cause? The stochasticity in the gradient descend algrithm, the random selection\n",
    "# of training and test samples? Intuitively I'd say at least a mixture of these two, but happy to hear more about it.\n",
    "\n",
    "ANNreg = nn.Sequential(\n",
    "            nn.Linear(1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1,1)\n",
    "            )\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "learn_rate = 0.05\n",
    "optimizer  = torch.optim.SGD(ANNreg.parameters(),lr=learn_rate)\n",
    "\n",
    "\n",
    "train_prop = int(len(x)*.8)\n",
    "train_idx  = np.random.choice(range(n),train_prop,replace=False)\n",
    "train_bool = np.zeros(n,dtype=bool)\n",
    "train_bool[train_idx] = True\n",
    "\n",
    "\n",
    "num_epochs   = 500\n",
    "losses_train = torch.zeros(num_epochs)\n",
    "losses_test  = torch.zeros(num_epochs)\n",
    "\n",
    "for epochs_i in range(num_epochs):\n",
    "\n",
    "    # Forward pass\n",
    "    yHat      = ANNreg(x[train_bool])\n",
    "    yHat_test = ANNreg(x[~train_bool])\n",
    "\n",
    "    # Compute loss\n",
    "    loss      = loss_fun(yHat,y[train_bool])\n",
    "    test_loss = loss_fun(yHat_test,y[~train_bool])\n",
    "\n",
    "    losses_train[epochs_i] = loss\n",
    "    losses_test[epochs_i]  = test_loss\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1744019768499,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "hV1zGa5Rw5Zz",
    "outputId": "7540938a-38af-4477-ba98-8df0cad3d053"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(losses_train.detach(),'o-',alpha=.75)\n",
    "plt.plot(losses_test.detach(),'o-',alpha=.75)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.legend(['Train losses','Test losses'])\n",
    "plt.title('Losses over epochs for training and test dataset')\n",
    "\n",
    "plt.savefig('figure13_cross_validation_dataloader_extra3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure13_cross_validation_dataloader_extra3.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1744019799342,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "UonRnSqFqGCW",
    "outputId": "d5d2eead-0215-4c31-a0e9-07cff4fcdea4"
   },
   "outputs": [],
   "source": [
    "# More plotting\n",
    "\n",
    "pred_test = ANNreg(x[~train_bool])\n",
    "test_loss = (pred_test-y[~train_bool]).pow(2).mean()\n",
    "\n",
    "print(f'Final train loss: {loss.detach():.2f}')\n",
    "print(f'Final test loss: {test_loss.detach():.2f}')\n",
    "\n",
    "pred_test  = pred_test.detach().numpy()\n",
    "pred_train = ANNreg(x[train_bool]).detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.plot(x,y,'o',label='All data')\n",
    "plt.plot(x[train_bool],pred_train,'s',label='Training predictions')\n",
    "plt.plot(x[~train_bool],pred_test,'^',label='Test predictions')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Model performance on training and test data')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure19_cross_validation_dataloader_extra3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure19_cross_validation_dataloader_extra3.png')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6xveRc8GWSq4t1T6YJr4A",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
