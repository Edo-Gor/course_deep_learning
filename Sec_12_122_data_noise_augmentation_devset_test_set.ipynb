{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajZQZPTAL35r"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 12.122\n",
    "#    Data noise augmentation (with dev set and test set)\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH8Ro002LsPZ"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhIGs4nRQNth"
   },
   "outputs": [],
   "source": [
    "# %% Function to get the data (train, dev, test)\n",
    "\n",
    "# Load data and normalise\n",
    "data_all       = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "data_all[:,1:] = data_all[:,1:] / np.max(data_all[:,1:])\n",
    "\n",
    "# Function\n",
    "def get_dataset(n,double_data=False):\n",
    "\n",
    "    # Remove labels (i.e., numbers IDs) from dataset, and select only n data\n",
    "    labels = data_all[:n,0]\n",
    "    data   = data_all[:n,1:]\n",
    "\n",
    "    # Make a noisy copy of all the data (uniform noise with range [0,0.5], half\n",
    "    # the data range, normalised before to [0,1], so by adding some noise we\n",
    "    # need to renormalise)\n",
    "    #if double_data==True:\n",
    "    #    data_noisy = data + np.random.random_sample(data.shape)/2\n",
    "    #    data       = np.concatenate((data,data_noisy),axis=0)\n",
    "    #    data       = data / np.max(data)\n",
    "    #    labels     = np.concatenate((labels,labels),axis=0)\n",
    "\n",
    "    # Covert to tensor\n",
    "    data_T   = torch.tensor(data).float()\n",
    "    labels_T = torch.tensor(labels).long()\n",
    "\n",
    "    # Split data with scikitlearn\n",
    "    train_data,dev_data, train_labels,dev_labels = train_test_split(data_T,labels_T,test_size=0.1)\n",
    "\n",
    "    # Make an exact copy of the train data after splitting, to avoid twin items\n",
    "    # in train and dev sets (normalise each image individually to its max; it\n",
    "    # shouldn't be a big deal here)\n",
    "    if double_data==True:\n",
    "        train_data_noisy = train_data + torch.rand_like(train_data)/2\n",
    "        train_data       = torch.cat((train_data,train_data_noisy),axis=0)\n",
    "        train_data       = train_data / train_data.max(dim=1,keepdim=True)[0]\n",
    "        train_labels     = torch.cat((train_labels,train_labels),axis=0)\n",
    "\n",
    "    # PyTorch datasets\n",
    "    train_data = TensorDataset(train_data,train_labels)\n",
    "    dev_data   = TensorDataset(dev_data,dev_labels)\n",
    "\n",
    "    # DataLoader objects\n",
    "    batch_size   = 20\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    dev_loader   = DataLoader(dev_data,batch_size=dev_data.tensors[0].shape[0])\n",
    "\n",
    "    # Get fresh test set from unused data (basically all the remaining data)\n",
    "    test_data   = torch.tensor(data_all[n:,1:]).float()\n",
    "    test_labels = torch.tensor(data_all[n:,0]).long()\n",
    "\n",
    "    return train_loader,dev_loader,(test_data,test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1753738796970,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "6-uyBup3Qu_-",
    "outputId": "51aa3e2d-cd0b-4cbe-a5ff-f8773d074159"
   },
   "outputs": [],
   "source": [
    "# %% Test the data function and visualise\n",
    "\n",
    "# Function\n",
    "train_loader,dev_loader,test_dataset = get_dataset(12,True)\n",
    "\n",
    "# Plot\n",
    "images, _ = next(iter(train_loader))\n",
    "img = images.detach()\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(3,4,figsize=(phi*6,6))\n",
    "for i,ax in enumerate(ax.flatten()):\n",
    "    ax.imshow(np.reshape(img[i,:],(28,28)),cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Some data and some augmented noisy data')\n",
    "\n",
    "plt.savefig('figure18_data_noise_augmentation_devset_test_set.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure18_data_noise_augmentation_devset_test_set.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pArt1IFQu90"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.hid1   = nn.Linear(64,32)\n",
    "            self.hid2   = nn.Linear(32,32)\n",
    "            self.output = nn.Linear(32,10)\n",
    "\n",
    "        def forward(self,x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid1(x))\n",
    "            x = F.relu(self.hid2(x))\n",
    "\n",
    "            return self.output(x)\n",
    "\n",
    "    # Model instance, loss function, and optimizer\n",
    "    ANN       = model()\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9S9sINzVQu7g"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Epochs (few to keep some varaibility in performace) and fresh model instance\n",
    "    num_epochs = 50\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    # Preallocate vars\n",
    "    losses    = torch.zeros(num_epochs)\n",
    "    train_acc = torch.zeros(num_epochs)\n",
    "    dev_acc   = torch.zeros(num_epochs)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training data batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward pass, backpropagation, and optimizer step\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        losses[epoch_i]    = np.mean(batch_loss).item()\n",
    "        train_acc[epoch_i] = np.mean(batch_acc).item()\n",
    "\n",
    "        # Dev set accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(dev_loader))\n",
    "            yHat = ANN(X)\n",
    "            dev_acc[epoch_i] = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,dev_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAYO2rNsQu5E"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment on amount of data\n",
    "\n",
    "# Parameters and preallocated vars\n",
    "sample_sizes   = np.arange(500,4001,500)\n",
    "results_single = torch.zeros((len(sample_sizes),3))\n",
    "results_double = torch.zeros((len(sample_sizes),3))\n",
    "\n",
    "# Run the experiment (takes ~3 mins)\n",
    "for i,sample_size in enumerate(sample_sizes):\n",
    "\n",
    "    # Non-doubled data\n",
    "    train_loader,dev_loader,test_dataset = get_dataset(sample_size,False)\n",
    "    train_acc,dev_acc,losses,ANN         = train_model()\n",
    "\n",
    "    # Get results\n",
    "    results_single[i,0] = torch.mean(train_acc[-5:]).item()\n",
    "    results_single[i,1] = torch.mean(dev_acc[-5:]).item()\n",
    "    results_single[i,2] = torch.mean(losses[-5:]).item()\n",
    "\n",
    "    # Doubled data\n",
    "    train_loader,dev_loader,test_loader = get_dataset(sample_size,True)\n",
    "    train_acc,dev_acc,losses,ANN        = train_model()\n",
    "\n",
    "    # Get results\n",
    "    results_double[i,0] = torch.mean(train_acc[-5:]).item()\n",
    "    results_double[i,1] = torch.mean(dev_acc[-5:]).item()\n",
    "    results_double[i,2] = torch.mean(losses[-5:]).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "executionInfo": {
     "elapsed": 1232,
     "status": "ok",
     "timestamp": 1753738666153,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "D-PPEOkoQuwj",
    "outputId": "beca93e4-efcc-4add-d2ba-83fb6d97ba01"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(1.5*6*phi,6))\n",
    "\n",
    "# Axes and title labels\n",
    "titles    = ['Train accuracy','Dev. set accuracy','Losses']\n",
    "yaxlabels = ['Accuracy','Accuracy','Losses']\n",
    "\n",
    "# Common features\n",
    "for i in range(3):\n",
    "\n",
    "    # Plot the train and dev set info\n",
    "    ax[i].plot(sample_sizes,results_single[:,i],'s-',label='Original')\n",
    "    ax[i].plot(sample_sizes,results_double[:,i],'s-',label='Doubled')\n",
    "\n",
    "    # Make it nicer\n",
    "    ax[i].set_ylabel(yaxlabels[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('Unique sample size')\n",
    "    ax[i].grid('on')\n",
    "\n",
    "    if i<2:\n",
    "        ax[i].set_ylim([20,102])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure19_data_noise_augmentation_devset_test_set.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure19_data_noise_augmentation_devset_test_set.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_-755goXSB3"
   },
   "outputs": [],
   "source": [
    "# %% Re-run model with n = 500\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "# Original data\n",
    "train_loader,dev_loader,test_dataset = get_dataset(sample_size,False)\n",
    "train_acc_O,dev_acc_O,losses_O,ANN_O = train_model()\n",
    "\n",
    "# Augmented data\n",
    "train_loader,dev_loader,test_dataset = get_dataset(sample_size,True)\n",
    "train_acc_A,dev_acc_A,losses_A,ANN_A = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1753738692783,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "WiDyrYsfXR-c",
    "outputId": "ab253be0-896e-47ed-a313-ae5816b6d14e"
   },
   "outputs": [],
   "source": [
    "# %% Test on actual test data\n",
    "\n",
    "# Test data\n",
    "X,y = test_dataset\n",
    "\n",
    "# Original data\n",
    "yHat_O     = ANN_O(X)\n",
    "test_acc_O = 100*torch.mean((torch.argmax(yHat_O,axis=1)==y).float())\n",
    "\n",
    "# Augmented data\n",
    "yHat_A     = ANN_A(X)\n",
    "test_acc_A = 100*torch.mean((torch.argmax(yHat_A,axis=1)==y).float())\n",
    "\n",
    "# Print output\n",
    "print(f'ORIGINAL MODEL (N={sample_size}):\\n  Train: {train_acc_O[-1]:.2f}%, devset: {dev_acc_O[-1]:.2f}%, test: {test_acc_O:.2f}%\\n')\n",
    "print(f'AUGMENTED MODEL (N={sample_size}):\\n  Train: {train_acc_A[-1]:.2f}%, devset: {dev_acc_A[-1]:.2f}%, test: {test_acc_A:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9cD3oA9dEXC"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 0\n",
    "#    Could we have added Gaussian noise instead of uniform noise?\n",
    "\n",
    "# Sure, adapt code and re run:\n",
    "# > train_data_noisy = train_data + torch.randn(train_data.shape)*0.2\n",
    "# The results are similar for a gaussian noise  scaled by 1/5 (one can try\n",
    "# change the noise but without scaling dawn, the std of the Gaussian would be as\n",
    "# large as te range of the normalised data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yKBn_1lXR09"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    It looks like noise augmentation only helped for small sample sizes (<1000). Write code to run a new experiment that\n",
    "#    reproduces this experiment 10 times, but only using sample sizes [500,1000,2000]. Then make a plot showing the increase\n",
    "#    in devset accuracy for all 10 runs. That will help determine whether our finding above was a quirk of sampling\n",
    "#    variability or a meaningful effect.\n",
    "\n",
    "# Assuming I understood the exercise right, these parametric experiment shows that\n",
    "# for low sample sizes the model underperforms, but there is also more variability.\n",
    "# With more data the accuracy increases and the variability also shrinks considerably.\n",
    "# Also, the augmented model shows better performance and lower variability even\n",
    "# with less data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 364350,
     "status": "ok",
     "timestamp": 1753739187079,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "D-w3aptNiBX9",
    "outputId": "5355500a-68ba-4e20-b7af-26b3854a5ce3"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1 - continue ...\n",
    "#    Run experiment for multiple sample sizes and repeat 10 times each\n",
    "\n",
    "# Parameters\n",
    "sample_sizes = [500,1000,2000]\n",
    "reps         = 10\n",
    "\n",
    "# Preallocate vars\n",
    "mean_acc_O = np.zeros(len(sample_sizes))\n",
    "std_acc_O  = np.zeros(len(sample_sizes))\n",
    "mean_acc_A = np.zeros(len(sample_sizes))\n",
    "std_acc_A  = np.zeros(len(sample_sizes))\n",
    "\n",
    "all_accs_O = [[] for _ in sample_sizes]\n",
    "all_accs_A = [[] for _ in sample_sizes]\n",
    "\n",
    "# Go with the experiment! (takes ~6 mins)\n",
    "for i,sample_size in enumerate(sample_sizes):\n",
    "    print(f'\\n<<< SAMPLE SIZE: {sample_size} >>>')\n",
    "\n",
    "    # Store test accuracies\n",
    "    test_accs_O = []\n",
    "    test_accs_A = []\n",
    "\n",
    "    for run in range(reps):\n",
    "        print(f'\\n--- Run {run+1}/{reps} ---')\n",
    "\n",
    "        # Original data\n",
    "        train_loader,dev_loader,test_dataset = get_dataset(sample_size,False)\n",
    "        train_acc_O,dev_acc_O,losses_O,ANN_O = train_model()\n",
    "\n",
    "        # Augmented data\n",
    "        train_loader,dev_loader,test_dataset = get_dataset(sample_size,True)\n",
    "        train_acc_A,dev_acc_A,losses_A,ANN_A = train_model()\n",
    "\n",
    "        # Test data\n",
    "        X,y = test_dataset\n",
    "\n",
    "        # Original model accuracy\n",
    "        yHat_O     = ANN_O(X)\n",
    "        test_acc_O = 100*torch.mean((torch.argmax(yHat_O,axis=1)==y).float())\n",
    "        all_accs_O[i].append(test_acc_O)\n",
    "\n",
    "        # Augmented model accuracy\n",
    "        yHat_A     = ANN_A(X)\n",
    "        test_acc_A = 100*torch.mean((torch.argmax(yHat_A,axis=1)==y).float())\n",
    "        all_accs_A[i].append(test_acc_A)\n",
    "\n",
    "        # Print for this run\n",
    "        print(f'Original  - Train: {train_acc_O[-1]:.2f}%, Dev: {dev_acc_O[-1]:.2f}%, Test: {test_acc_O:.2f}%')\n",
    "        print(f'Augmented - Train: {train_acc_A[-1]:.2f}%, Dev: {dev_acc_A[-1]:.2f}%, Test: {test_acc_A:.2f}%')\n",
    "\n",
    "    # Store mean and std\n",
    "    mean_acc_O[i] = np.mean(all_accs_O[i])\n",
    "    std_acc_O[i]  = np.std(all_accs_O[i])\n",
    "    mean_acc_A[i] = np.mean(all_accs_A[i])\n",
    "    std_acc_A[i]  = np.std(all_accs_A[i])\n",
    "\n",
    "    # Print average results\n",
    "    print(f'\\n>>> AVERAGE RESULTS FOR N={sample_size} <<<')\n",
    "    print(f'Original  - Mean ± std test accuracy: {mean_acc_O[i]:.2f}% ± {std_acc_O[i]:.2f}%')\n",
    "    print(f'Augmented - Mean ± std test accuracy: {mean_acc_A[i]:.2f}% ± {std_acc_A[i]:.2f}%\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1753739308621,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "wrk5hdYjj4IR",
    "outputId": "54fe5f6b-2591-44db-95e8-adcb5980e573"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1 - continue ...\n",
    "#    Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "# Convert sample sizes to x-locations\n",
    "x = np.array(sample_sizes)\n",
    "\n",
    "# Plot error bars\n",
    "offset = 10\n",
    "plt.errorbar(x-offset,mean_acc_O, yerr=std_acc_O, fmt='s-',label='Original',capsize=5)\n",
    "plt.errorbar(x+offset,mean_acc_A, yerr=std_acc_A, fmt='o-',label='Augmented',capsize=5)\n",
    "\n",
    "# Overlay individual runs\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    jitter = 10\n",
    "    xs_O   = np.random.normal(loc=sample_size-30,scale=jitter,size=reps)\n",
    "    xs_A   = np.random.normal(loc=sample_size+30,scale=jitter,size=reps)\n",
    "    plt.scatter(xs_O,all_accs_O[i],color='tab:blue',alpha=0.3,s=40)\n",
    "    plt.scatter(xs_A,all_accs_A[i],color='tab:orange',alpha=0.3,s=40)\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel('Sample size')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.xticks(sample_sizes, [str(n) for n in sample_sizes])\n",
    "plt.ylim([20,100])\n",
    "plt.suptitle('Test Accuracy')\n",
    "plt.title(f'Mean ± Std and individual runs (n = {reps})')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure22_data_noise_augmentation_devset_test_set_extra1.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure22_data_noise_augmentation_devset_test_set_extra1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9MbM_hgXUgD"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Immediately after loading in the MNIST data (top of the script), there is the following code:\n",
    "#    dataFull[:,1:] = dataFull[:,1:] / np.max(dataFull)\n",
    "#    This is different from the corresponding normalization code in the previous MNIST videos. Do you need the [:,1:]?\n",
    "#    What happens when you change that line to: dataFull = dataFull / np.max(dataFull)\n",
    "#    Can you still train the model?\n",
    "\n",
    "# I'd say the indexing is needed because we want to normalise the pixels in the\n",
    "# images, ranging [1,256], not the labels, stored in the 1st column. In this\n",
    "# regard I was wondering whether one should normalise as:\n",
    "#\n",
    "# > data_all[:,1:] = data_all[:,1:] / np.max(data_all[:,1:])\n",
    "#\n",
    "# I mean, here the max is 255 (256-1) so no problems in practice, but maybe it's\n",
    "# less confusing\n",
    "# That said, if we normalise as:\n",
    "#\n",
    "# > dataFull = dataFull / np.max(dataFull)\n",
    "#\n",
    "# we'll normailise also the labels; this is not a problem per se, the labels are\n",
    "# just labels, but of course it's more practical to have meaningful labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heRZI8AFXj_Q"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    We augmented the data using noisy copies. Another idea is to augment the dataset using color-flipped copies. Thus,\n",
    "#    the numbers will be black on a white background. Try it and see how the results compare to the noise-added version!\n",
    "\n",
    "# Surprisingly or not, there is still an effect, and augmented model performs a\n",
    "# bit better. I'm not sure whether that was expected or not, so happy\n",
    "# to hear any consideration on that.\n",
    "# Intuitively I'd say that since we are adding the inverse images only in the\n",
    "# training data, this kind of augmentation is quite irrelevant, because then in\n",
    "# the dev set or in the test set we only have \"regural\" images, and the image\n",
    "# inversion doesn't really help in adding noise (i.e., the data distribution\n",
    "# is still the same). On the other hand, learning to classify also inverted images\n",
    "# should reduce overfitting, and thus improve classification on the dev set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9Tv3uqgwQAH"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3 - continue ...\n",
    "#    Modify data function\n",
    "\n",
    "# %% Function to get the data (train, dev, test)\n",
    "\n",
    "# Load data and normalise\n",
    "data_all       = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "data_all[:,1:] = data_all[:,1:] / np.max(data_all[:,1:])\n",
    "\n",
    "# Function\n",
    "def get_dataset(n,double_data=False):\n",
    "\n",
    "    # Remove labels (i.e., numbers IDs) from dataset, and select only n data\n",
    "    labels = data_all[:n,0]\n",
    "    data   = data_all[:n,1:]\n",
    "\n",
    "    # Covert to tensor\n",
    "    data_T   = torch.tensor(data).float()\n",
    "    labels_T = torch.tensor(labels).long()\n",
    "\n",
    "    # Split data with scikitlearn\n",
    "    train_data,dev_data, train_labels,dev_labels = train_test_split(data_T,labels_T,test_size=0.1)\n",
    "\n",
    "    # Make an exact copy of the train data after splitting, to avoid twin items\n",
    "    # in train and dev sets (normalise each image individually to its max)\n",
    "    if double_data==True:\n",
    "        train_data_noisy = train_data + torch.rand_like(train_data)/2\n",
    "        train_data       = torch.cat((train_data,train_data_noisy),axis=0)\n",
    "        train_data       = train_data / train_data.max(dim=1,keepdim=True)[0]\n",
    "        train_labels     = torch.cat((train_labels,train_labels),axis=0)\n",
    "\n",
    "    # PyTorch datasets\n",
    "    train_data = TensorDataset(train_data,train_labels)\n",
    "    dev_data   = TensorDataset(dev_data,dev_labels)\n",
    "\n",
    "    # DataLoader objects\n",
    "    batch_size   = 20\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    dev_loader   = DataLoader(dev_data,batch_size=dev_data.tensors[0].shape[0])\n",
    "\n",
    "    # Get fresh test set from unused data (basically all the remaining data)\n",
    "    test_data   = torch.tensor(data_all[n:,1:]).float()\n",
    "    test_labels = torch.tensor(data_all[n:,0]).long()\n",
    "\n",
    "    return train_loader,dev_loader,(test_data,test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKvxWsv2bX83dVY+t8lPw4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
