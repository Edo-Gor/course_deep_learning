{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIzJv0wIds3L"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 6.34\n",
    "#    Gradient descent in 2D\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JG5yVXKd02F"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import sympy               as sym\n",
    "import copy\n",
    "\n",
    "from mpl_toolkits.mplot3d             import Axes3D\n",
    "from google.colab                     import files\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ljTQdZLd5r6"
   },
   "outputs": [],
   "source": [
    "# %% Define function\n",
    "\n",
    "# This function is called 'peaks' and in facts is the function in the MatLab logo,\n",
    "# may I be forgiven for implementing it in bloody Python\n",
    "\n",
    "def peaks(x,y):\n",
    "\n",
    "    # Expand to a 2D mesh\n",
    "    x,y = np.meshgrid(x,y)\n",
    "\n",
    "    z = 3*(1-x)**2 * np.exp(-(x**2) - (y+1)**2) \\\n",
    "        - 10*(x/5 - x**3 - y**5) * np.exp(-x**2 - y**2) \\\n",
    "        - 1/3*np.exp(-(x+1)**2 - y**2)\n",
    "\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1740868334830,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "xGff7hhfgjNI",
    "outputId": "f47d8d77-165f-4bba-8dc3-4cfbbb5ef804"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "# Create landscape\n",
    "x = np.linspace(-3,3,201)\n",
    "y = np.linspace(-3,3,201)\n",
    "\n",
    "z = peaks(x,y)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower',cmap='jet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1740868373760,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "i0WNguBnhf8S",
    "outputId": "db69a708-2ef4-4756-fb50-87b21e4b5554"
   },
   "outputs": [],
   "source": [
    "# %% Compute derivative with sympy\n",
    "\n",
    "# Create symbols and redefine function for sympy\n",
    "sx,sy = sym.symbols('sx,sy')\n",
    "sz    = 3*(1-sx)**2 * sym.exp(-(sx**2) - (sy+1)**2) \\\n",
    "        - 10*(sx/5 - sx**3 - sy**5) * sym.exp(-sx**2 - sy**2) \\\n",
    "        - 1/3*sym.exp(-(sx+1)**2 - sy**2)\n",
    "\n",
    "# Compute partial derivatives (.lambdify() transforms the symbolic function into a numpy usable function)\n",
    "df_x = sym.lambdify( (sx,sy),sym.diff(sz,sx),'sympy' )\n",
    "df_y = sym.lambdify( (sx,sy),sym.diff(sz,sy),'sympy' )\n",
    "\n",
    "# Example of partial derivative computation\n",
    "df_x(1,1).evalf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2261,
     "status": "ok",
     "timestamp": 1740865924351,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "9WvHPQvGi3Tz",
    "outputId": "ddb9b906-0c6e-465b-8ba5-8c751a214cd4"
   },
   "outputs": [],
   "source": [
    "# %% Gradient descent in 2D\n",
    "\n",
    "# 1) Random starting point (uniform between -2 and +2); try also fixed at [0,1.4]\n",
    "local_min = np.random.rand(2)*4-2\n",
    "start_pnt = local_min[:]\n",
    "\n",
    "print(f'Random starting local mininum: {local_min}')\n",
    "\n",
    "# 2) Learning parameters\n",
    "learning_rate   = .01\n",
    "training_epochs = 1000\n",
    "\n",
    "# 3) Loop over epochs\n",
    "trajectory = np.zeros((training_epochs,2))\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    gradient = np.array([ df_x(local_min[0],local_min[1]).evalf(),\n",
    "                          df_y(local_min[0],local_min[1]).evalf()\n",
    "                          ])\n",
    "    local_min       = local_min - gradient*learning_rate\n",
    "    trajectory[i,:] = local_min\n",
    "\n",
    "print(f'Estimated local mininum: {local_min}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1740865943522,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "CoGU-ZlAmQRT",
    "outputId": "b6e98fea-cdc3-4afa-902e-055d6353973a"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "plt.imshow(z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower',cmap='jet')\n",
    "plt.plot(start_pnt[0],start_pnt[1],'bs')\n",
    "plt.plot(local_min[0],local_min[1],'ro')\n",
    "plt.plot(trajectory[:,0],trajectory[:,1],'r')\n",
    "plt.legend(['Rand start','Local min'])\n",
    "plt.suptitle('Random starting point')\n",
    "plt.title(f'Training epochs: {training_epochs} and learning rate: {learning_rate}')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.savefig('figure16_gradient_descent_2d.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure16_gradient_descent_2d.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1740869030677,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "vNqTu4ETwsAj",
    "outputId": "0c181cd4-f483-4697-a832-28b7a4e8fb00"
   },
   "outputs": [],
   "source": [
    "# %% Plot the function in 3D for fun\n",
    "\n",
    "x = np.linspace(-3,3,201)\n",
    "y = np.linspace(-3,3,201)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax  = fig.add_subplot(111,projection='3d')\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "ax.plot_surface(X,Y,z,cmap='jet')\n",
    "ax.view_init(elev=30,azim=250)\n",
    "ax.set_xlabel('x axis')\n",
    "ax.set_ylabel('y axis')\n",
    "ax.set_zlabel('f(x,y)')\n",
    "ax.set_title('3D Surface Plot')\n",
    "\n",
    "plt.savefig('figure24_gradient_descent_2d.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure24_gradient_descent_2d.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "executionInfo": {
     "elapsed": 266035,
     "status": "ok",
     "timestamp": 1740870308342,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "93MADk600mkH",
    "outputId": "d9a61b67-211b-4c59-b65b-8f74299a0295"
   },
   "outputs": [],
   "source": [
    "# %% Visualise the basins of attraction for the gradient descent algorithm\n",
    "\n",
    "# Grid for function visualization\n",
    "x = np.linspace(-4,4,201)\n",
    "y = np.linspace(-4,4,201)\n",
    "\n",
    "# Gradient descent parameters\n",
    "learning_rate   = 0.01\n",
    "training_epochs = 100\n",
    "\n",
    "# Generate multiple starting points in a 25x25 grid\n",
    "start_x      = np.linspace(-2.5,2.5,25)\n",
    "start_y      = np.linspace(-2.5,2.5,25)\n",
    "start_points = np.array(np.meshgrid(start_x,start_y)).T.reshape(-1,2)\n",
    "\n",
    "# Plot function\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower',cmap='jet')\n",
    "\n",
    "# Run gradient descent for each starting point\n",
    "for start_pnt in start_points:\n",
    "    local_min  = start_pnt.copy()\n",
    "    trajectory = np.zeros((training_epochs, 2))\n",
    "\n",
    "    for i in range(training_epochs):\n",
    "        gradient = np.array([df_x(local_min[0],local_min[1]),\n",
    "                             df_y(local_min[0],local_min[1])\n",
    "                             ])\n",
    "        local_min       = local_min - gradient*learning_rate\n",
    "        trajectory[i,:] = local_min\n",
    "\n",
    "    # Plot trajectory\n",
    "    plt.plot(start_pnt[0],start_pnt[1],'bs',markersize=2)\n",
    "    plt.plot(local_min[0],local_min[1],'ko',markersize=3)\n",
    "    plt.plot(trajectory[:,0],trajectory[:,1],'r',alpha=0.5)\n",
    "\n",
    "plt.legend(['Start point','Local min'])\n",
    "plt.suptitle('Basins of Attraction for Gradient Descent')\n",
    "plt.title(f'Training epochs: {training_epochs}, Learning rate: {learning_rate}')\n",
    "\n",
    "plt.savefig('figure25_gradient_descent_2d.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure25_gradient_descent_2d.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux2r7NvFoQBz"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Modify the code to force the initial guess to be [0,1.4]. Does the model reach a reasonable local minimum?\n",
    "\n",
    "# No, it gets stuck at a poor local minimun on every run ([0.296 0.320])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3rHG9JaoQ4Z"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Using the same starting point, change the number of training epochs to 10,000. Does the final solution differ from\n",
    "#    using 1000 epochs?\n",
    "\n",
    "# No, still stucked at [0.296 0.320]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9iaCCexoQdO"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    (Again with the same starting location) Change the learning to .1 (1000 epochs). What do you notice about the trajectory?\n",
    "#    Try again with the learning rate set to .5, and then to .00001.\n",
    "\n",
    "# A rate of .1 doesn't change much, the gradient still falls in the same poor local minimum; a rate of .5 produces\n",
    "# a catastrophic result, the rate is so out-of-scale compared to the data that when multiplied with the gradient\n",
    "# it just makes the local minima jump everywhere across the function; finally, a rate of .00001 is also out-of-scale\n",
    "# because it generates steps so small that with 1000 iterations the gradient is still nowhere close to a minimum\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNT4EXxaXq+y7F5S4bg5w/t",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
