{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxLsu_5_Ao80"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 11.107\n",
    "#    FFN to classify digits\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFOW2iaUArr4"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Xk2V5LoUo8F"
   },
   "outputs": [],
   "source": [
    "# %% Architecture of FFN model\n",
    "\n",
    "# We will here use a fairly simple model with :\n",
    "# > Input layer (N=28x28=784)\n",
    "# > Hidden fully connected layer 1 (N=64)\n",
    "# > Hidden fully connected layer 2 (N=32)\n",
    "# > Output layer (N=10)\n",
    "# > ReLu after each layer, except softmax after the output layer (to get probs)\n",
    "\n",
    "# Extra note :\n",
    "# > We will also use the log of the softmax to stretch out small probabilities\n",
    "#   and increase the penalty for incorrect guesses; this can increase category\n",
    "#   separability and numerical stability (but standard/linear softmax also works\n",
    "#   fine sometimes, especially for a relative small number of categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvGQ1zKsUO8m"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Split labels from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# Normalise data (original range is (0,255))\n",
    "data_norm = data / np.max(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 1804,
     "status": "ok",
     "timestamp": 1750197714039,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "zmGQORDzVm9r",
    "outputId": "300f3560-aa13-4b76-a096-6aff1feed198"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(6*phi,6))\n",
    "\n",
    "ax[0].hist(data.flatten(),50)\n",
    "ax[0].set_xlabel('Pixel intensity values')\n",
    "ax[0].set_ylabel('Pixel count')\n",
    "ax[0].set_title('Original data')\n",
    "ax[0].set_yscale('log')\n",
    "\n",
    "ax[1].hist(data_norm.flatten(),50)\n",
    "ax[1].set_xlabel('Pixel intensity values')\n",
    "ax[1].set_ylabel('Pixel count')\n",
    "ax[1].set_title('Normalised data')\n",
    "ax[1].set_yscale('log')\n",
    "\n",
    "plt.savefig('figure5_mnist_classify_digits.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure5_mnist_classify_digits.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4S59tPjVohJ"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Convert to tensor (float and integers)\n",
    "data_tensor   = torch.tensor(data_norm).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "# Split data with scikitlearn (10% test data)\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1750198351389,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "cfUiM74V8xiK",
    "outputId": "a3d84a70-d887-4afd-94f7-4a514e182333"
   },
   "outputs": [],
   "source": [
    "# %% Check variables in workspace so far\n",
    "\n",
    "%whos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO5LXN3D87TB"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.fc1    = nn.Linear( 64,32)\n",
    "            self.fc2    = nn.Linear( 32,32)\n",
    "            self.output = nn.Linear( 32,10)\n",
    "\n",
    "        # Forward propagation (log-softmax because NLLLoss instead of CrossEntropyLoss)\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = torch.log_softmax( self.output(x),axis=1 )\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create model instance\n",
    "    ANN = mnist_FFN()\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.NLLLoss()\n",
    "\n",
    "    # Optimizer (SGD to slow down learning for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7920,
     "status": "ok",
     "timestamp": 1750325532306,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "1P-V22PH62Rx",
    "outputId": "fc95bfba-0c27-4ef0-98ab-4f348b0d8a99"
   },
   "outputs": [],
   "source": [
    "# Test the model on one batch\n",
    "\n",
    "ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "X,y  = next(iter(train_loader))\n",
    "yHat = ANN(X)\n",
    "\n",
    "# Print log-softmax output (size should be batch_size by output nodes)\n",
    "print(yHat)\n",
    "print(yHat.shape)\n",
    "print()\n",
    "\n",
    "# Print probabilities\n",
    "print(torch.exp(yHat))\n",
    "print()\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fun(yHat,y)\n",
    "print('Loss: ')\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BPr1HkE8WXF"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        X,y = next(iter(test_loader))\n",
    "        yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FEWUdEM-bTc"
   },
   "outputs": [],
   "source": [
    "# %% Run the training\n",
    "\n",
    "train_acc,test_acc,losses,ANN = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1750282816525,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "OWhWN4BX-iDa",
    "outputId": "baf40953-09d9-4853-b5eb-d1e4b55b139d"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(train_acc,label='Train accuracy')\n",
    "ax[1].plot(test_acc,label='Test accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure6_mnist_classify_digits.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure6_mnist_classify_digits.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1750336982734,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "qWkNGaEI_HPn",
    "outputId": "9b36441b-b1c6-4f72-9a12-21d44688e292"
   },
   "outputs": [],
   "source": [
    "# %% Inspect output in more detail\n",
    "\n",
    "# Run model though for test data\n",
    "X,y   = next(iter(test_loader))\n",
    "preds = ANN(X).detach()\n",
    "\n",
    "print(preds)\n",
    "print(torch.exp(preds))\n",
    "\n",
    "# Evidence for all numbers from one sample (log-softmax and softmax)\n",
    "sample2show = 120\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "plt.bar(range(10),preds[sample2show])\n",
    "plt.xticks(range(10))\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Evidence for that number (log-softmax)')\n",
    "plt.title(f'True number was {y[sample2show].item()}')\n",
    "\n",
    "plt.savefig('figure7_mnist_classify_digits.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure7_mnist_classify_digits.png')\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "plt.bar(range(10),np.exp(preds[sample2show]))\n",
    "plt.xticks(range(10))\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Evidence for that number (softmax)')\n",
    "plt.title(f'True number was {y[sample2show].item()}')\n",
    "\n",
    "plt.savefig('figure8_mnist_classify_digits.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure8_mnist_classify_digits.png')\n",
    "\n",
    "# Find and print the errors\n",
    "errors = np.where( torch.max(preds,axis=1)[1] != y )[0]\n",
    "print(errors)\n",
    "\n",
    "# Evidence for all numbers from one sample\n",
    "sample2show = 2\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(6*phi,6))\n",
    "\n",
    "ax[0].bar(range(10),np.exp(preds[errors[sample2show]]))\n",
    "ax[0].set_xticks(range(10))\n",
    "ax[0].set_xlabel('Number')\n",
    "ax[0].set_ylabel('Evidence for that number')\n",
    "ax[0].set_title(f'True number: {y[errors[sample2show]].item()}, model guessed {torch.argmax(preds[errors[sample2show]]).item()}')\n",
    "\n",
    "ax[1].imshow( np.reshape(X[errors[sample2show],:],(28,28)) ,cmap='gray')\n",
    "\n",
    "plt.savefig('figure9_mnist_classify_digits.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure9_mnist_classify_digits.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1750336808327,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "XDlHF7sXCnAP",
    "outputId": "a4b2ccf5-7c09-482d-b14c-161b3911059e"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Average together the correct 7's and the error 7's, and make images of them (that is, one image\n",
    "#    of all correct 7's and one image of all incorrectly labeled 7's). How do they look?\n",
    "\n",
    "# A pretty nice platonic seven for the right ones, and an ugly blob for the incorrect\n",
    "# classifications\n",
    "\n",
    "# Get correct and wrong sevens and average\n",
    "X,y   = next(iter(test_loader))\n",
    "preds = ANN(X).detach()\n",
    "\n",
    "pred_y    = torch.argmax(preds,axis=1)\n",
    "pred_y_np = pred_y.numpy()\n",
    "true_y_np = y.numpy()\n",
    "\n",
    "num  = 7\n",
    "true_indices = np.where(true_y_np == num)[0]\n",
    "correct = [j for j in true_indices if pred_y_np[j] == num]\n",
    "errors  = [j for j in true_indices if pred_y_np[j] != num]\n",
    "\n",
    "avg_errors  = torch.mean(X[errors],axis=0)\n",
    "avg_correct = torch.mean(X[correct],axis=0)\n",
    "\n",
    "# Plot\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(6*phi,6))\n",
    "\n",
    "ax[0].imshow(np.reshape(avg_errors.numpy(),(28,28)),cmap='gray')\n",
    "ax[0].set_title('Average of all wrongly\\nclassified 7s')\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(np.reshape(avg_correct.numpy(),(28,28)),cmap='gray')\n",
    "ax[1].set_title('Average of all correctly classified 7s\\n(\"The Platonic 7\")')\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.savefig('figure10_mnist_classify_digits_extra1.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure10_mnist_classify_digits_extra1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "executionInfo": {
     "elapsed": 1843,
     "status": "ok",
     "timestamp": 1750336668120,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "3cU5CcmeD5Ls",
    "outputId": "f09caa69-d7a7-469d-f11d-f2949160a60a"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Repeat #1 for all numbers to produce a 2x10 matrix of images with corrects on top\n",
    "#    and errors on the bottom.\n",
    "\n",
    "# Same as above, but loop over all the numbers; some of the errors are surprising\n",
    "# but some averages are indeed pretty unrecognisable\n",
    "\n",
    "# Get correct and wrong numbers and average\n",
    "X,y   = next(iter(test_loader))\n",
    "preds = ANN(X).detach()\n",
    "\n",
    "pred_y    = torch.argmax(preds,axis=1)\n",
    "pred_y_np = pred_y.numpy()\n",
    "true_y_np = y.numpy()\n",
    "\n",
    "nums = np.arange(10)\n",
    "\n",
    "avg_errors_img  = np.zeros((28, 28, 10))\n",
    "avg_correct_img = np.zeros((28, 28, 10))\n",
    "\n",
    "for i, num in enumerate(nums):\n",
    "\n",
    "    true_indices = np.where(true_y_np == num)[0]\n",
    "\n",
    "    correct = [j for j in true_indices if pred_y_np[j] == num]\n",
    "    errors  = [j for j in true_indices if pred_y_np[j] != num]\n",
    "\n",
    "    avg_errors  = torch.mean(X[errors],axis=0)\n",
    "    avg_correct = torch.mean(X[correct],axis=0)\n",
    "\n",
    "    avg_errors_img[:,:,i]  = np.reshape(avg_errors.numpy(),(28,28))\n",
    "    avg_correct_img[:,:,i] = np.reshape(avg_correct.numpy(),(28,28))\n",
    "\n",
    "# Plot\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(2,10,figsize=(2*8*phi,8))\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    ax[0,i].imshow(avg_errors_img[:,:,i],cmap='gray')\n",
    "    ax[0,i].set_title(f'Wrong\\n{i}s')\n",
    "    ax[0,i].axis('off')\n",
    "\n",
    "    ax[1,i].imshow(avg_correct_img[:,:,i],cmap='gray')\n",
    "    ax[1,i].set_title(f'Correct\\n{i}s')\n",
    "    ax[1,i].axis('off')\n",
    "\n",
    "plt.savefig('figure11_mnist_classify_digits_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure11_mnist_classify_digits_extra2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1750338829168,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "lBnyMjBUD8Tm",
    "outputId": "41ed0308-3f8c-41f5-c4b0-8be2b617fc11"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    Identify \"almost errors,\" which we can define as correct categorizations that had a probability of\n",
    "#    e.g., >.1 for any other number. Make images of some of these numbers. Can you understand why the model\n",
    "#    was confused?\n",
    "\n",
    "# To an human eye it's quite obvious which one is the correct classification, but\n",
    "# sometimes one can see why the model couldn't get it right\n",
    "\n",
    "# Get all misclassified labels, turn back log-softmax to probs, get misclassified\n",
    "# digits, compute margin between top-2 predictions, and then get nearly correctly\n",
    "# classified digits\n",
    "X,y    = next(iter(test_loader))\n",
    "preds  = ANN(X).detach()\n",
    "probs  = torch.exp(preds)\n",
    "\n",
    "pred_labels = torch.argmax(probs,dim=1)\n",
    "true_labels = y\n",
    "\n",
    "misclassified = (pred_labels != true_labels)\n",
    "top2          = torch.topk(probs,2,dim=1)\n",
    "margin        = top2.values[:,0] - top2.values[:,1]\n",
    "\n",
    "margin_threshold    = 0.1\n",
    "almost_right_margin = misclassified & (margin <= margin_threshold)\n",
    "\n",
    "print(f\"Total misclassified digits: {misclassified.sum().item()}\")\n",
    "print(f\"Diguts misclassified with a low-margin errors (≤ {margin_threshold}): {almost_right_margin.sum().item()}\")\n",
    "\n",
    "# Plot\n",
    "X_almost     = X[almost_right_margin]\n",
    "y_almost     = y[almost_right_margin]\n",
    "preds_almost = pred_labels[almost_right_margin]\n",
    "\n",
    "indices = torch.where(almost_right_margin)[0]\n",
    "\n",
    "if len(indices) >= 2:\n",
    "\n",
    "    selected = np.random.choice(indices.tolist(),2)\n",
    "\n",
    "    phi = ( 1 + np.sqrt(5) ) / 2\n",
    "    fig,ax = plt.subplots(1,2,figsize=(6*phi,6))\n",
    "\n",
    "    for i,img2show in enumerate(selected):\n",
    "\n",
    "        img      = X[img2show].reshape(28,28).squeeze().numpy()\n",
    "        true_lbl = y[img2show].item()\n",
    "        pred_lbl = pred_labels[img2show].item()\n",
    "        m        = margin[img2show].item()\n",
    "\n",
    "        ax[i].imshow(img, cmap='gray')\n",
    "        ax[i].set_title(f\"True: {true_lbl}, Pred: {pred_lbl}\\nMargin: {m:.3f}\")\n",
    "        ax[i].axis('off')\n",
    "\n",
    "    plt.savefig('figure12_mnist_classify_digits_extra3.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    files.download('figure12_mnist_classify_digits_extra3.png')\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Not enough almost-right misclassified samples in this batch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozltac_9D9x_"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 4\n",
    "#    I didn't use .train(), .eval(), or no_grad() here. Is that a problem? Can you add those in without checking\n",
    "#    other notebooks?\n",
    "\n",
    "# It shouldn't be a problem here because we are not using regularisation methods\n",
    "# such as nodes dropout or batch normalisation, but one could place the .train()\n",
    "# and .eval() switches before training starts and before evaluation starts,\n",
    "# respectively; no_grad() could also be used to kill the gradient tracking\n",
    "# during the test phase and save up memory\n",
    "\n",
    "\n",
    "# Modified function to train the model\n",
    "def train_model():\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPY46CWawChZCryfkiHfNm8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
