{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le5MsLuthBqv"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 9.78\n",
    "#    Batch traning in action\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waCpZrD-9UFQ"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyFgMWvNuS-N"
   },
   "outputs": [],
   "source": [
    "# %% Import Iris dataset\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Convert from pandas df to tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Species to numbers\n",
    "labels = torch.zeros(len(data),dtype=torch.long)\n",
    "labels[iris.species=='setosa']     = 0\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1746889551882,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "MgT36_D6dX4M",
    "outputId": "a59133e9-683d-421d-e495-85a4f7a51e43"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "iris.plot(marker='o',linestyle='none',figsize=(12,6))\n",
    "\n",
    "plt.xlabel('Sample number')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Iris dataset features')\n",
    "\n",
    "plt.savefig('figure34_batching_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure34_batching_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ng9312Lu1V5"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects (test data are not partitioned, we don't regularise in testing)\n",
    "# Try size 4, 16, and 32\n",
    "batch_size   = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746892538914,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "hWo9fInWvKer",
    "outputId": "a65cb977-8734-48fd-e340-e7285a474506"
   },
   "outputs": [],
   "source": [
    "# %% Check sizes of data batches\n",
    "\n",
    "# Notice size and numbers of last mini-batch with 'drop_last' option turned True or False\n",
    "for X,y in train_loader:\n",
    "    print(X.shape,y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df2s-SeXv19U"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    # Architecture\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(4,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,3))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (small lr for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.0005)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uR2bGnR9ychG"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 500\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Initialise accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        X,y = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append(  100*torch.mean((pred_labels==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fB9zY9qWyvWZ"
   },
   "outputs": [],
   "source": [
    "# %% Test the model\n",
    "\n",
    "ANN,loss_fun,optimizer = gen_model()\n",
    "train_acc,test_acc,losses = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1746892573660,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "koOdFKumzEdA",
    "outputId": "98d08f13-d3d7-4ec6-f58d-8464fd8e3e9f"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(losses,'^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses with minibatch size = ' + str(batch_size))\n",
    "\n",
    "ax[1].plot(train_acc,'o-')\n",
    "ax[1].plot(test_acc,'s-')\n",
    "ax[1].set_title('Accuracy with minibatch size = ' + str(batch_size))\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "ax[1].set_ylim([27,103])\n",
    "\n",
    "plt.savefig('figure35_batching_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure35_batching_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746891690239,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "9Z-MxZSB03ac",
    "outputId": "d66a8a18-f84a-458c-9b9b-1367850e85ab"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Is there a relationship between the test_size parameter in train_test_split(), the batchsize parameter in DataLoader,\n",
    "#    and the length of test_data? Think of your answer first, then test it in code, by creating new dataloader objects\n",
    "#    with varying test_size parameters.\n",
    "#    Hint: You can use the code 'len(test_data.dataset.tensors[1])', which returns the length of the labels vector.\n",
    "\n",
    "# Changing the proportion of test data will affect the number of mini-batches, depending on the size of the\n",
    "# mini-batches; large proportions of test data combined with large mini-batches sizes -for example- are at risk\n",
    "# of dropping larger chunks of data (if drop_last=True)\n",
    "\n",
    "# %% Modified split\n",
    "\n",
    "# Try different test_size values\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Try different sizes\n",
    "batch_size   = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "# Check length of label vector and batch sizes\n",
    "print(len(test_data.tensors[1]))\n",
    "for X,y in train_loader:\n",
    "    print(X.shape,y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwYYxdlZ036B"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Let's say you didn't care about the three types of irises; you only want a model that labels a flower as setosa or\n",
    "#    \"other.\" What would you have to change in the data and in the model to make this work?\n",
    "\n",
    "# Some minimal changes would allow to change the model to classify 'setosa' and 'others'; mainly, one\n",
    "# need to change the labels and the number of output nodes\n",
    "\n",
    "# %% Modify labels and output nodes\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "labels = torch.zeros(len(data),dtype=torch.long)\n",
    "labels[iris.species=='setosa']     = 0\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 1\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    # Architecture\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(4,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,2))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (small lr for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.0005)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1746892922243,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "DSy6i2TT03v5",
    "outputId": "b9b62438-dd20-4565-ae18-96d14bfb2c2c"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    In the course section \"More on data,\" you will learn that unbalanced designs can be problematic for DL models (an\n",
    "#    unbalanced design means that there is an uneven distribution of samples in different categories). Does the\n",
    "#    modification in #2 produce an unbalanced design? To find out, count the number of data labels that are 0 (setosa) or\n",
    "#    1 (not setosa).\n",
    "\n",
    "# Yes, the previous model was designed to classify 3 labels and the data were divided into 3 balanced\n",
    "# categories, merging 2 categories together produces an unbalanced design\n",
    "\n",
    "print(labels[labels==0].shape)\n",
    "print(labels[labels==1].shape)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0/c9OADtxEgbp24WHUAtk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
