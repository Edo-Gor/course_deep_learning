{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le5MsLuthBqv"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 9.76\n",
    "#    L1 regularisation in practise\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waCpZrD-9UFQ"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glVG6q-KCLKx"
   },
   "outputs": [],
   "source": [
    "# %% Import Iris dataset\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Convert from pandas df to tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Species to numbers\n",
    "labels = torch.zeros(len(data),dtype=torch.long)\n",
    "labels[iris.species=='setosa']     = 0\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxDQFDRhCQJt"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 64\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xunhcNQ29_uY"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    # Architecture\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(4,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,3))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (no weight_decay option)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.005)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7806,
     "status": "ok",
     "timestamp": 1746809897447,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "fcliigO_9dh3",
    "outputId": "63f859e3-04d1-48fe-8a64-880dae2a778b"
   },
   "outputs": [],
   "source": [
    "# %% Explore model more in detail\n",
    "\n",
    "tmp_model = gen_model()[0]\n",
    "\n",
    "# Model architecture\n",
    "print(tmp_model)\n",
    "\n",
    "# Model's parameters\n",
    "for i in tmp_model.named_parameters():\n",
    "    print(i[0],i[1].shape,i[1].numel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO1gmB5t_eMD"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model(L1_lambda):\n",
    "\n",
    "    # Initialise accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Count total number\n",
    "    n_weights = 0\n",
    "    for pname,weights in ANN.named_parameters():\n",
    "        if 'bias' not in pname:\n",
    "            n_weights = n_weights + weights.numel()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Add L1 regularisation\n",
    "            L1_term = torch.tensor(0.,requires_grad=True)\n",
    "\n",
    "            # Sum up all abs weights\n",
    "            for pname,weight in ANN.named_parameters():\n",
    "                if 'bias' not in pname:\n",
    "                    L1_term = L1_term + torch.sum(torch.abs(weight))\n",
    "\n",
    "            # Add L1 penalty term to loss\n",
    "            loss = loss + (L1_lambda*L1_term)/n_weights\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        X,y = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append(  100*torch.mean((pred_labels==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sk8b9WTJHqcl"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "ANN,loss_fun,optimizer = gen_model()\n",
    "L1_lambda = 0.001\n",
    "train_acc,test_acc,losses = train_model(L1_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1746812371908,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "YCTAXuN3H9iK",
    "outputId": "5b71926c-2ae2-432b-f4f1-860ca6be0e5a"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(losses,'^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses with L1 $\\lambda$=' + str(L1_lambda))\n",
    "\n",
    "ax[1].plot(train_acc,'o-')\n",
    "ax[1].plot(test_acc,'s-')\n",
    "ax[1].set_title('Accuracy with L1 $\\lambda$=' + str(L1_lambda))\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "\n",
    "plt.savefig('figure27_weight_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure27_weight_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK66JR67IhAH"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qN2wOCZ6IkuE"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment on L1 lambda parameter\n",
    "\n",
    "# L1 parameters and output preallocation\n",
    "L1_lambdas = np.linspace(0,0.005,10)\n",
    "acc_train  = np.zeros((num_epochs,len(L1_lambdas)))\n",
    "acc_result = np.zeros((num_epochs,len(L1_lambdas)))\n",
    "\n",
    "# Loop over batch sizes\n",
    "for L1_i in range(len(L1_lambdas)):\n",
    "\n",
    "    # Generate and train model\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "    train_acc,test_acc,losses = train_model(L1_lambdas[L1_i])\n",
    "\n",
    "    # Store outputs\n",
    "    acc_train[:,L1_i]  = smooth_adaptive(train_acc,10)\n",
    "    acc_result[:,L1_i] = smooth_adaptive(test_acc,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1746816057903,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "LcqiNl1kJmGJ",
    "outputId": "dbf54b91-d9a3-47d8-fe73-fc877f2f6ce4"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(L1_lambdas)))\n",
    "for i in range(len(L1_lambdas)):\n",
    "    ax[0].plot(acc_train[:,i],color=cmaps[i])\n",
    "    ax[1].plot(acc_result[:,i],color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# make the legend easier to read\n",
    "leglabels = [np.round(i,4) for i in L1_lambdas]\n",
    "\n",
    "# common features\n",
    "for i in range(2):\n",
    "  ax[i].legend(leglabels)\n",
    "  ax[i].set_xlabel('Epoch')\n",
    "  ax[i].set_ylabel('Accuracy (%)')\n",
    "  ax[i].set_ylim([50,101])\n",
    "  ax[i].grid()\n",
    "\n",
    "\n",
    "plt.savefig('figure28_weight_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure28_weight_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1746816133872,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "GwpWJEFKMiLX",
    "outputId": "807fe333-3db4-4540-e858-9ff629839c2c"
   },
   "outputs": [],
   "source": [
    "# %% Show average accuracy by L1 rates\n",
    "\n",
    "# Pick only a range of epochs\n",
    "epoch_range = [160,360]\n",
    "\n",
    "plt.plot(L1_lambdas,\n",
    "         np.mean(acc_train[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'bo-',label='TRAIN')\n",
    "\n",
    "plt.plot(L1_lambdas,\n",
    "         np.mean(acc_result[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'rs-',label='TEST')\n",
    "\n",
    "plt.xlabel('L1 regularization amount')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Average accuracy by L1 regularization amount')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure29_weight_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure29_weight_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oR3aw8tdNYXq"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    In the previous video we used a pytorch function to implement L2 regularization, and in this video we implemented\n",
    "#    L1 regularization manually. Modify the code here to create a manual L2 regularizer.\n",
    "\n",
    "# Given the above code, this can be done relatively easily simply by squaring the\n",
    "# weights in the computation of the penalty term. Note that the variables in the\n",
    "# code below are still named L1_term/L1_lambda, but they are effectively an L2\n",
    "# penalty term; I'm just being lazy\n",
    "\n",
    "# %% Modified function\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model(L1_lambda):\n",
    "\n",
    "    # Initialise accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Count total number\n",
    "    n_weights = 0\n",
    "    for pname,weights in ANN.named_parameters():\n",
    "        if 'bias' not in pname:\n",
    "            n_weights = n_weights + weights.numel()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Add L1 regularisation\n",
    "            L1_term = torch.tensor(0.,requires_grad=True)\n",
    "\n",
    "            # Sum up all squared weights (.abs() not needed because of squaring)\n",
    "            for pname,weight in ANN.named_parameters():\n",
    "                if 'bias' not in pname:\n",
    "                    L1_term = L1_term + torch.sum(weight**2)\n",
    "\n",
    "            # Add L2 penalty term to loss\n",
    "            loss = loss + (L1_lambda*L1_term)/n_weights\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        X,y = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append(  100*torch.mean((pred_labels==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n",
    "\n",
    "# %% Modified parametric experiment on manual L2 lambda parameter\n",
    "\n",
    "# L2 parameters and output preallocation\n",
    "L1_lambdas = np.linspace(0,.1,10)\n",
    "acc_train  = np.zeros((num_epochs,len(L1_lambdas)))\n",
    "acc_result = np.zeros((num_epochs,len(L1_lambdas)))\n",
    "\n",
    "# Loop over batch sizes\n",
    "for L1_i in range(len(L1_lambdas)):\n",
    "\n",
    "    # Generate and train model\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "    train_acc,test_acc,losses = train_model(L1_lambdas[L1_i])\n",
    "\n",
    "    # Store outputs\n",
    "    acc_train[:,L1_i]  = smooth_adaptive(train_acc,10)\n",
    "    acc_result[:,L1_i] = smooth_adaptive(test_acc,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WteCaWqwNcVx"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Based on your modification above, create a combined L1+L2 regularizer. Does it make sense to use the same lambda\n",
    "#    parameter, or do you think it should be adjusted?\n",
    "\n",
    "# Given the above code, this can also be done relatively easily simply by adding\n",
    "# the L2 regularisation to the L1 regularisatiom already in place. The variables\n",
    "# are still named L1_term/L1_lambda, but they are effectively an L1+L2 penalty\n",
    "# term (elastic net); I'm just still lazy. The modified function, however, allows\n",
    "# to use different lambda values for L1 and L2, because for L2 regularisation, it\n",
    "# makes sense to have larger lambdas (due to the squaring)\n",
    "\n",
    "# %% Modified function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model(L1_lambda,L2_lambda):\n",
    "\n",
    "    # Initialise accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Count total number\n",
    "    n_weights = 0\n",
    "    for pname,weights in ANN.named_parameters():\n",
    "        if 'bias' not in pname:\n",
    "            n_weights = n_weights + weights.numel()\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Add L1 and L2 regularisation\n",
    "            L1_term = torch.tensor(0.,requires_grad=True)\n",
    "            L2_term = torch.tensor(0.,requires_grad=True)\n",
    "\n",
    "            # Sum up all abs weights\n",
    "            for pname,weight in ANN.named_parameters():\n",
    "                if 'bias' not in pname:\n",
    "                    L1_term = L1_term + torch.sum(torch.abs(weight))\n",
    "                    L2_term = L2_term + torch.sum(weight**2)\n",
    "\n",
    "            # Add L1 penalty term to loss\n",
    "            loss = loss + (L1_lambda*L1_term)/n_weights + (L2_lambda*L2_term)/n_weights\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        X,y = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append(  100*torch.mean((pred_labels==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n",
    "\n",
    "# %% Modified parametric experiment on manual L2+L1 lambda parameter\n",
    "\n",
    "# L1 and L2 parameters and output preallocation\n",
    "L1_lambdas = np.linspace(0,0.005,10)\n",
    "L2_lambdas = np.linspace(0,.1,10)\n",
    "acc_train  = np.zeros((num_epochs,len(L1_lambdas)))\n",
    "acc_result = np.zeros((num_epochs,len(L1_lambdas)))\n",
    "\n",
    "# Loop over batch sizes\n",
    "for L1_i in range(len(L1_lambdas)):\n",
    "\n",
    "    # Generate and train model\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "    train_acc,test_acc,losses = train_model(L1_lambdas[L1_i],L2_lambdas[L1_i])\n",
    "\n",
    "    # Store outputs\n",
    "    acc_train[:,L1_i]  = smooth_adaptive(train_acc,10)\n",
    "    acc_result[:,L1_i] = smooth_adaptive(test_acc,10)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMyb/+FfidZJPV/EbPLLFn8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
