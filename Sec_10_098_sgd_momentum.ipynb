{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yveCkLJIDJYR"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 10.98\n",
    "#    SGD with momentum\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD2FwLtjDeQv"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E13x3iXLksd"
   },
   "outputs": [],
   "source": [
    "# %% Create data\n",
    "\n",
    "# General params\n",
    "n_by_clust = 300\n",
    "blurring   = 1\n",
    "\n",
    "# Centroids\n",
    "A = [1,1]\n",
    "B = [5,1]\n",
    "C = [4,4]\n",
    "\n",
    "# Generate data\n",
    "a = [ A[0]+np.random.randn(n_by_clust)*blurring, A[1]+np.random.randn(n_by_clust)*blurring ]\n",
    "b = [ B[0]+np.random.randn(n_by_clust)*blurring, B[1]+np.random.randn(n_by_clust)*blurring ]\n",
    "c = [ C[0]+np.random.randn(n_by_clust)*blurring, C[1]+np.random.randn(n_by_clust)*blurring ]\n",
    "\n",
    "# Labels\n",
    "labels_np = np.hstack(( np.zeros((n_by_clust)),\n",
    "                        np.ones((n_by_clust)),\n",
    "                        2*np.ones((n_by_clust)) ))\n",
    "\n",
    "# Data matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# Data into PyTorch tensors (long format for CCE)\n",
    "data   = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1748809898954,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "PZtNc2p0MHIs",
    "outputId": "017e9eef-db1a-4b5d-e0c0-bc5c117821ff"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'s',alpha=.75)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'o',alpha=.75)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'^',alpha=.75)\n",
    "\n",
    "plt.title('Some clusters')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig('figure69_sgd_momentum.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure69_sgd_momentum.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeKupuRqMXWL"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.1)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vp2y6xFMMfsL"
   },
   "outputs": [],
   "source": [
    "# %% Create the model\n",
    "\n",
    "def gen_model(momentum):\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(2,8)\n",
    "            self.hid1   = nn.Linear(8,8)\n",
    "            self.output = nn.Linear(8,3)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid1(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Model instance\n",
    "    ANN = model()\n",
    "\n",
    "    # Loss function and optimizer (note the extra input into optimizer)\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01,momentum=momentum)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748812246037,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "IDGEPCweM1un",
    "outputId": "74418267-14de-4bff-d5f7-88649c00a7a7"
   },
   "outputs": [],
   "source": [
    "# %% Test momentum\n",
    "\n",
    "optim = gen_model(.9)[2]\n",
    "print(optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2PfHUWfNNiN"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Epochs\n",
    "num_epochs = 50\n",
    "\n",
    "def train_model(momentum):\n",
    "\n",
    "    # Model instance\n",
    "    ANN,loss_fun,optimizer = gen_model(momentum)\n",
    "\n",
    "    # Initialise\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Epochs loop\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Train mode on\n",
    "        ANN.train()\n",
    "\n",
    "        # Initialise and loop over batches\n",
    "        batch_losses = []\n",
    "        batch_acc    = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute loss and accuracy from this batch\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y  # booleans\n",
    "            matches_num = matches.float()                 # convert to numbers\n",
    "            acc_percent = 100*torch.mean(matches_num)     # average and percent\n",
    "            batch_acc.append(acc_percent)\n",
    "\n",
    "        # Average train accuracy and losses from batches\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_losses))\n",
    "\n",
    "        # Test accuracy (turn autograd off)\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZrV0j13P3jR"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AictBCNMNeIL"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over momenta\n",
    "\n",
    "momenta = [0,0.5,0.9,0.95,0.999]\n",
    "results = np.zeros((num_epochs,len(momenta),3))\n",
    "\n",
    "# Test all momentum vals on the same data\n",
    "for i,momentum in enumerate(momenta):\n",
    "    train_acc,test_acc,losses,ANN = train_model(momentum)\n",
    "    results[:,i,0] = smooth_adaptive(train_acc,5)\n",
    "    results[:,i,1] = smooth_adaptive(test_acc,5)\n",
    "    results[:,i,2] = smooth_adaptive(losses,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 835,
     "status": "ok",
     "timestamp": 1748812432374,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "fCZIWc15OXvM",
    "outputId": "c0e113cf-4379-426f-b425-32447e0b2c34"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "# Notice how momenta between 0.9 and 0.95 tend to produce the better learning\n",
    "# process; lower values are still fine but slower, but higher (~1) make the\n",
    "# learning collapse\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(1.5*6*phi,6))\n",
    "\n",
    "for i in range(3):\n",
    "  ax[i].plot(results[:,:,i])\n",
    "  ax[i].legend(momenta)\n",
    "  ax[i].set_xlabel('Epochs')\n",
    "  ax[i].grid()\n",
    "\n",
    "  if i==0 or i==1:\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].set_ylim([20,100])\n",
    "  else:\n",
    "    ax[i].set_ylabel('Loss')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[1].set_title('Test')\n",
    "ax[2].set_title('Losses')\n",
    "ax[2].set_ylim([0,2])\n",
    "\n",
    "plt.savefig('figure70_sgd_momentum.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure70_sgd_momentum.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 28886,
     "status": "ok",
     "timestamp": 1748812154308,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "gSS2LoSwRDjM",
    "outputId": "f5b2f84f-be0e-4502-fea4-e89624f55e74"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Now that you see the results across a broad range of beta (momentum) parameters, try re-running the experiment\n",
    "#    using a narrower range. For example, you don't need to test b=0 or b=.999.\n",
    "\n",
    "# Indeed there is a general improvement for higher momenta, if ranging between\n",
    "# 0.5 and 0.95\n",
    "\n",
    "# Re-run\n",
    "momenta = np.arange(0.55,0.96,0.05)\n",
    "results = np.zeros((num_epochs,len(momenta),3))\n",
    "\n",
    "for i,momentum in enumerate(momenta):\n",
    "    train_acc,test_acc,losses,ANN = train_model(momentum)\n",
    "    results[:,i,0] = smooth_adaptive(train_acc,5)\n",
    "    results[:,i,1] = smooth_adaptive(test_acc,5)\n",
    "    results[:,i,2] = smooth_adaptive(losses,5)\n",
    "\n",
    "# Re-plot\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(1.5*6*phi,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(momenta)))\n",
    "for j in range(len(momenta)):\n",
    "    for i in range(3):\n",
    "        ax[i].plot(results[:,j,i], color=cmaps[j], alpha=0.8, label=f\"{momenta[j]:.2f}\")\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].legend(title='Momenta',fontsize=9)\n",
    "    ax[i].set_xlabel('Epochs')\n",
    "    ax[i].grid()\n",
    "\n",
    "    if i==0 or i==1:\n",
    "        ax[i].set_ylabel('Accuracy (%)')\n",
    "        ax[i].set_ylim([20,100])\n",
    "    else:\n",
    "        ax[i].set_ylabel('Loss')\n",
    "\n",
    "ax[0].set_title('Train')\n",
    "ax[1].set_title('Test')\n",
    "ax[2].set_title('Losses')\n",
    "ax[2].set_ylim([0,2])\n",
    "\n",
    "plt.savefig('figure71_sgd_momentum_extra1.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure71_sgd_momentum_extra1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0VMJ5egRe2i"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    The beta parameter multiplies the learning rate in the computation (see formula in slides). That means that these\n",
    "#    results will interact with the learning rate. Repeat the experiment using a different learning rate.\n",
    "\n",
    "# Trying with lr = 0.001 and lr = 0.1; with a smaller lr, the gradual effect of\n",
    "# higher momenta becomes more evident, while with a smaller lr, the effect is\n",
    "# reversed and the moementum is actually detrimental (i.e., the basic SGD seems\n",
    "# more stable); not sure how to explain this to myself, beside that bigger steps\n",
    "# might interact with the momentum and make the gradient jump here and there\n",
    "# without settling in one minimum (?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTT5hSBGX83t"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    If you wanted to test the relationship between momentum and learning rate in a full parametric experiment, how would\n",
    "#    you set it up? Would you store the loss/accuracy over all epochs?\n",
    "\n",
    "# One can slightly modify the functions to generate and train the model, and then\n",
    "# running a nested loop to go over changing lrs and momenta. One technically does\n",
    "# not need the loss/accuracy over all epochs for visualisation (only the final\n",
    "# accuracies and losses). See code below.\n",
    "# As for the interpretation, one can see that there are two main 'corners' of the\n",
    "# parameters space, where the (final) accuracy/losses are lower/higher: 1) high\n",
    "# lr and large momentum, and 2) low lr and small momentum. Everything in-between\n",
    "# looks like a fairly happy parameters flatland.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kYVuPzhYI4o"
   },
   "outputs": [],
   "source": [
    "# %% Create the model\n",
    "\n",
    "def gen_model(momentum,lr):\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(2,8)\n",
    "            self.hid1   = nn.Linear(8,8)\n",
    "            self.output = nn.Linear(8,3)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid1(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Model instance\n",
    "    ANN = model()\n",
    "\n",
    "    # Loss function and optimizer (note the extra input into optimizer)\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=lr,momentum=momentum)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX47uej0YKHM"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Epochs\n",
    "num_epochs = 50\n",
    "\n",
    "def train_model(momentum,lr):\n",
    "\n",
    "    # Model instance\n",
    "    ANN,loss_fun,optimizer = gen_model(momentum,lr)\n",
    "\n",
    "    # Initialise\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Epochs loop\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Train mode on\n",
    "        ANN.train()\n",
    "\n",
    "        # Initialise and loop over batches\n",
    "        batch_losses = []\n",
    "        batch_acc    = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute loss and accuracy from this batch\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y  # booleans\n",
    "            matches_num = matches.float()                 # convert to numbers\n",
    "            acc_percent = 100*torch.mean(matches_num)     # average and percent\n",
    "            batch_acc.append(acc_percent)\n",
    "\n",
    "        # Average train accuracy and losses from batches\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_losses))\n",
    "\n",
    "        # Test accuracy (turn autograd off)\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 517182,
     "status": "ok",
     "timestamp": 1748814049478,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "THjVODTeYTjc",
    "outputId": "a05fafd6-c509-4923-9da6-10da323fb070"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over momenta and learning rates\n",
    "\n",
    "# Takes ~8 mins\n",
    "momenta = np.arange(0.55,0.96,0.05)\n",
    "lrs     = np.logspace(-3,-1,20)\n",
    "results = np.zeros((num_epochs,len(momenta),len(lrs),3))\n",
    "\n",
    "for i,momentum in enumerate(momenta):\n",
    "    for j,lr in enumerate(lrs):\n",
    "        train_acc,test_acc,losses,ANN = train_model(momentum,lr)\n",
    "        results[:,i,j,0] = train_acc\n",
    "        results[:,i,j,1] = test_acc\n",
    "        results[:,i,j,2] = losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1748815311358,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "hjQyi2I8Yhau",
    "outputId": "e573577b-8c66-4b80-aca8-942c7c3d58f9"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib           import cm\n",
    "\n",
    "# Meshgrid of momenta and learning rates\n",
    "M, LR = np.meshgrid(momenta, lrs, indexing='ij')\n",
    "\n",
    "# Get final epoch values\n",
    "final_train = results[-1, :, :, 0]\n",
    "final_test  = results[-1, :, :, 1]\n",
    "final_loss  = results[-1, :, :, 2]\n",
    "\n",
    "# Plot\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(1.5*6*phi, 6))\n",
    "\n",
    "titles = ['Final Train Accuracy','Final Test Accuracy','Final Loss']\n",
    "data   = [final_train,final_test,final_loss]\n",
    "zlabels = ['Accuracy (%)','Accuracy (%)','Losses']\n",
    "\n",
    "for i in range(3):\n",
    "    ax   = fig.add_subplot(1,3,i+1,projection='3d')\n",
    "    surf = ax.plot_surface(M,LR,data[i],cmap='plasma',edgecolors='none',linewidth=0,antialiased=True,alpha=0.9)\n",
    "\n",
    "    ax.set_xlabel('Momentum')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_zlabel(zlabels[i])\n",
    "    ax.set_title(titles[i])\n",
    "    fig.colorbar(surf,ax=ax,shrink=0.4,aspect=10,pad=0.2)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('figure74_sgd_momentum_extra3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure74_sgd_momentum_extra3.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1748816102203,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "w72jyRSKhI9i",
    "outputId": "548cdb0e-962d-4870-a5d2-7197fb1d25f5"
   },
   "outputs": [],
   "source": [
    "# %% More plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(1.75*6*phi,6))\n",
    "\n",
    "titles = [\"Final Train Accuracy\", \"Final Test Accuracy\", \"Final Loss\"]\n",
    "data   = [final_train, final_test, final_loss]\n",
    "cmap   = \"plasma\"\n",
    "\n",
    "for i,ax in enumerate(ax):\n",
    "\n",
    "    if i < 2:\n",
    "        im = ax.imshow(\n",
    "            data[i],\n",
    "            origin='lower',\n",
    "            cmap='plasma',\n",
    "            interpolation='bicubic',\n",
    "            aspect='auto',\n",
    "            vmin=30,\n",
    "            vmax=100)\n",
    "\n",
    "    else:\n",
    "        im = ax.imshow(\n",
    "            data[i],\n",
    "            origin='lower',\n",
    "            cmap='plasma',\n",
    "            interpolation='bicubic',\n",
    "            aspect='auto')\n",
    "\n",
    "    ax.set_xticks(np.arange(len(lrs)))\n",
    "    ax.set_xticklabels([f\"{lr:.3f}\" for lr in lrs], rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(np.arange(len(momenta)))\n",
    "    ax.set_yticklabels([f\"{m:.2f}\" for m in momenta])\n",
    "\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_ylabel(\"Momentum\")\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "    cbar = fig.colorbar(im,ax=ax,shrink=0.6,pad=0.03)\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure75_sgd_momentum_extra3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure75_sgd_momentum_extra3.png')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPTTu1TbLaxf1OuVDcFbOF+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
