{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yveCkLJIDJYR"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 10.87\n",
    "#    Batch normalisation in practice\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD2FwLtjDeQv"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1748199247600,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "RU9j18lEDqcA",
    "outputId": "8074f2ae-3b6c-4d0d-eeb2-66b3cfee5917"
   },
   "outputs": [],
   "source": [
    "# %% Load and prepare data\n",
    "\n",
    "# Load\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url,sep=';')\n",
    "\n",
    "# Remove some outliers (see lec. 82 for why)\n",
    "data = data[data['total sulfur dioxide']<200]\n",
    "\n",
    "# Z-score all the variables but quality\n",
    "cols2zscore = data.keys()\n",
    "cols2zscore = cols2zscore.drop('quality')\n",
    "\n",
    "for col in cols2zscore:\n",
    "    mean_val  = np.mean(data[col])\n",
    "    std_val   = np.std(data[col])\n",
    "    data[col] = (data[col] - mean_val) / std_val\n",
    "\n",
    "# Binarise quality\n",
    "data.loc[:,'boolean_quality'] = 0\n",
    "data.loc[data['quality']>5, 'boolean_quality'] = 1\n",
    "data.loc[data['quality']<6, 'boolean_quality'] = 0 # Implicit but here for clarity\n",
    "\n",
    "# Convert from pandas dataframe to PyTorch tensor\n",
    "data_t = torch.tensor( data[cols2zscore].values ).float()\n",
    "labels = torch.tensor( data['boolean_quality'].values ).float()\n",
    "\n",
    "print(f'Data shape: {data_t.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "# Labels need to be multidimentional for PyTorch, not an array, and need to be long integers too\n",
    "labels = labels[:,None]\n",
    "print(f'Proper labels shape: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPWb5p3-D4OG"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_t,labels,test_size=0.1)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUyGwdI8D_XH"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "class ANN_with_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Architecture and batch normalisation\n",
    "        self.input  = nn.Linear(11,16)\n",
    "\n",
    "        self.hid1   = nn.Linear(16,32)\n",
    "        self.bn1    = nn.BatchNorm1d(16)\n",
    "        self.hid2   = nn.Linear(32,20)\n",
    "        self.bn2    = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.output = nn.Linear(20,1)\n",
    "\n",
    "    # Forward propagation (boolean arg to switch batchnorm on and off)\n",
    "    def forward(self,x,doBN):\n",
    "\n",
    "        # Input (already normalised if data normalised)\n",
    "        x = F.relu( self.input(x) )\n",
    "\n",
    "        # Hidden layers (batchnorm, weighted sum, activation function)\n",
    "        if doBN:\n",
    "\n",
    "            x = self.bn1(x)\n",
    "            x = self.hid1(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = self.bn2(x)\n",
    "            x = self.hid2(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        else:\n",
    "\n",
    "            x = self.hid1( F.relu(x) )\n",
    "            x = self.hid2( F.relu(x) )\n",
    "\n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-9Dfg7InCl6"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model(doBN=True):\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_fun  = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    # Initialise losses\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Switch training mode on\n",
    "        ANN.train()\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss (with batchnorm arg)\n",
    "            yHat = ANN(X,doBN)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X,doBN)\n",
    "        test_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0BHgILGn5UT"
   },
   "outputs": [],
   "source": [
    "# %% Test the model\n",
    "\n",
    "# Create model instance and train, with and without batch normalisation (takes ~3 mins)\n",
    "ANN = ANN_with_BN()\n",
    "train_acc_BN,test_acc_BN,losses_BN = train_model(True)\n",
    "\n",
    "ANN = ANN_with_BN()\n",
    "train_acc_noBN,test_acc_noBN,losses_noBN = train_model(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4DH_IMgp6q6"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1748201002924,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "2HKQCZUJoYMj",
    "outputId": "b82497cb-16a1-4ff7-b078-884f8691ee67"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(17,5))\n",
    "\n",
    "ax[0].plot(smooth_adaptive(losses_BN,20),label='With batchnorm')\n",
    "ax[0].plot(smooth_adaptive(losses_noBN,20),label='Without batchnorm')\n",
    "ax[0].set_title('Losses')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(smooth_adaptive(train_acc_BN,20),label='With batchnorm')\n",
    "ax[1].plot(smooth_adaptive(train_acc_noBN,20),label='Without batchnorm')\n",
    "ax[1].set_title('Train accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(smooth_adaptive(test_acc_BN,20),label='With batchnorm')\n",
    "ax[2].plot(smooth_adaptive(test_acc_noBN,20),label='Without batchnorm')\n",
    "ax[2].set_title('Test accuracy')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.savefig('figure18_batch_normalisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure18_batch_normalisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A_EJ4NwrKcx"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    BatchNorm1d takes several additional inputs, including one called \"momentum.\" You will learn what this parameter\n",
    "#    means later in this section, but briefly: it is a smoothing parameter that helps stabilize and improve learning\n",
    "#    weights. The default value is .1. What happens when you change it to .001? How about setting it to zero?\n",
    "\n",
    "# Not sure about the meaning of this parameter, but the test accuracy collapses\n",
    "# when set to 0\n",
    "\n",
    "# %% Modified model class\n",
    "\n",
    "class ANN_with_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Architecture and batch normalisation\n",
    "        self.input  = nn.Linear(11,16)\n",
    "\n",
    "        self.hid1   = nn.Linear(16,32)\n",
    "        self.bn1    = nn.BatchNorm1d(16,momentum=0.001)\n",
    "        self.hid2   = nn.Linear(32,20)\n",
    "        self.bn2    = nn.BatchNorm1d(32,momentum=0.001)\n",
    "\n",
    "        self.output = nn.Linear(20,1)\n",
    "\n",
    "    # Forward propagation (boolean arg to switch batchnorm on and off)\n",
    "    def forward(self,x,doBN):\n",
    "\n",
    "        # Input (already normalised if data normalised)\n",
    "        x = F.relu( self.input(x) )\n",
    "\n",
    "        # Hidden layers (batchnorm, weighted sum, activation function)\n",
    "        if doBN:\n",
    "\n",
    "            x = self.bn1(x)\n",
    "            x = self.hid1(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            x = self.bn2(x)\n",
    "            x = self.hid2(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        else:\n",
    "\n",
    "            x = self.hid1( F.relu(x) )\n",
    "            x = self.hid2( F.relu(x) )\n",
    "\n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwP50tt2rREh"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    In the lecture, I said to apply batchnorm to the data *before* applying the nonlinearity (here, relu). This is also\n",
    "#    implemented in the code above. However, this is discussed in the field and online, and not everyone agrees. Modify\n",
    "#    the code to apply batch normalization *after* applying relu. Does that make a big difference? Thinking about the\n",
    "#    math, which order makes more sense to you? (Don't worry, it's OK to disagree with me!)\n",
    "\n",
    "# No substantial differences in the performance of the model\n",
    "\n",
    "# %% Modified model class\n",
    "\n",
    "class ANN_with_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Architecture and batch normalisation\n",
    "        self.input  = nn.Linear(11,16)\n",
    "\n",
    "        self.hid1   = nn.Linear(16,32)\n",
    "        self.bn1    = nn.BatchNorm1d(32)\n",
    "        self.hid2   = nn.Linear(32,20)\n",
    "        self.bn2    = nn.BatchNorm1d(20)\n",
    "\n",
    "        self.output = nn.Linear(20,1)\n",
    "\n",
    "    # Forward propagation (boolean arg to switch batchnorm on and off)\n",
    "    def forward(self,x,doBN):\n",
    "\n",
    "        # Input (already normalised if data normalised)\n",
    "        x = F.relu( self.input(x) )\n",
    "\n",
    "        # Hidden layers (batchnorm, weighted sum, activation function)\n",
    "        if doBN:\n",
    "\n",
    "            x = self.hid1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.bn1(x)\n",
    "\n",
    "            x = self.hid2(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.bn2(x)\n",
    "\n",
    "        else:\n",
    "\n",
    "            x = self.hid1( F.relu(x) )\n",
    "            x = self.hid2( F.relu(x) )\n",
    "\n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMVnoZ/CdAFp1TtIAlkr7Lv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
