{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hfGCkQTBmTu"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 6.36\n",
    "#    Parametric experiments on gradient descent\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdkrq0raBv8R"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import sympy               as sym\n",
    "import copy\n",
    "\n",
    "from google.colab                     import files\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 493,
     "status": "ok",
     "timestamp": 1740939134746,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "wyzLmZ4UC2m3",
    "outputId": "dd6c05f4-cc59-4225-c022-9869209ea24f"
   },
   "outputs": [],
   "source": [
    "# %% Function\n",
    "\n",
    "# Function and derivative\n",
    "x  = np.linspace(-2*np.pi,2*np.pi,401)\n",
    "fx = np.sin(x)*np.exp(-x**2*.05)\n",
    "df = np.cos(x)*np.exp(-x**2*.05) + np.sin(x)*(-.1*x)*np.exp(-x**2*.05)\n",
    "\n",
    "# Quick inspection\n",
    "plt.plot(x,fx,x,df)\n",
    "plt.legend(['f(x)',\"f'(x)\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUI_JGWhD03g"
   },
   "outputs": [],
   "source": [
    "# %% Define Python functions for ease\n",
    "\n",
    "def fx(x):\n",
    "    return np.sin(x)*np.exp(-x**2*.05)\n",
    "\n",
    "def df(x):\n",
    "    return np.cos(x)*np.exp(-x**2*.05) + np.sin(x)*(-.1*x)*np.exp(-x**2*.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1740940727286,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "1I-tywo6Bvls",
    "outputId": "b7cf8d3f-ebc9-43bc-91a2-6247250cb66c"
   },
   "outputs": [],
   "source": [
    "# %% Gradient descent\n",
    "\n",
    "# Gradient descent\n",
    "local_min    = np.random.choice(x,1)\n",
    "learn_rate   = .01\n",
    "train_epochs = 1000\n",
    "\n",
    "for i in range(train_epochs):\n",
    "    gradient  = df(local_min)\n",
    "    local_min = local_min - gradient*learn_rate\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x,fx(x),x,df(x),'--')\n",
    "plt.plot(local_min,df(local_min),'ro')\n",
    "plt.plot(local_min,fx(local_min),'ro')\n",
    "\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(['f(x)',\"f'(x)\"])\n",
    "plt.title('Empirical local minimum: %s' %round(local_min[0],4))\n",
    "\n",
    "plt.savefig('figure33_parametric_experiment_0.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure33_parametric_experiment_0.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1740940822659,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "PiuVaKpeGsCs",
    "outputId": "a660a4e2-46d7-4724-c522-10ed350d16e2"
   },
   "outputs": [],
   "source": [
    "# %% Experiment 1\n",
    "#    Manipulate uniquely the initial value\n",
    "\n",
    "# Parameters\n",
    "start_loc = np.linspace(-5,5,50)\n",
    "final_res = np.zeros(len(start_loc))\n",
    "\n",
    "# Loop over starting point\n",
    "for idx,local_min in enumerate(start_loc):\n",
    "\n",
    "    for i in range(train_epochs):\n",
    "        gradient  = df(local_min)\n",
    "        local_min = local_min - gradient*learn_rate\n",
    "\n",
    "    final_res[idx] = local_min\n",
    "\n",
    "# Plotting\n",
    "plt.plot(start_loc,final_res,'s-')\n",
    "plt.xlabel('Starting point')\n",
    "plt.ylabel('Final guess')\n",
    "plt.suptitle('Minima identified by gradient descent for a range of initial values')\n",
    "plt.title('True global min: x = -1.4289')\n",
    "\n",
    "plt.savefig('figure34_parametric_experiment_1.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure34_parametric_experiment_1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1740943010991,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "ZDsiFV69Ca5X",
    "outputId": "971778ca-8ef5-4f1b-8047-9f381315ec23"
   },
   "outputs": [],
   "source": [
    "# %% Experiment 2\n",
    "#    Manipulate uniquely the learning rate\n",
    "\n",
    "# Parameters\n",
    "learn_rate = np.linspace(1e-10,1e-1,50)\n",
    "final_res  = np.zeros(len(learn_rate))\n",
    "\n",
    "# Loop over learning rates (keep starting point fixed at 0 for this experiment)\n",
    "for idx,learnRate in enumerate(learn_rate):\n",
    "    local_min = 0\n",
    "\n",
    "    # Run through training\n",
    "    for i in range(train_epochs):\n",
    "        gradient = df(local_min)\n",
    "        local_min = local_min - gradient*learnRate\n",
    "\n",
    "    final_res[idx] = local_min\n",
    "\n",
    "# Plotting\n",
    "plt.plot(learn_rate,final_res,'s-')\n",
    "plt.xlabel('Learning rates')\n",
    "plt.ylabel('Final guess')\n",
    "plt.suptitle('Minima identified by gradient descent for a range of learning rates ')\n",
    "plt.title('True global min: x = -1.4289, fixed initial location = 0')\n",
    "\n",
    "plt.savefig('figure35_parametric_experiment_2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure35_parametric_experiment_2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4my2tAhOCfV5"
   },
   "outputs": [],
   "source": [
    "# %% Experiment 3\n",
    "#    Manipulate the learning rate and the training epochs together\n",
    "\n",
    "# Parameters\n",
    "learn_rate   = np.linspace(1e-10,1e-1,50)\n",
    "train_epochs = np.round(np.linspace(10,500,40))\n",
    "final_res    = np.zeros((len(learn_rate),len(train_epochs)))\n",
    "\n",
    "# Loop over learning rates\n",
    "for Lidx,learnRate in enumerate(learn_rate):\n",
    "\n",
    "    # Loop over training epochs (keep starting point fixed at 0 for this experiment as well)\n",
    "    for Tidx,trainEpochs in enumerate(train_epochs):\n",
    "        local_min = 0\n",
    "\n",
    "        for i in range(int(trainEpochs)):\n",
    "            gradient = df(local_min)\n",
    "            local_min = local_min - gradient*learnRate\n",
    "\n",
    "        final_res[Lidx,Tidx] = local_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "executionInfo": {
     "elapsed": 1019,
     "status": "ok",
     "timestamp": 1740946048094,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "g7ehxdvhVN4-",
    "outputId": "1e2cc318-4947-411d-fcf7-ca2564a5739f"
   },
   "outputs": [],
   "source": [
    "# %% Experiment 3\n",
    "#    Plotting\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "plt.imshow(final_res,extent=[learn_rate[0],learn_rate[-1],train_epochs[0],train_epochs[-1]],\n",
    "           aspect='auto',origin='lower',vmin=-1.45,vmax=-1.2,cmap='jet')\n",
    "plt.xlabel('Learning rate')\n",
    "plt.ylabel('Training epochs')\n",
    "plt.suptitle('Final guess by manipulating learning rate and epochs number')\n",
    "plt.title('#True global min: x = -1.4289')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.savefig('figure36_parametric_experiment_3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure36_parametric_experiment_3.png')\n",
    "\n",
    "# Another visualization\n",
    "\n",
    "plt.plot(learn_rate,final_res)\n",
    "plt.xlabel('Learning rates')\n",
    "plt.ylabel('Final function estimate')\n",
    "plt.title('Each line is a training epochs N')\n",
    "\n",
    "plt.savefig('figure37_parametric_experiment_3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure37_parametric_experiment_3.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyATQYU7WzTQ"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    In experiment 3, set the starting location to be 1.6. Re-run the experiment and the image. You'll need to re-adjust\n",
    "#    the figure color limits; check the line plots at the top of the code to determine a useful color range. Does the new\n",
    "#    starting value change your conclusions about the interaction between learning rate and training epochs?\n",
    "\n",
    "# Not really, a similar relationship is seen also for a local minima; however, since the initial point for the gradient\n",
    "# descent is more far away from the minimum than using x = 0, then the 'suboptimal area' for epochs and leraninig rates is much wider\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1adAQdTOW3gw"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    In the same experiment, now change the starting location to be random (use code: np.random.choice(x,1)). How do these\n",
    "#    results look? Are you surprised? Are the results of this experiment still interpretable and what does this tell you\n",
    "#    about running experiments in DL?\n",
    "\n",
    "# This is a quite nice one. It makes sense that by introducing randomness in the starting point the structure of the\n",
    "# whole matrix falls (almost entirely) apart. It is however interesting to note that for high numbers of epochs and high numbers\n",
    "# of learning rate, the values still (mostly) settle at either the global minimum (~x=-1.43) or the local minimum (~x=4.4), while\n",
    "# in the areas of low values (more visible when keeping a fixed starting point as a sort of hyperbola), the gradient descent tends\n",
    "# to get stuck at more heterogeneous locations.\n",
    "# So, it's harder to see, but there is still some structure left; that said, maybe it's better not to vary too many variables when doing\n",
    "# parametric experiments\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyvYEIPL6foy3bp2KZWi/K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
