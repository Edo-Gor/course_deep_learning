{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxLsu_5_Ao80"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 10.103\n",
    "#    Learning rate decay\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFOW2iaUArr4"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6qvfXGxp4UM"
   },
   "outputs": [],
   "source": [
    "# %% Reminder\n",
    "\n",
    "# As mentioned before, one way to adjust the leraning rate is through the so-called\n",
    "# learning rate decay: in this case lr is set to be proportinal to the training epoch.\n",
    "# It's unrelated to the actual model performance (unlike Adam or RMSprop), but it\n",
    "# also works fine sometimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlPJL1_Xqt6h"
   },
   "outputs": [],
   "source": [
    "# %% Create data\n",
    "\n",
    "# General params\n",
    "n_by_clust = 300\n",
    "blurring   = 1\n",
    "\n",
    "# Centroids\n",
    "A = [1,1]\n",
    "B = [5,1]\n",
    "C = [4,4]\n",
    "\n",
    "# Generate data\n",
    "a = [ A[0]+np.random.randn(n_by_clust)*blurring, A[1]+np.random.randn(n_by_clust)*blurring ]\n",
    "b = [ B[0]+np.random.randn(n_by_clust)*blurring, B[1]+np.random.randn(n_by_clust)*blurring ]\n",
    "c = [ C[0]+np.random.randn(n_by_clust)*blurring, C[1]+np.random.randn(n_by_clust)*blurring ]\n",
    "\n",
    "# Labels\n",
    "labels_np = np.hstack(( np.zeros((n_by_clust)),\n",
    "                        np.ones((n_by_clust)),\n",
    "                        2*np.ones((n_by_clust)) ))\n",
    "\n",
    "# Data matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# Data into PyTorch tensors (long format for CCE)\n",
    "data   = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 724,
     "status": "ok",
     "timestamp": 1749220605026,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "0GYaZqRjq5O1",
    "outputId": "a6d109e9-4412-428a-c97d-e30a64e83ca9"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'s',alpha=.75)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'o',alpha=.75)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'^',alpha=.75)\n",
    "\n",
    "plt.title('Some clusters')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig('figure96_leraning_rate_decay.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure96_leraning_rate_decay.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPbKX5UnqzPu"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.1)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QE-T3J1jrHBQ"
   },
   "outputs": [],
   "source": [
    "# %% Create the model\n",
    "\n",
    "def gen_model(initial_learning_rate):\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(2,8)\n",
    "            self.hid1   = nn.Linear(8,8)\n",
    "            self.output = nn.Linear(8,3)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid1(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Model instance\n",
    "    ANN = model()\n",
    "\n",
    "    # Loss function and optimizer (set learning rate decay)\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=initial_learning_rate)\n",
    "\n",
    "    step_size = batch_size*len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=.5)\n",
    "\n",
    "    return ANN,loss_fun,optimizer,scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749221310853,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "fdXuVjR9roPQ",
    "outputId": "65286bbe-13ac-43f1-d42e-c1c49a98f520"
   },
   "outputs": [],
   "source": [
    "# %% How many steps until the learning rate changes?\n",
    "\n",
    "# Same number as training datapoints\n",
    "len(train_loader)*batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1749221173951,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "vaTaRKUVr4AI",
    "outputId": "27da51cb-8c2d-4aa3-c1e9-bef6816a0e4a"
   },
   "outputs": [],
   "source": [
    "# %% Explore learning rate decay parameter\n",
    "\n",
    "# Create network\n",
    "ANN = gen_model(0.01)[0]\n",
    "\n",
    "# Fresh optimizer\n",
    "optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=.5)\n",
    "\n",
    "# Test change in learning rate\n",
    "for epoch_i in range(3):\n",
    "    for batch_num in range(10):\n",
    "\n",
    "        print(f'Batch {batch_num+1}, epoch {epoch_i}: LR = {scheduler.get_last_lr()[0]}')\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJqrCefOtG7S"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model(initial_learning_rate,toggle_lr_decay):\n",
    "\n",
    "    # Epochs\n",
    "    num_epochs = 50\n",
    "\n",
    "    # Model instance\n",
    "    ANN,loss_fun,optimizer,scheduler = gen_model(initial_learning_rate)\n",
    "\n",
    "    # Initialise\n",
    "    losses     = []\n",
    "    train_acc  = []\n",
    "    test_acc   = []\n",
    "    current_LR = []\n",
    "\n",
    "    # Epochs loop\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Train mode on\n",
    "        ANN.train()\n",
    "\n",
    "        # Initialise and loop over batches\n",
    "        batch_losses = []\n",
    "        batch_acc    = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Toggle learning rate decay (after optimizer step)\n",
    "            if toggle_lr_decay:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Compute loss and accuracy from this batch\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y  # booleans\n",
    "            matches_num = matches.float()                 # convert to numbers\n",
    "            acc_percent = 100*torch.mean(matches_num)     # average and percent\n",
    "            batch_acc.append(acc_percent)\n",
    "\n",
    "            current_LR.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "        # Average train accuracy and losses from batches\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_losses))\n",
    "\n",
    "        # Test accuracy (turn autograd off)\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN,current_LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "executionInfo": {
     "elapsed": 8537,
     "status": "ok",
     "timestamp": 1749223042356,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "VD0XMtSlu0iM",
    "outputId": "216d3bbf-4489-4ed2-f7a9-ce3cb94d4ebf"
   },
   "outputs": [],
   "source": [
    "# %% Sanity check\n",
    "\n",
    "# On\n",
    "train_acc,test_acc,losses,ANN,current_LR = train_model(0.01,True)\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(4*phi,4))\n",
    "\n",
    "plt.plot(current_LR)\n",
    "plt.title('Learning rate should change')\n",
    "plt.xlabel('Number of minibatches')\n",
    "plt.ylabel('Learning rate')\n",
    "\n",
    "plt.savefig('figure97_leraning_rate_decay.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure97_leraning_rate_decay.png')\n",
    "\n",
    "# Off\n",
    "train_acc,test_acc,losses,ANN,current_LR = train_model(0.01,False)\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(4*phi,4))\n",
    "\n",
    "plt.plot(current_LR)\n",
    "plt.title('Learning rate should not change')\n",
    "plt.xlabel('Number of minibatches')\n",
    "plt.ylabel('Learning rate')\n",
    "\n",
    "plt.savefig('figure98_leraning_rate_decay.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure98_leraning_rate_decay.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-Kykef9wa9Q"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over learning rate decay\n",
    "\n",
    "train_acc_dyn,test_acc_dyn,losses_dyn,ANN,current_LR = train_model(0.01,True)\n",
    "train_acc_stat,test_acc_stat,losses_stat,ANN,current_LR = train_model(0.01,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1749223098949,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "-TQCv6-twuoY",
    "outputId": "a6488cca-f9b5-4bfd-fdb5-bda416fcb8e0"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "plt.plot(train_acc_dyn,'-',color='tab:blue',label='Dyn: Train')\n",
    "plt.plot(test_acc_dyn,'--',color='tab:blue',label='Dyn: Test')\n",
    "\n",
    "plt.plot(train_acc_stat,'-',color='tab:orange',label='Stat: Train')\n",
    "plt.plot(test_acc_stat,'--',color='tab:orange',label='Stat: Test')\n",
    "\n",
    "plt.xlabel('Training epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure99_leraning_rate_decay.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure99_leraning_rate_decay.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nr1Jz_ROxxBl"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    When you run the experiment in the previous cell multiple times, you can get different answers. This indicates\n",
    "#    that the network and/or training regimen is not stable enough. What can you do to increase the stability of the\n",
    "#    model and training? That is, what can you change to make the results more similar each time you re-run the experiment?\n",
    "\n",
    "# Many options are available, virtually any metaparameter can be changed to see\n",
    "# if the performance becomes more stable (e.g., epochs number, optimizer, batch\n",
    "# normalisation, activation and loss functions, weight regularisation, optimizers\n",
    "# optimisation, just to name those discussed in previous videos). Here for example\n",
    "# I switched to an RMSprop optimizer. I'd say the model is a bit more stable simply\n",
    "# because learning is achieved faster; on the other hand, the performance is a bit\n",
    "# more variable with respect to the training epoch (i.e. more stable across runs,\n",
    "# but less stable across epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsnEd6svx4Pc"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    There are several more options for dynamic learning rates in Pytorch. Try modifying the code!\n",
    "#    See https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "# Wow I was expecting a lot of options but couldn't imagine there were so many. I tried\n",
    "# an .EponentialLR() method (i.e. \"decays the learning rate of each parameter group\n",
    "# by gamma every epoch\"); it only needs a gamma parameter, and no step size (as far\n",
    "# as I understood, the lr is adapted at each step), I picked a gamma close to 1 so that\n",
    "# the lr decay is not too extreme (0.99 and 0.999). Also, I went back to SGD for\n",
    "# comparability.\n",
    "# Surprisingly, the two values make a hige difference! I mean ... exponential\n",
    "# decay is as triky as it is fast, but still interesting to observe how small changes\n",
    "# in one single parameter produce large and qualitative changes in the output (by\n",
    "# qualitative I mean that, in practice and given all the other metaparameters, one\n",
    "# model doesn't work, and one does)\n",
    "\n",
    "# %% Create the model\n",
    "def gen_model(initial_learning_rate):\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(2,8)\n",
    "            self.hid1   = nn.Linear(8,8)\n",
    "            self.output = nn.Linear(8,3)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid1(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Model instance\n",
    "    ANN = model()\n",
    "\n",
    "    # Loss function and optimizer (set learning rate decay)\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=initial_learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.999)\n",
    "\n",
    "    return ANN,loss_fun,optimizer,scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1749224409000,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "H5CtHlDV2nlJ",
    "outputId": "323cce5e-fc74-4a4d-eaf9-05ffebf48e82"
   },
   "outputs": [],
   "source": [
    "# %% Explore learning rate decay parameter\n",
    "\n",
    "# Create network\n",
    "ANN = gen_model(0.01)[0]\n",
    "\n",
    "# Fresh optimizer\n",
    "optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.999)\n",
    "\n",
    "# Test change in learning rate\n",
    "for epoch_i in range(3):\n",
    "    for batch_num in range(10):\n",
    "\n",
    "        print(f'Batch {batch_num+1}, epoch {epoch_i}: LR = {scheduler.get_last_lr()[0]}')\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1I_8ilKI4UNS"
   },
   "outputs": [],
   "source": [
    "# %% ( same training function )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "executionInfo": {
     "elapsed": 5701,
     "status": "ok",
     "timestamp": 1749224419103,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "28UpPEda30Yi",
    "outputId": "4efab7c1-3b78-4dd1-9434-f521e3dc8da3"
   },
   "outputs": [],
   "source": [
    "# %% Sanity check\n",
    "\n",
    "# On\n",
    "train_acc,test_acc,losses,ANN,current_LR = train_model(0.01,True)\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(4*phi,4))\n",
    "\n",
    "plt.plot(current_LR)\n",
    "plt.title('Learning rate should change')\n",
    "plt.xlabel('Number of minibatches')\n",
    "plt.ylabel('Learning rate')\n",
    "\n",
    "plt.savefig('figure102_leraning_rate_decay_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure102_leraning_rate_decay_extra2.png')\n",
    "\n",
    "# Off\n",
    "train_acc,test_acc,losses,ANN,current_LR = train_model(0.01,False)\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(4*phi,4))\n",
    "\n",
    "plt.plot(current_LR)\n",
    "plt.title('Learning rate should not change')\n",
    "plt.xlabel('Number of minibatches')\n",
    "plt.ylabel('Learning rate')\n",
    "\n",
    "plt.savefig('figure103_leraning_rate_decay_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure103_leraning_rate_decay_extra2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWPh0MVE4Pj-"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over learning rate decay\n",
    "\n",
    "train_acc_dyn,test_acc_dyn,losses_dyn,ANN,current_LR = train_model(0.01,True)\n",
    "train_acc_stat,test_acc_stat,losses_stat,ANN,current_LR = train_model(0.01,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 294,
     "status": "ok",
     "timestamp": 1749224476334,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "25C1nxC_4Pj-",
    "outputId": "f1b34cd4-838b-4df5-80c7-f331f3678660"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "plt.plot(train_acc_dyn,'-',color='tab:blue',label='Dyn: Train')\n",
    "plt.plot(test_acc_dyn,'--',color='tab:blue',label='Dyn: Test')\n",
    "\n",
    "plt.plot(train_acc_stat,'-',color='tab:orange',label='Stat: Train')\n",
    "plt.plot(test_acc_stat,'--',color='tab:orange',label='Stat: Test')\n",
    "\n",
    "plt.xlabel('Training epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure104_leraning_rate_decay_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure104_leraning_rate_decay_extra2.png')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNeumQYerU7kVwsmLxmTG/X",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
