{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajZQZPTAL35r"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 12.126\n",
    "#    Save the best performing model\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH8Ro002LsPZ"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RBmDOxjqck8"
   },
   "outputs": [],
   "source": [
    "# %% A note\n",
    "\n",
    "# This procedure entails \"researcher overfitting\", as we are deliberately picking\n",
    "# the bets model, we thus need a train, dev, and a test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1754154439069,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "t4VA0l-rvIXr",
    "outputId": "88a5ab4d-cbc0-4686-bffb-5cd635d2ab6b"
   },
   "outputs": [],
   "source": [
    "# %% How to save the highest of random numbers\n",
    "\n",
    "best = [0,0]\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # \"Train\" model\n",
    "    accuracy = np.random.rand()\n",
    "\n",
    "    # See if it's better than the previous run\n",
    "    if accuracy > best[0]:\n",
    "        best = [accuracy,i]\n",
    "\n",
    "print( f'Highest \"accuracy\" was {100*best[0]:.2f}% in run {best[1]+1}.' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 897,
     "status": "ok",
     "timestamp": 1754154744502,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "4HOETkzAvIVJ",
    "outputId": "7dd14cf0-27ff-42f9-e909-ba4de79de7eb"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "# %% Data\n",
    "\n",
    "n_clust = 300\n",
    "blur    = 1\n",
    "\n",
    "A = [ 1,1 ]\n",
    "B = [ 5,1 ]\n",
    "C = [ 4,3 ]\n",
    "\n",
    "a = [ A[0]+np.random.randn(n_clust)*blur, A[1]+np.random.randn(n_clust)*blur ]\n",
    "b = [ B[0]+np.random.randn(n_clust)*blur, B[1]+np.random.randn(n_clust)*blur ]\n",
    "c = [ C[0]+np.random.randn(n_clust)*blur, C[1]+np.random.randn(n_clust)*blur ]\n",
    "\n",
    "# True labels\n",
    "labels_np = np.hstack(( np.zeros((n_clust)),\n",
    "                        np.ones( (n_clust)),\n",
    "                        np.ones( (n_clust))+1 ))\n",
    "\n",
    "# Concatanate into a matrix\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data   = torch.tensor(data_np).float()\n",
    "labels = torch.tensor(labels_np).long()\n",
    "\n",
    "# Plotting (with distance from origin)\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig = plt.figure(figsize=(phi*6,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(0.2,0.9,len(np.unique(labels))))\n",
    "\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'s',color=cmaps[0],alpha=.5)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'o',color=cmaps[1],alpha=.5)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'^',color=cmaps[2],alpha=.5)\n",
    "\n",
    "plt.grid(color=[.9,.9,.9])\n",
    "plt.title('Some data')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "plt.savefig('figure38_save_best_model.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure38_save_best_model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1754155107833,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "O6jsIm1zvISv",
    "outputId": "b427ca68-3f70-40eb-cc9e-ae066f4f17fa"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Partition of data\n",
    "partitions = [ 3*n_clust-400,200,200 ]\n",
    "\n",
    "# Split all data with scikitlearn (train + others)\n",
    "train_data,other_data, train_labels,other_labels = train_test_split(data,labels,train_size=partitions[0])\n",
    "\n",
    "# Split other data with scikitlearn (dev + test)\n",
    "dev_data,test_data, dev_labels,test_labels = train_test_split(other_data,other_labels,train_size=partitions[1])\n",
    "\n",
    "print('   Total data size: ' + str(data.shape) + '\\n')\n",
    "print('Training data size: ' + str(train_data.shape))\n",
    "print('  Devset data size: ' + str(dev_data.shape))\n",
    "print('    Test data size: ' + str(test_data.shape))\n",
    "\n",
    "# Convert to PyTorch Datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "dev_data   = TensorDataset(dev_data,dev_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert to dataloader object\n",
    "batch_size   = 30\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last= True)\n",
    "dev_loader   = DataLoader(dev_data,batch_size=dev_data.tensors[0].shape[0])\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aU3rQoeqvIQV"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "def gen_model():\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input = nn.Linear(2,8)\n",
    "            self.hid = nn.Linear(8,8)\n",
    "            self.output = nn.Linear(8,3)\n",
    "\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Model instance, loss function, and optimiser\n",
    "    ANN       = model()\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOXyAc2gvINw"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    best_model = {'Accuracy':0, 'Model':None}\n",
    "\n",
    "    # Number of epochs and model instance\n",
    "    num_epochs = 100\n",
    "    ANN,loss_function,optimizer = gen_model()\n",
    "\n",
    "    # Preallocate variables\n",
    "    losses    = torch.zeros(num_epochs)\n",
    "    train_acc = torch.zeros(num_epochs)\n",
    "    dev_acc   = torch.zeros(num_epochs)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Batches loop\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward prop and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_function(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute loss and accuracy for this batch\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_acc.append(100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item())\n",
    "\n",
    "        # Compute loss and accuracy for the epoch\n",
    "        losses[epoch_i]    = np.mean(batch_loss)\n",
    "        train_acc[epoch_i] = np.mean(batch_acc)\n",
    "\n",
    "        # Test accuracy (switch to evaluation mode and then back to training\n",
    "        # mode to save up computation)\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(dev_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "\n",
    "        dev_acc[epoch_i] = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item()\n",
    "        ANN.train()\n",
    "\n",
    "        # Store this model if it's the best so far\n",
    "        if dev_acc[epoch_i] > best_model['Accuracy']:\n",
    "\n",
    "            best_model['Accuracy'] = dev_acc[-1]\n",
    "            best_model['Model']    = copy.deepcopy(ANN.state_dict())\n",
    "\n",
    "    return train_acc,dev_acc,losses,ANN,best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2363,
     "status": "ok",
     "timestamp": 1754156280164,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "C4TGfK8c0V_m",
    "outputId": "1de220a2-a40c-459c-c3c2-0aa74a81f5bb"
   },
   "outputs": [],
   "source": [
    "# %% Run the model\n",
    "\n",
    "train_acc,dev_acc,losses,ANN,best_model = train_model()\n",
    "print(f'Best accuracy: {best_model[\"Accuracy\"]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 699,
     "status": "ok",
     "timestamp": 1754156301386,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "m_F-IjsF0V9D",
    "outputId": "5ceaaa6e-a50a-4295-c1b5-2a7fe825e0d6"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*phi*6,6))\n",
    "\n",
    "ax[0].plot(losses,'o-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_title('Losses')\n",
    "\n",
    "ax[1].plot(train_acc,'o-',label='Train')\n",
    "ax[1].plot(dev_acc,'o-',label='Devset')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_title('Accuracy')\n",
    "#ax[1].set_ylim([85,95])\n",
    "#ax[1].set_xlim([80,105])\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure39_save_best_model.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure39_save_best_model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PKJDg7X2muB"
   },
   "outputs": [],
   "source": [
    "# %% Test on actual test set - intuitive but wrong way\n",
    "\n",
    "# Pass data through best model (won't work)\n",
    "X,y  = next(iter(test_loader))\n",
    "yHat = best_model[\"Model\"](X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1754156317678,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "s60X63Va0V6q",
    "outputId": "1a4e9a2c-052d-4dbd-bfb5-c08a988481f3"
   },
   "outputs": [],
   "source": [
    "# %% Test on actual test set - correct way\n",
    "\n",
    "# Recreate best model and run through test data\n",
    "best_network = gen_model()[0]\n",
    "best_network.load_state_dict(best_model[\"Model\"])\n",
    "\n",
    "X,y  = next(iter(test_loader))\n",
    "yHat = best_network(X)\n",
    "\n",
    "best_acc = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item()\n",
    "print(f'Best accuracy: {best_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1754156471760,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "N2KX4wYX3V2Y",
    "outputId": "76f66760-c2f9-40df-9034-ed4ef815927e"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig = plt.figure(figsize=(phi*6,6))\n",
    "\n",
    "plt.plot(train_acc,'o-',label='Train accuracy')\n",
    "plt.plot(dev_acc,'o-',label='Dev set accuracy')\n",
    "plt.plot([0,len(dev_acc)],[best_acc,best_acc],'r--',label='Best dev model on test data')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Accuracy')\n",
    "plt.ylim([best_acc-5,best_acc+5])\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure43_save_best_model.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure43_save_best_model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlBsFUBc4VRm"
   },
   "outputs": [],
   "source": [
    "# %% A note - More datasets\n",
    "\n",
    "# If you want more data, the most popular repository is now kagggle.com; it's\n",
    "# focused on competiotion, but you can still get the data and see others'\n",
    "# solutions.\n",
    "\n",
    "# Otherwise you can also google by topic and see what comes up.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMgD7beRjqF5C0ZhLvnXJpA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
