{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le5MsLuthBqv"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 9.75\n",
    "#    L2 regularisation in practise\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty5MFy8KhV0e"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glVG6q-KCLKx"
   },
   "outputs": [],
   "source": [
    "# %% Import Iris dataset\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Convert from pandas df to tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Species to numbers\n",
    "labels = torch.zeros(len(data),dtype=torch.long)\n",
    "labels[iris.species=='setosa']     = 0\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxDQFDRhCQJt"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 64\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGEc4dxTCTqC"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model(L2_lambda):\n",
    "\n",
    "    # Architecture\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(4,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,3))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.005,weight_decay=L2_lambda)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tjETYHmr_Py"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Initialise empty accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training data batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch accuracy\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item() )\n",
    "            batch_loss.append( loss.item() )\n",
    "\n",
    "        # Average training accuracy from batches\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "        losses.append( np.mean(batch_loss) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append( 100*torch.mean((pred_labels==y).float()).item() )\n",
    "\n",
    "        # Reset to train mode (with weight reg. this switching back and forth is technically not needed)\n",
    "        ANN.train()\n",
    "\n",
    "    # Output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3aTJU00CEeH"
   },
   "outputs": [],
   "source": [
    "# %% Test the model\n",
    "\n",
    "L2_lambda = 0.01\n",
    "ANN,loss_fun,optimizer    = gen_model(L2_lambda)\n",
    "train_acc,test_acc,losses = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1746559268403,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "6JnD8rL988k2",
    "outputId": "89936967-8d2d-40c7-a1c2-d57b3007f331"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(losses,'^-')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_title('Losses with L2 $\\lambda$=' + str(L2_lambda))\n",
    "\n",
    "ax[1].plot(train_acc,'o-')\n",
    "ax[1].plot(test_acc,'s-')\n",
    "ax[1].set_title('Accuracy with L2 $\\lambda$=' + str(L2_lambda))\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].legend(['Train','Test'])\n",
    "\n",
    "plt.savefig('figure20_weight_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure20_weight_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewmODpmh-RZ2"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtWMUS-nC7Lj"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment\n",
    "\n",
    "# Initialise stuff\n",
    "l2_lambdas        = np.linspace(0,.1,10)\n",
    "acc_results_train = np.zeros((num_epochs,len(l2_lambdas)))\n",
    "acc_results_test  = np.zeros((num_epochs,len(l2_lambdas)))\n",
    "\n",
    "# Loop over batch sizes\n",
    "for lambda_i in range(len(l2_lambdas)):\n",
    "\n",
    "    # Generate and train model\n",
    "    ANN,loss_fun,optimizer    = gen_model(l2_lambdas[lambda_i])\n",
    "    train_acc,test_acc,losses = train_model()\n",
    "\n",
    "    # Store\n",
    "    acc_results_train[:,lambda_i] = smooth_adaptive(train_acc,10)\n",
    "    acc_results_test[:,lambda_i]  = smooth_adaptive(test_acc,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 1028,
     "status": "ok",
     "timestamp": 1746564997221,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "A9YgIcE9Ejjy",
    "outputId": "5acfd2ab-3866-46fe-f044-fd18fab50393"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(l2_lambdas)))\n",
    "for i in range(len(l2_lambdas)):\n",
    "    ax[0].plot(acc_results_train[:,i],color=cmaps[i])\n",
    "    ax[1].plot(acc_results_test[:,i],color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# Make the legend easier to read\n",
    "leglabels = [np.round(i,2) for i in l2_lambdas]\n",
    "\n",
    "# Common features\n",
    "for i in range(2):\n",
    "    ax[i].legend(leglabels)\n",
    "    ax[i].set_xlabel('Epoch')\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].set_ylim([50,101])\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.savefig('figure21_weight_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure21_weight_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1746565020647,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "Kev0gA2eH9IY",
    "outputId": "b091bbd0-3834-49a6-9080-6839e82a86c1"
   },
   "outputs": [],
   "source": [
    "# %% Show average accuracy by L2 rates\n",
    "\n",
    "# Pick only a range of epochs\n",
    "epoch_range = [1000,2000]\n",
    "\n",
    "plt.plot(l2_lambdas,\n",
    "         np.mean(acc_results_train[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'bo-',label='TRAIN')\n",
    "\n",
    "plt.plot(l2_lambdas,\n",
    "         np.mean(acc_results_test[epoch_range[0]:epoch_range[1],:],axis=0),\n",
    "         'rs-',label='TEST')\n",
    "\n",
    "plt.xlabel('L2 regularization amount')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Average accuracy by L2 regularization amount')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure22_weight_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure22_weight_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEOoRIvyIo0p"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    In general, regularization tends to benefit large, complex models, and has less impact (and sometimes even a negative\n",
    "#    impact) on smaller or simpler model architectures. Modify the model architecture to have three hidden layers, and\n",
    "#    see whether that changes the effect of L2 regularization on performance. (You might want to increase the number of\n",
    "#    epochs.)\n",
    "\n",
    "# Indeed a deeper model highlights more dramatic differences in the regularisation amount. Notably, as the regularisation\n",
    "# increases, the accuracy decreases (or it takes more time to increase), for both training and test data.\n",
    "\n",
    "# Modified function (num_epochs increased to 2500)\n",
    "def gen_model(L2_lambda):\n",
    "\n",
    "    # Architecture\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(4,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,3))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.005,weight_decay=L2_lambda)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rfd8WvLIorw"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Multiple regularization methods can be combined. Add 15% dropout to the hidden layer(s) and see how that affects\n",
    "#    the model's performance.\n",
    "\n",
    "# A combination of weight (L2) and dropout (15%) regularisation does improve the performace of the model in the\n",
    "# sense that, even if it requires more training, the training accuracy is reduced but the testing accuracy is boosted.\n",
    "\n",
    "# Modified function (simpler model, but num_epochs still increased to 2500)\n",
    "def gen_model(L2_lambda,dropout_rate):\n",
    "\n",
    "    # Architecture (nn.Dropout(p=...) does not require switching between train and eval)\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(4,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Dropout(p=dropout_rate),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Dropout(p=dropout_rate),\n",
    "             nn.Linear(64,3))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.005,weight_decay=L2_lambda)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n",
    "\n",
    "# [... train model function ...]\n",
    "\n",
    "# Parametric experiment\n",
    "l2_lambdas        = np.linspace(0,.1,10)\n",
    "dropout_rate      = 0.15\n",
    "acc_results_train = np.zeros((num_epochs,len(l2_lambdas)))\n",
    "acc_results_test  = np.zeros((num_epochs,len(l2_lambdas)))\n",
    "\n",
    "# Loop over batch sizes\n",
    "for lambda_i in range(len(l2_lambdas)):\n",
    "\n",
    "    # Generate and train model\n",
    "    ANN,loss_fun,optimizer    = gen_model(l2_lambdas[lambda_i],dropout_rate)\n",
    "    train_acc,test_acc,losses = train_model()\n",
    "\n",
    "    # Store\n",
    "    acc_results_train[:,lambda_i] = smooth_adaptive(train_acc,10)\n",
    "    acc_results_test[:,lambda_i]  = smooth_adaptive(test_acc,10)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMdthf42A8LXhRDBx/4z3Fj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
