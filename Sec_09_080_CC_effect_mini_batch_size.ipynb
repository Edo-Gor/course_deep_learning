{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETbx_2RNW6vP"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 9.80\n",
    "#    Code challenge 7: effects of mini-batch size\n",
    "\n",
    "#    1) Copy code from Sec_09_078_batching_regularisation.ipynb file\n",
    "#    2) Run a mini-batch size parametric experiment by setting the batch size to 2^n, n=1,..6\n",
    "#    3) Set the learning rate to 0.001\n",
    "#    4) Store train and test accuracies over epoch, for each batch size\n",
    "#    5) Plot the accuracies\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eWTRgVIXP0z"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyFgMWvNuS-N"
   },
   "outputs": [],
   "source": [
    "# %% Import Iris dataset\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Convert from pandas df to tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Species to numbers\n",
    "labels = torch.zeros(len(data),dtype=torch.long)\n",
    "labels[iris.species=='setosa']     = 0\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1746901728638,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "MgT36_D6dX4M",
    "outputId": "a6a16c3a-7b25-41c4-be32-9ac95082f385"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "iris.plot(marker='o',linestyle='none',figsize=(12,6))\n",
    "\n",
    "plt.xlabel('Sample number')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Iris dataset features')\n",
    "\n",
    "plt.savefig('figure42_code_challenge_7.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure42_code_challenge_7.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ng9312Lu1V5"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects (test data are not partitioned, we don't regularise in testing)\n",
    "test_loader = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "# > the train_loader is moved inside the train_model() function to allow a parametric test of the batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df2s-SeXv19U"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    # Architecture\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(4,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,64),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(64,3))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (small lr for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.0005)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOG_NVNztQwX"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 500\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Initialise accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        X,y = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append(  100*torch.mean((pred_labels==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xslvfhxctfPk"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over mini-batches size\n",
    "\n",
    "batch_size_exp = np.arange(1,7)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "train_acc   = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "test_acc    = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "losses      = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "\n",
    "for i,exp_i in enumerate(batch_size_exp):\n",
    "\n",
    "        batch_size   = int(2**exp_i)\n",
    "        train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "        ANN,loss_fun,optimizer = gen_model()\n",
    "        train_acc[:,i],test_acc[:,i],losses[:,i] = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiLwYUplu_2r"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1746908447761,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "koOdFKumzEdA",
    "outputId": "a3327124-2289-4169-986e-623701ad8bc8"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(batch_size_exp)))\n",
    "for i in range(len(batch_size_exp)):\n",
    "    ax[0].plot(smooth_adaptive(train_acc[:,i],20),color=cmaps[i])\n",
    "    ax[1].plot(smooth_adaptive(test_acc[:,i],20),color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# Make the legend easier to read\n",
    "leglabels = [2**int(i) for i in batch_size_exp]\n",
    "\n",
    "# Common features\n",
    "for i in range(2):\n",
    "    ax[i].legend(leglabels)\n",
    "    ax[i].set_xlabel('Epoch')\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].set_ylim([50,101])\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.savefig('figure43_code_challenge_7.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure43_code_challenge_7.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2kd9yb6d8HM"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Why are the minibatch sizes specified in powers of 2? That's partly because DL is developed by math/computer nerds,\n",
    "#    but it's also partly an attempt to optimize computation speed, because computer memory comes in powers of two.\n",
    "#    But 2**N is not a specific requirement. Adjust the code to use batch sizes corresponding to six linearly spaced\n",
    "#    integers between 2 and 50.\n",
    "\n",
    "# Easily done by changing the right variable\n",
    "\n",
    "# %% Modified parametric experiment over mini-batches size\n",
    "\n",
    "batch_size_exp = np.linspace(2,50,6,dtype=int)\n",
    "test_loader    = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "train_acc   = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "test_acc    = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "losses      = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "\n",
    "for i,val in enumerate(batch_size_exp):\n",
    "\n",
    "        batch_size   = int(val)\n",
    "        train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "        ANN,loss_fun,optimizer = gen_model()\n",
    "        train_acc[:,i],test_acc[:,i],losses[:,i] = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEftbhoov3lq"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Why did I ask you to set the learning rate to .001? Try this experiment again using higher and lower learning rates.\n",
    "#    What do you conclude about batch sizes and learning rate?\n",
    "\n",
    "# Assuming a constant number of epochs of 500, varying the learning rate (e.g., to 0.01 and 0.0005) makes of course\n",
    "# the model accuracy collapse for small lr vales, however, there seems to be a complex interaction with the batch\n",
    "# size; with a larger lr, the learning curve is relatively stable (except for very large batches), while for a\n",
    "# smaller lr, the learning curve is highly dependent on the batch size. However, one must also take into account\n",
    "# that the Iris dataset is quite omogenous (i.e., small batches work better)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM37dqH14pvOidw67q73W4z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
