{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxLsu_5_Ao80"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 11.112\n",
    "#    Code challenge 17: optimizers and MNIST\n",
    "\n",
    "#    1) Start from code from video 11.107, 11.111, and 10.101\n",
    "#    2) Vary systematically the optimizer (SGD, RMSprop, Adam)\n",
    "#    3) Vary systematically the learning rate (0.0001 to 0.1 in 6 log steps)\n",
    "#    4) Plot final accuracy from train and test, against the learning rates, for\n",
    "#       the three optimizers (avg accuracy over last 10 epochs)\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFOW2iaUArr4"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJokslogtW1t"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Split labels from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# Normalise data (original range is (0,255))\n",
    "data_norm = data / np.max(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5IiMkI-v6EI"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Convert to tensor (float and integers)\n",
    "data_tensor   = torch.tensor(data_norm).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "# Split data with scikitlearn (10% test data)\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T793J-Qnwt9b"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "#    Keep flexibly loop over model depth/breadth, add flexibility over optimizer\n",
    "\n",
    "def gen_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate):\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self,nUnits,nLayers,dropout_rate):\n",
    "            super().__init__()\n",
    "\n",
    "            # Dictionary to store the layers\n",
    "            self.layers  = nn.ModuleDict()\n",
    "            self.nLayers = nLayers\n",
    "\n",
    "            # Dropout\n",
    "            self.dropout_rate = dropout_rate\n",
    "\n",
    "            # Architecture\n",
    "            self.layers['input'] = nn.Linear(784,nUnits)\n",
    "            for i in range(nLayers):\n",
    "                self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n",
    "            self.layers['output'] = nn.Linear(nUnits,10)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.layers['input'](x))\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            for i in range(self.nLayers):\n",
    "                x = F.relu(self.layers[f'hidden{i}'](x))\n",
    "                x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            x = self.layers['output'](x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create model instance\n",
    "    ANN = mnist_FFN(nUnits,nLayers,drop_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (can be selected)\n",
    "    opti_fun  = getattr( torch.optim,optimizer_alg )\n",
    "    optimizer = opti_fun(ANN.parameters(),lr=learning_rate)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1751806748348,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "bHwG-7pBzs9d",
    "outputId": "56e6a966-9002-45f2-9a6b-bd197477c555"
   },
   "outputs": [],
   "source": [
    "# %% Generate an instance of the model and check it\n",
    "#    Try 'SGD', 'RMSprop', and 'Adam'\n",
    "\n",
    "nUnitsPerLayer = 10\n",
    "nLayers        = 2\n",
    "optim          = 'Adam'\n",
    "lr             = 0.01\n",
    "\n",
    "model,loss_fun,optimizer = gen_model(nUnitsPerLayer,nLayers,0.25,optim,lr)\n",
    "print(model)\n",
    "\n",
    "optim = gen_model(nUnitsPerLayer,nLayers,0.25,optim,lr)[2]\n",
    "print(optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1751806781238,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "ExnBzEBt0dX6",
    "outputId": "66b6e71d-65a2-4c0c-8606-52804ee747f3"
   },
   "outputs": [],
   "source": [
    "# %% Run the model to check its internal consistency\n",
    "\n",
    "# Samples and dimentions\n",
    "tmpx = torch.randn(6,784)\n",
    "\n",
    "# Run the model\n",
    "y = model(tmpx)\n",
    "\n",
    "# Show the output shape and the output\n",
    "print(y.shape)\n",
    "print()\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9wI96zd0kF_"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate):\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate)\n",
    "\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5499,
     "status": "ok",
     "timestamp": 1751807172326,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "1p9_hg-81dfL",
    "outputId": "6ba851bc-3f16-483b-fd05-41dc3d4cd22d"
   },
   "outputs": [],
   "source": [
    "# %% Test the whole setting\n",
    "\n",
    "nUnitsPerLayer = 10\n",
    "nLayers        = 2\n",
    "drop_rate      = 0\n",
    "optim          = 'Adam'\n",
    "lr             = 0.01\n",
    "\n",
    "train_acc,test_acc,losses,ANN = train_model(nUnitsPerLayer,nLayers,drop_rate,optim,lr)\n",
    "\n",
    "print(train_acc[-1])\n",
    "print(test_acc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9q7u4ljh2Dxw"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment\n",
    "\n",
    "# Takes ~28 mins (with only 2 layers, 60 units each)\n",
    "num_layers     = np.linspace(2,2,1,dtype=int)\n",
    "num_units      = np.linspace(60,60,1,dtype=int)\n",
    "drop_rate      = 0\n",
    "optimizers     = ['SGD','RMSprop','Adam']\n",
    "learning_rates = np.logspace(-4,-1,6)\n",
    "\n",
    "# Preallocate output matrices\n",
    "shape             = (len(optimizers),len(learning_rates),len(num_units),len(num_layers))\n",
    "performance_train = np.zeros(shape)\n",
    "performance_test  = np.zeros(shape)\n",
    "training_times    = np.zeros(shape)\n",
    "\n",
    "# Buckle up, here's the experiment!\n",
    "for opt_i, optimizer in enumerate(optimizers):\n",
    "    for lr_i, lr in enumerate(learning_rates):\n",
    "        for unit_i, n_units in enumerate(num_units):\n",
    "            for layer_i, n_layers in enumerate(num_layers):\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_acc,test_acc,losses,ANN = train_model(\n",
    "                                                            n_units,\n",
    "                                                            n_layers,\n",
    "                                                            drop_rate,\n",
    "                                                            optimizer,\n",
    "                                                            lr)\n",
    "\n",
    "                duration = time.time() - start_time\n",
    "\n",
    "                performance_train[opt_i, lr_i, unit_i, layer_i] = np.mean(train_acc[-10:])\n",
    "                performance_test[opt_i, lr_i, unit_i, layer_i]  = np.mean(test_acc[-10:])\n",
    "                training_times[opt_i, lr_i, unit_i, layer_i]    = duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1751833260932,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "bfKBY0T15VPE",
    "outputId": "baa9dd1a-1995-45d5-b532-db141cb227ae"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "# Pick architecture to visualise\n",
    "selected_units  = 60\n",
    "selected_layers = 2\n",
    "\n",
    "# Indices for slicing\n",
    "unit_idx  = list(num_units).index(selected_units)\n",
    "layer_idx = list(num_layers).index(selected_layers)\n",
    "\n",
    "# Plot\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,axs = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(0.2,0.9,len(optimizers)))\n",
    "for i,opt in enumerate(optimizers):\n",
    "    axs[0].plot(learning_rates,performance_train[i,:,unit_idx,layer_idx],'-o',color=cmaps[i],label=opt)\n",
    "axs[0].set_title('Training accuracy')\n",
    "axs[0].set_xlabel('Learning rate')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].legend(title='Optimizer')\n",
    "axs[0].grid(True)\n",
    "axs[0].set_ylim(5,105)\n",
    "\n",
    "for i, opt in enumerate(optimizers):\n",
    "    axs[1].plot(learning_rates, performance_test[i,:,unit_idx,layer_idx],'-o',color=cmaps[i],label=opt)\n",
    "axs[1].set_title('Test accuracy')\n",
    "axs[1].set_xlabel('Learning rate')\n",
    "axs[1].set_ylabel('Accuracy (%)')\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].legend(title='Optimizer')\n",
    "axs[1].grid(True)\n",
    "axs[1].set_ylim(5,105)\n",
    "\n",
    "plt.suptitle(f'Performance\\n(units={selected_units}, layers={selected_layers})',fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure47_code_challenge_17.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure47_code_challenge_17.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1751833276170,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "h2pbfx-H5VBT",
    "outputId": "3e028d02-7471-48a7-c535-b060c805f17c"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig, axs = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(0.2,0.9,len(learning_rates)))\n",
    "for j, lr in enumerate(learning_rates):\n",
    "    axs[0].plot(optimizers,performance_train[:,j,unit_idx,layer_idx],'-o',color=cmaps[j],label=f'lr={lr:.4f}')\n",
    "axs[0].set_title('Training accuracy')\n",
    "axs[0].set_xlabel('Optimizer')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].legend(title='Learning rate')\n",
    "axs[0].grid(True)\n",
    "axs[0].set_ylim(10,105)\n",
    "\n",
    "for j, lr in enumerate(learning_rates):\n",
    "    axs[1].plot(optimizers,performance_test[:,j,unit_idx,layer_idx],'-o',color=cmaps[j],label=f'lr={lr:.4f}')\n",
    "axs[1].set_title('Test accuracy')\n",
    "axs[1].set_xlabel('Optimizer')\n",
    "axs[1].set_ylabel('Accuracy (%)')\n",
    "axs[1].legend(title='Learning rate')\n",
    "axs[1].grid(True)\n",
    "axs[1].set_ylim(10,105)\n",
    "\n",
    "plt.suptitle(f'Performance\\n(units={selected_units}, layers={selected_layers})',fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure48_code_challenge_17.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure48_code_challenge_17.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnPKZKzY8cJS"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Replace the learning rate factor with L2 regularization. Does L2 regularization help all optimization\n",
    "#    methods in the same way?\n",
    "\n",
    "# Adding an L2 regularisation factor of 0.001 doesn't help much the performace,\n",
    "# and the models perform pretty much the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBSXSKDI92MG"
   },
   "outputs": [],
   "source": [
    "# %% Ex. 1 - Continue ...\n",
    "\n",
    "def gen_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate,l2_lambda):\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self,nUnits,nLayers,dropout_rate):\n",
    "            super().__init__()\n",
    "\n",
    "            self.layers  = nn.ModuleDict()\n",
    "            self.nLayers = nLayers\n",
    "\n",
    "            self.dropout_rate = dropout_rate\n",
    "\n",
    "            self.layers['input'] = nn.Linear(784,nUnits)\n",
    "            for i in range(nLayers):\n",
    "                self.layers[f'hidden{i}'] = nn.Linear(nUnits,nUnits)\n",
    "            self.layers['output'] = nn.Linear(nUnits,10)\n",
    "\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.layers['input'](x))\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            for i in range(self.nLayers):\n",
    "                x = F.relu(self.layers[f'hidden{i}'](x))\n",
    "                x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            x = self.layers['output'](x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    ANN      = mnist_FFN(nUnits,nLayers,drop_rate)\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    opti_fun  = getattr( torch.optim,optimizer_alg )\n",
    "    optimizer = opti_fun(ANN.parameters(),lr=learning_rate,weight_decay=l2_lambda)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n",
    "\n",
    "\n",
    "def train_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate,l2_lambda):\n",
    "\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate,l2_lambda)\n",
    "\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE_WkxMr-NQ6"
   },
   "outputs": [],
   "source": [
    "# %% Ex. 1 - Continue ...\n",
    "\n",
    "# Takes ~36 mins (with only 2 layers, 60 units each)\n",
    "num_layers     = np.linspace(2,2,1,dtype=int)\n",
    "num_units      = np.linspace(60,60,1,dtype=int)\n",
    "drop_rate      = 0\n",
    "optimizers     = ['SGD','RMSprop','Adam']\n",
    "learning_rates = np.logspace(-4,-1,6)\n",
    "l2_lambda      = 1e-3\n",
    "\n",
    "# Preallocate output matrices\n",
    "shape             = (len(optimizers),len(learning_rates),len(num_units),len(num_layers))\n",
    "performance_train = np.zeros(shape)\n",
    "performance_test  = np.zeros(shape)\n",
    "training_times    = np.zeros(shape)\n",
    "\n",
    "# Buckle up, here's the experiment!\n",
    "for opt_i, optimizer in enumerate(optimizers):\n",
    "    for lr_i, lr in enumerate(learning_rates):\n",
    "        for unit_i, n_units in enumerate(num_units):\n",
    "            for layer_i, n_layers in enumerate(num_layers):\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_acc,test_acc,losses,ANN = train_model(\n",
    "                                                            n_units,\n",
    "                                                            n_layers,\n",
    "                                                            drop_rate,\n",
    "                                                            optimizer,\n",
    "                                                            lr,\n",
    "                                                            l2_lambda)\n",
    "\n",
    "                duration = time.time() - start_time\n",
    "\n",
    "                performance_train[opt_i, lr_i, unit_i, layer_i] = np.mean(train_acc[-10:])\n",
    "                performance_test[opt_i, lr_i, unit_i, layer_i]  = np.mean(test_acc[-10:])\n",
    "                training_times[opt_i, lr_i, unit_i, layer_i]    = duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J102Yni8cAV"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    We previously observed the batch normalization boosted model performance. Does it help here as well?\n",
    "#    (Note: Best to pick just one learning rate for this experiment.)\n",
    "\n",
    "# It does, and it does so increadibly well, as now the performance reaches\n",
    "# virtually ceiling for any optimizer and leraning rate (here shown just 0.0001\n",
    "# and 0.1 for the sake of time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYzixF7aKQsR"
   },
   "outputs": [],
   "source": [
    "# %% Ex. 2 - Continue ...\n",
    "\n",
    "def gen_model(nUnits, nLayers, drop_rate, optimizer_alg, learning_rate, l2_lambda, doBN=True):\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self, nUnits, nLayers, dropout_rate, doBN):\n",
    "            super().__init__()\n",
    "\n",
    "            self.nLayers = nLayers\n",
    "            self.dropout_rate = dropout_rate\n",
    "            self.doBN = doBN\n",
    "\n",
    "            self.layers = nn.ModuleDict()\n",
    "            self.bns    = nn.ModuleDict() if doBN else None\n",
    "\n",
    "            self.layers['input'] = nn.Linear(784, nUnits)\n",
    "            if self.doBN:\n",
    "                self.bns['input'] = nn.BatchNorm1d(nUnits)\n",
    "\n",
    "            for i in range(nLayers):\n",
    "                self.layers[f'hidden{i}'] = nn.Linear(nUnits, nUnits)\n",
    "                if self.doBN:\n",
    "                    self.bns[f'hidden{i}'] = nn.BatchNorm1d(nUnits)\n",
    "\n",
    "            self.layers['output'] = nn.Linear(nUnits, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.layers['input'](x)\n",
    "            if self.doBN:\n",
    "                x = self.bns['input'](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            for i in range(self.nLayers):\n",
    "                x = self.layers[f'hidden{i}'](x)\n",
    "                if self.doBN:\n",
    "                    x = self.bns[f'hidden{i}'](x)\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "            x = self.layers['output'](x)\n",
    "            return x\n",
    "\n",
    "    ANN = mnist_FFN(nUnits, nLayers, drop_rate, doBN)\n",
    "\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    opti_fun = getattr(torch.optim, optimizer_alg)\n",
    "    optimizer = opti_fun(ANN.parameters(), lr=learning_rate, weight_decay=l2_lambda)\n",
    "\n",
    "    return ANN, loss_fun, optimizer\n",
    "\n",
    "\n",
    "def train_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate,l2_lambda):\n",
    "\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model(nUnits,nLayers,drop_rate,optimizer_alg,learning_rate,l2_lambda,doBN=True)\n",
    "\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5KmreAGKVyD"
   },
   "outputs": [],
   "source": [
    "# %% Ex. 2 - Continue ...\n",
    "\n",
    "# Takes ~12 mins (with only 2 layers, 60 units each, only 2 lr)\n",
    "num_layers     = np.linspace(2,2,1,dtype=int)\n",
    "num_units      = np.linspace(60,60,1,dtype=int)\n",
    "drop_rate      = 0\n",
    "optimizers     = ['SGD','RMSprop','Adam']\n",
    "learning_rates = np.logspace(-4,-1,2)\n",
    "l2_lambda      = 0\n",
    "\n",
    "# Preallocate output matrices\n",
    "shape             = (len(optimizers),len(learning_rates),len(num_units),len(num_layers))\n",
    "performance_train = np.zeros(shape)\n",
    "performance_test  = np.zeros(shape)\n",
    "training_times    = np.zeros(shape)\n",
    "\n",
    "# Buckle up, here's the experiment!\n",
    "for opt_i, optimizer in enumerate(optimizers):\n",
    "    for lr_i, lr in enumerate(learning_rates):\n",
    "        for unit_i, n_units in enumerate(num_units):\n",
    "            for layer_i, n_layers in enumerate(num_layers):\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_acc,test_acc,losses,ANN = train_model(\n",
    "                                                            n_units,\n",
    "                                                            n_layers,\n",
    "                                                            drop_rate,\n",
    "                                                            optimizer,\n",
    "                                                            lr,\n",
    "                                                            l2_lambda)\n",
    "\n",
    "                duration = time.time() - start_time\n",
    "\n",
    "                performance_train[opt_i, lr_i, unit_i, layer_i] = np.mean(train_acc[-10:])\n",
    "                performance_test[opt_i, lr_i, unit_i, layer_i]  = np.mean(test_acc[-10:])\n",
    "                training_times[opt_i, lr_i, unit_i, layer_i]    = duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLs6tioR8h04"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    The dataset contains 20,000 images. Do we need that many to get good accuracy? Pick one combination of\n",
    "#    optimizer and learning rate, and train the model using only 2,000 images. Do you still get >95% accuracy?\n",
    "#    How about 200 images?\n",
    "\n",
    "# Surprisingly (or not), the models with RMSprop still work quite well, while\n",
    "# SGD suffers the most, especially for smaller and smaller leraning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awLwXO6EQvzW"
   },
   "outputs": [],
   "source": [
    "# %% Ex. 3 - Continue ...\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Split labels from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# Normalise data (original range is (0,255))\n",
    "data_norm = data / np.max(data)\n",
    "\n",
    "# Set number of samples\n",
    "n_samples = 200\n",
    "\n",
    "# Shuffle and slice\n",
    "np.random.seed(99)\n",
    "shuffler = np.random.permutation(data_norm.shape[0])\n",
    "\n",
    "data_norm = data_norm[shuffler[:n_samples]]\n",
    "labels    = labels[shuffler[:n_samples]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGkOaTFXRQGg"
   },
   "outputs": [],
   "source": [
    "# %% Ex. 3 - Continue ...\n",
    "\n",
    "# Takes ~2 mins for 2000 images, and ~15 secs for 200 (with only 2 layers, 60 units each)\n",
    "num_layers     = np.linspace(2,2,1,dtype=int)\n",
    "num_units      = np.linspace(60,60,1,dtype=int)\n",
    "drop_rate      = 0\n",
    "optimizers     = ['SGD','RMSprop','Adam']\n",
    "learning_rates = np.logspace(-4,-1,6)\n",
    "\n",
    "# Preallocate output matrices\n",
    "shape             = (len(optimizers),len(learning_rates),len(num_units),len(num_layers))\n",
    "performance_train = np.zeros(shape)\n",
    "performance_test  = np.zeros(shape)\n",
    "training_times    = np.zeros(shape)\n",
    "\n",
    "# Buckle up, here's the experiment!\n",
    "for opt_i, optimizer in enumerate(optimizers):\n",
    "    for lr_i, lr in enumerate(learning_rates):\n",
    "        for unit_i, n_units in enumerate(num_units):\n",
    "            for layer_i, n_layers in enumerate(num_layers):\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                train_acc,test_acc,losses,ANN = train_model(\n",
    "                                                            n_units,\n",
    "                                                            n_layers,\n",
    "                                                            drop_rate,\n",
    "                                                            optimizer,\n",
    "                                                            lr)\n",
    "\n",
    "                duration = time.time() - start_time\n",
    "\n",
    "                performance_train[opt_i, lr_i, unit_i, layer_i] = np.mean(train_acc[-10:])\n",
    "                performance_test[opt_i, lr_i, unit_i, layer_i]  = np.mean(test_acc[-10:])\n",
    "                training_times[opt_i, lr_i, unit_i, layer_i]    = duration\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7rZWE+5IU6oVSQF5JRZa1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
