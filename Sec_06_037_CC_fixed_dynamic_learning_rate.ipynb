{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87mGPB5aukFT"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 6.37\n",
    "#    Code challenge 3: fixed vs. dynamic learning rate\n",
    "\n",
    "#    1) Use code from Sec_06_032_gradient_descent_1D\n",
    "#    2) Think about how to change the learning rate:\n",
    "#       - Time (training epoch)\n",
    "#       - Derivative\n",
    "#       - Loss\n",
    "#       - Current local minimum value\n",
    "#    3) Implement and test your ideas\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DvTQD__uzzg"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import sympy               as sym\n",
    "import copy\n",
    "\n",
    "from google.colab                     import files\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM6g7tdsu5pe"
   },
   "outputs": [],
   "source": [
    "# %% Define function\n",
    "\n",
    "# The function\n",
    "def fx(x):\n",
    "    return 3*x**2 - 3*x + 4\n",
    "\n",
    "def df(x):\n",
    "    return 6*x -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1741213494408,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "9Gaws5HswPgP",
    "outputId": "80f3ecf9-48fd-441d-952f-d3f3568ed384"
   },
   "outputs": [],
   "source": [
    "# %% Gradient descent\n",
    "#    Method 0 - No scaling\n",
    "\n",
    "x = np.linspace(-2,2,2001)\n",
    "\n",
    "# Set a min to pick for other methods too\n",
    "local_min_start = np.random.choice(x,1).item()\n",
    "local_min       = local_min_start\n",
    "\n",
    "print(f'Starting local mininum: {local_min:.8f}')\n",
    "\n",
    "learning_rate   = .01\n",
    "training_epochs = 50\n",
    "model_paramsFix = np.zeros((training_epochs,3))\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    gradient             = df(local_min)\n",
    "    lr                   = learning_rate\n",
    "    local_min            = local_min - gradient*lr\n",
    "    model_paramsFix[i,:] = local_min,gradient,lr\n",
    "\n",
    "print(f'Estimated local mininum: {local_min:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1741211778123,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "4DSv0BMRwajC",
    "outputId": "cd20f131-8507-472d-fd9f-34e0c613c54b"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "plt.plot(x,fx(x),x,df(x))\n",
    "plt.plot(local_min,df(local_min),'ro')\n",
    "plt.plot(local_min,fx(local_min),'ro')\n",
    "\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend([\"f(x)\",\"f'(x)\",\"f(x) min\"])\n",
    "plt.suptitle('A function and its derivative')\n",
    "plt.title('Empirical local minimum: %s' %np.round(local_min,4))\n",
    "\n",
    "plt.savefig('figure42_code_challenge_3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure42_code_challenge_3.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1741214730711,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "gIYAzsi_-nFx",
    "outputId": "32a86558-49c7-4577-cb22-558bc4d29b42"
   },
   "outputs": [],
   "source": [
    "# %% Gradient descent\n",
    "#    Method 1 - Scale learning rate by gradient\n",
    "#    Useful because adaptive, but for more complex functions it requires additional parameters\n",
    "#    and appropriae scaling\n",
    "#    In the industry, it is usually incorporated in optimizers such as RMSprop and Adam\n",
    "\n",
    "# Choose between a random or a fixed starting point\n",
    "local_min = local_min_start\n",
    "local_min = np.random.choice(x,1).item()\n",
    "print(f'Starting local mininum: {local_min:.8f}')\n",
    "\n",
    "learning_rate   = .01\n",
    "training_epochs = 50\n",
    "model_paramsGrd = np.zeros((training_epochs,3))\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    gradient             = df(local_min)\n",
    "    lr                   = learning_rate * abs(gradient)\n",
    "    local_min            = local_min - gradient*lr\n",
    "    model_paramsGrd[i,:] = local_min,gradient,lr\n",
    "\n",
    "print(f'Estimated local mininum: {local_min:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1741213901558,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "K5Vv709MD52a",
    "outputId": "90c0df57-1241-4f18-a4c8-5e212cdb2c4f"
   },
   "outputs": [],
   "source": [
    "# %% Gradient descent\n",
    "#    Method 2 - Scale learning rate by number of epochs\n",
    "#    Quite good, often done in blocks, but unrelated to the model performance or\n",
    "#    accuracy, from which the l.r. is caled independently\n",
    "#    In the industry it is called \"learning rate decay\"\n",
    "\n",
    "# Choose between a random or a fixed starting point\n",
    "local_min = local_min_start\n",
    "local_min = np.random.choice(x,1).item()\n",
    "print(f'Starting local mininum: {local_min:.8f}')\n",
    "\n",
    "learning_rate   = .01\n",
    "training_epochs = 50\n",
    "model_paramsTim = np.zeros((training_epochs,3))\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    gradient             = df(local_min)\n",
    "    lr                   = learning_rate * (1-(i+1)/training_epochs)\n",
    "    local_min            = local_min - gradient*lr\n",
    "    model_paramsTim[i,:] = local_min,gradient,lr\n",
    "\n",
    "print(f'Estimated local mininum: {local_min:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "elapsed": 1195,
     "status": "ok",
     "timestamp": 1741214737579,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "fb-wrcrB-srB",
    "outputId": "572625eb-17f4-4bfe-eadf-dd747c8180d2"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(13,5))\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].plot(model_paramsFix[:,i],'o-',markerfacecolor='none')\n",
    "    ax[i].plot(model_paramsGrd[:,i],'s-',markerfacecolor='none')\n",
    "    ax[i].plot(model_paramsTim[:,i],'^-',markerfacecolor='none')\n",
    "    ax[i].set_xlabel('Iteration')\n",
    "\n",
    "ax[0].set_title('Local Minimum')\n",
    "ax[1].set_title('Gradient')\n",
    "ax[2].set_title('Learning rate')\n",
    "ax[2].legend(['Fixed l.r.','Grad-based l.r.','Time-based l.r.'])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure43_code_challenge_3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure43_code_challenge_3.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjVuTjchQ5GN"
   },
   "outputs": [],
   "source": [
    "# %% Excercise 1\n",
    "#    Change the initial learning rate in the \"time\" experiment from .1 to .01. Do you still reach the same conclusion that\n",
    "#    dynamic learning rates are better than a fixed learning rate?\n",
    "\n",
    "# No, it is now actually performing quite poorly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1741214259649,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "H3ZML7NRRC_N",
    "outputId": "fbcbbd1f-ee85-4f00-aed6-694948febeb0"
   },
   "outputs": [],
   "source": [
    "# %% Excercise 2\n",
    "#    Compute the average of all time-based learning rates (see variable 'modelparamsTime'). Next, replace the fixed\n",
    "#    learning rate with the average over all dynamic learning rates. How does that affect the model's performance?\n",
    "\n",
    "avg_lr = np.mean(model_paramsTim[:,2])\n",
    "\n",
    "local_min = local_min_start\n",
    "print(f'Starting local mininum: {local_min:.8f}')\n",
    "\n",
    "learning_rate   = avg_lr\n",
    "training_epochs = 50\n",
    "model_paramsFix = np.zeros((training_epochs,3))\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    gradient             = df(local_min)\n",
    "    lr                   = learning_rate\n",
    "    local_min            = local_min - gradient*lr\n",
    "    model_paramsFix[i,:] = local_min,gradient,lr\n",
    "\n",
    "print(f'Estimated local mininum: {local_min:.8f}')\n",
    "\n",
    "fig,ax = plt.subplots(1,3,figsize=(13,5))\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].plot(model_paramsFix[:,i],'o-',markerfacecolor='none')\n",
    "    ax[i].plot(model_paramsGrd[:,i],'s-',markerfacecolor='none')\n",
    "    ax[i].plot(model_paramsTim[:,i],'^-',markerfacecolor='none')\n",
    "    ax[i].set_xlabel('Iteration')\n",
    "\n",
    "ax[0].set_title('Local Minimum')\n",
    "ax[1].set_title('Gradient')\n",
    "ax[2].set_title('Learning rate')\n",
    "ax[2].legend(['Average time-based l.r.','Grad-based l.r.','Time-based l.r.'])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure43_code_challenge_3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure43_code_challenge_3.png')\n",
    "\n",
    "# With this relatively low number of epochs, making the fixed learning rate so small\n",
    "# makes the algorithm collapse, performing even worse than the time-based dynamic\n",
    "# learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXCtIpoTRC4R"
   },
   "outputs": [],
   "source": [
    "# %% Excercise 3\n",
    "#    Going back to the original code (without the modifications above), you saw that the fixed learning rate model didn't\n",
    "#    get to the same local minimum. What happens if you increase the number of training epochs from 50 to 500? Does that\n",
    "#    improve the situation, and what does that tell you about the relationship between learning rate and training epochs?\n",
    "\n",
    "# Increasing the epochs improves the estimation of the local minimum, simply because the algorithm is given\n",
    "# the opportunity to \"learn for longer\", a similar effect is also observed for the dynamic approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31yssdomRCv6"
   },
   "outputs": [],
   "source": [
    "# %% Excercise 4\n",
    "#    The code here initializes the starting value as a random number, which will differ for each learning rate method.\n",
    "#    Is that appropriate or inappropriate for this experiment? Why? Change the code so that the starting value is the\n",
    "#    same for all three learning rate models.\n",
    "\n",
    "# In this case the random starting point constitutes a uncontrolled variable in our parametric experiment; keeping\n",
    "# the initial value constant makes things easier to interpret. Indeed, by doing so, one can see that, at least for this function,\n",
    "# this number of epochs (N=50), and an initial learning rate of 0.01, the gradient approach is outperforming all the others,\n",
    "# the fixed approach still gives a good approximation (it would do better with more epochs), while the time-based approach\n",
    "# produce a poor result\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOoMKWRZSjUx7k7YL4OnzIr",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
