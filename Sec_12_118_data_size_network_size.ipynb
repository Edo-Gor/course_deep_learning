{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajZQZPTAL35r"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 12.118\n",
    "#    Data size and network size\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH8Ro002LsPZ"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKzvhjgCo9K_"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate data\n",
    "\n",
    "def gen_data(n_per_cluster):\n",
    "\n",
    "    # Centroids\n",
    "    A = [ 1,1 ]\n",
    "    B = [ 5,1 ]\n",
    "    C = [ 4,4 ]\n",
    "\n",
    "    # Data\n",
    "    a = [ A[0]+np.random.randn(n_per_cluster), A[1]+np.random.randn(n_per_cluster) ]\n",
    "    b = [ B[0]+np.random.randn(n_per_cluster), B[1]+np.random.randn(n_per_cluster) ]\n",
    "    c = [ C[0]+np.random.randn(n_per_cluster), C[1]+np.random.randn(n_per_cluster) ]\n",
    "\n",
    "    # Labels\n",
    "    labels_np = np.hstack(( np.zeros((n_per_cluster)),\n",
    "                            np.ones((n_per_cluster)),\n",
    "                            np.ones((n_per_cluster))+1 ))\n",
    "\n",
    "    # Concatenate data into matrix, put then in a dictionary and convert to pytorch tensor\n",
    "    data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "    output = {}\n",
    "    output['data']   = torch.tensor(data_np).float()\n",
    "    output['labels'] = torch.tensor(labels_np).long()\n",
    "\n",
    "    # Split data with scikitlearn\n",
    "    train_data,test_data, train_labels,test_labels = train_test_split(output['data'],output['labels'],train_size=0.9)\n",
    "\n",
    "    # Convert to PyTorch Datasets\n",
    "    train_data = TensorDataset(train_data,train_labels)\n",
    "    test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "    # Convert to dataloader object\n",
    "    batch_size = 8\n",
    "    output['train_data'] = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last= True)\n",
    "    output['test_data']  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1753382901917,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "EsvMl72HvL7x",
    "outputId": "892d88c6-e499-434c-f861-ba783e2e66ff"
   },
   "outputs": [],
   "source": [
    "# %% Test data function\n",
    "\n",
    "# Data\n",
    "some_data = gen_data(50)\n",
    "\n",
    "data   = some_data['data']\n",
    "labels = some_data['labels']\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# Plotting\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'s',alpha=.75)\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'o',alpha=.75)\n",
    "plt.plot(data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'^',alpha=.75)\n",
    "plt.title('Some data')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "plt.savefig('figure1_data_size_network_size.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure1_data_size_network_size.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlonEJp6xA3f"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "def gen_model(n_units,n_layers):\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self,n_units,n_layers):\n",
    "            super().__init__()\n",
    "\n",
    "            # Dictionary to store layers\n",
    "            self.layers   = nn.ModuleDict()\n",
    "            self.n_layers = n_layers\n",
    "\n",
    "            # Architecture\n",
    "            self.layers['input'] = nn.Linear(2,n_units)\n",
    "\n",
    "            for i in range(1,n_layers):\n",
    "                self.layers[f'hidden_{i}'] = nn.Linear(n_units,n_units)\n",
    "\n",
    "            self.layers['output'] = nn.Linear(n_units,3)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = self.layers['input'](x)\n",
    "            for i in range(1,self.n_layers):\n",
    "                x = F.relu(self.layers[f'hidden_{i}'](x))\n",
    "            x = self.layers['output'](x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Generate model instance\n",
    "    network = model(n_units,n_layers)\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (SGD to slow down training and appreciate parametrisation)\n",
    "    optimizer = torch.optim.SGD(network.parameters(),lr=0.01)\n",
    "\n",
    "    return network,loss_function,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1753392832997,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "uT88eBejxCLa",
    "outputId": "a245f212-b68a-4b2b-ba2f-58e8d36cf5b5"
   },
   "outputs": [],
   "source": [
    "# %% Test model function on random data\n",
    "\n",
    "n_units  = 12\n",
    "n_layers = 3\n",
    "\n",
    "network,loss_function,optimizer = gen_model(n_units,n_layers)\n",
    "print(network)\n",
    "\n",
    "input = torch.rand(10,2)\n",
    "print(network(input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4rPpr4oxCFB"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model(n_units,n_layers):\n",
    "\n",
    "    # Number of epochs and model instance\n",
    "    num_epochs = 50\n",
    "    network,loss_function,optimizer = gen_model(n_units,n_layers)\n",
    "\n",
    "    # Preallocate variables\n",
    "    losses    = torch.zeros(num_epochs)\n",
    "    train_acc = torch.zeros(num_epochs)\n",
    "    test_acc  = torch.zeros(num_epochs)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Batches loop\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_data:\n",
    "\n",
    "            # Forward prop and loss\n",
    "            yHat = network(X)\n",
    "            loss = loss_function(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute loss and accuracy for this batch\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_acc.append(100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item())\n",
    "\n",
    "        # Compute loss and accuracy for the epoch\n",
    "        losses[epoch_i]    = np.mean(batch_loss)\n",
    "        train_acc[epoch_i] = np.mean(batch_acc)\n",
    "\n",
    "        # Test accuracy (switch to evaluation mode and then back to training\n",
    "        # mode to save up computation)\n",
    "        network.eval()\n",
    "        X,y = next(iter(test_data))\n",
    "        with torch.no_grad():\n",
    "            yHat = network(X)\n",
    "\n",
    "        test_acc[epoch_i] = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item()\n",
    "        network.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1753384808000,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "PTt6F3NTxEq1",
    "outputId": "26a787e7-0fe3-4574-b551-8795515d5627"
   },
   "outputs": [],
   "source": [
    "# %% Test the entire machinery with some data\n",
    "\n",
    "some_data = gen_data(200)\n",
    "train_data = some_data['train_data']\n",
    "test_data  = some_data['test_data']\n",
    "\n",
    "train_acc,test_acc,losses,model = train_model(60,1)\n",
    "\n",
    "# Plotting\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].plot(losses.detach())\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_title('Losses')\n",
    "\n",
    "ax[1].plot(train_acc,label='Train accuracy')\n",
    "ax[1].plot(test_acc,label='Test accuracy')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure2_data_size_network_size.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure2_data_size_network_size.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liuldn7R1Wxt"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment\n",
    "\n",
    "# Parametrically vary depth of the model, while keeping the number of units\n",
    "# constant; parametrically vary the amount of data as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1753385176605,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "hqT2A0vq1Wem",
    "outputId": "4340ba09-d9ca-4601-fa0b-c1b4d344ff6f"
   },
   "outputs": [],
   "source": [
    "# %% Before the experiment, configure and confirm the metaparameters\n",
    "\n",
    "# specify the parameters for the model\n",
    "total_nodes    = 80\n",
    "layers_range   = [ 1,5,10,20 ]\n",
    "n_datapoints   = np.arange(50,551,50)\n",
    "\n",
    "# Legend for later plotting\n",
    "legend = []\n",
    "\n",
    "# Print out model architectures\n",
    "for layer_i,layers in enumerate(layers_range):\n",
    "\n",
    "    # create a model\n",
    "    units_by_layer = int(total_nodes/layers_range[layer_i])\n",
    "    network        = gen_model(units_by_layer,layers)[0]\n",
    "\n",
    "    # count its parameters (see also lecture ANNs: depth vs. breadth)\n",
    "    n_params = np.sum([ p.numel() for p in network.parameters() if p.requires_grad ])\n",
    "\n",
    "    legend.append( '%s layers, %s units, %s parameters' %(layers,units_by_layer,n_params) )\n",
    "    print('This model will have %s layers, each with %s units, totalling %s parameters' %(layers,units_by_layer,n_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nGxH1Lz1WXF"
   },
   "outputs": [],
   "source": [
    "# %% Proper experiment\n",
    "\n",
    "# initialize results matrix\n",
    "results = np.zeros(( len(n_datapoints),len(layers_range),2 ))\n",
    "\n",
    "# Takes ~6 mins\n",
    "for data_i,datapoints in enumerate(n_datapoints):\n",
    "\n",
    "    # create data (note: same data for each layer manipulation!)\n",
    "    data       = gen_data(datapoints)\n",
    "    train_data = data['train_data']\n",
    "    test_data  = data['test_data']\n",
    "\n",
    "    # Loop over layers\n",
    "    for layer_i,layers in enumerate(layers_range):\n",
    "\n",
    "        units_by_layer = int(total_nodes/layers_range[layer_i])\n",
    "        train_acc,test_acc,losses,model = train_model(units_by_layer,layers)\n",
    "\n",
    "        # Average of last 5 accuracies and losses\n",
    "        results[data_i,layer_i,0] = torch.mean(test_acc[-5:])\n",
    "        results[data_i,layer_i,1] = torch.mean(losses[-5:]).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 673,
     "status": "ok",
     "timestamp": 1753393571788,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "ka_-jZDf9Cg7",
    "outputId": "2caa6e79-614d-4ef2-a6af-c48f4b487387"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(0.2,0.9,len(layers_range)))\n",
    "for layer_i,layers in enumerate(layers_range):\n",
    "    ax[0].plot(n_datapoints,results[:,layer_i,1],'o-',label=legend[layer_i],color=cmaps[layer_i],alpha=0.5)\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Number of data points')\n",
    "ax[0].legend(legend)\n",
    "ax[0].set_title('Losses')\n",
    "\n",
    "for layer_i,layers in enumerate(layers_range):\n",
    "    ax[1].plot(n_datapoints,results[:,layer_i,0],'o-',label=legend[layer_i],color=cmaps[layer_i],alpha=0.5)\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_xlabel('Number of data points')\n",
    "ax[1].set_title('Test accuracy')\n",
    "ax[1].legend(legend)\n",
    "\n",
    "plt.savefig('figure3_data_size_network_size.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure3_data_size_network_size.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5PJbFl-9FLp"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    The model learns faster and better with the Adam optimizer. In fact, I intentionally used SGD here to make the\n",
    "#    model worse for this demonstration! Change the optimizer to Adam. What do you think is a good learning rate?\n",
    "#    More importantly: Do the conclusions of this experiment hold for the Adam optimizer?\n",
    "\n",
    "# The learning rate is relatively \"\"irrelevant\"\" (stress on the quotes), because\n",
    "# with the various strategies like adding momentum, RMSprop or Adam, the lr is\n",
    "# set to be adaptive on some parameters. As for the conclusions (it takes a bit\n",
    "# longer; ~11 mins), there are some quantitative changes in the sense that the\n",
    "# 3rd deepest model performs better, but the overall conclusions do not change,\n",
    "# for these data, shallow models do better than deep models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1753397988765,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "4tl92Llw9FHU",
    "outputId": "fca5a633-313d-4f4c-8ec3-f742270ef269"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Add a timer to the experiment loop. Does the training duration relate to the number of layers or the number\n",
    "#    of parameters?\n",
    "\n",
    "# The training duration seems to nicely depend on both the amount of data and the\n",
    "# number of layers; the number of trainable parameters, however, doesn't seem\n",
    "# to be directly related to training time\n",
    "\n",
    "# Experiment with timer (with SGD)\n",
    "results  = np.zeros(( len(n_datapoints),len(layers_range),2 ))\n",
    "timings  = np.zeros(( len(n_datapoints),len(layers_range) ))\n",
    "n_params = np.zeros(( len(n_datapoints),len(layers_range) ))\n",
    "\n",
    "for data_i,datapoints in enumerate(n_datapoints):\n",
    "\n",
    "    data       = gen_data(datapoints)\n",
    "    train_data = data['train_data']\n",
    "    test_data  = data['test_data']\n",
    "\n",
    "    for layer_i,layers in enumerate(layers_range):\n",
    "\n",
    "        units_by_layer = int(total_nodes/layers_range[layer_i])\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_acc,test_acc,losses,model = train_model(units_by_layer,layers)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        timings[data_i, layer_i] = elapsed_time\n",
    "\n",
    "        results[data_i,layer_i,0] = torch.mean(test_acc[-5:])\n",
    "        results[data_i,layer_i,1] = torch.mean(losses[-5:]).item()\n",
    "        n_params[data_i,layer_i]  = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "        print(f\"Data: {datapoints}, Layers: {layers}, Time: {elapsed_time:.2f}s\")\n",
    "\n",
    "# Plotting\n",
    "x_labels     = []\n",
    "x_param_vals = []\n",
    "\n",
    "for j,layers in enumerate(layers_range):\n",
    "\n",
    "    num_params = int(n_params[0, j])\n",
    "    x_labels.append(f'{num_params:,}\\n({layers} layers)')\n",
    "    x_param_vals.append(num_params)\n",
    "\n",
    "y_labels = [str(n) for n in n_datapoints]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(phi*6,6))\n",
    "im = ax.imshow(timings,cmap='plasma',aspect='auto')\n",
    "\n",
    "ax.set_xticks(range(len(x_labels)))\n",
    "ax.set_xticklabels(x_labels,rotation=45,ha='right')\n",
    "ax.set_xlabel('Number of parameters and of layers')\n",
    "\n",
    "ax.set_yticks(range(len(y_labels)))\n",
    "ax.set_yticklabels(y_labels)\n",
    "ax.set_ylabel('Number of datapoints')\n",
    "\n",
    "cbar = plt.colorbar(im,ax=ax)\n",
    "cbar.set_label('Training time (s)',rotation=270,labelpad=15)\n",
    "\n",
    "for i in range(len(n_datapoints)):\n",
    "    for j in range(len(layers_range)):\n",
    "        val = timings[i,j]\n",
    "        text_color = 'white' if val < np.max(timings) * 0.5 else 'black'\n",
    "        ax.text(j,i,f'{val:.0f} s',ha='center',va='center',color=text_color)\n",
    "\n",
    "ax.set_title('Training time for number of parameters, layers and datapoints')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure5_data_size_network_size_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure5_data_size_network_size_extra2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXH97oWH9FBj"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    Do the two deepest models eventually learn if you increase the number of training epochs? (Note: because this\n",
    "#    question is only about the deepest models and because training time will increase, you need only test the two\n",
    "#    models, not all four.)\n",
    "\n",
    "# Tried with 200 iterations (takes ~26 min) and SGD. There is a slight improvement\n",
    "# in which the 3rd deepest model reaches a good performance in a couple of runs\n",
    "# with relatively a lot of data; that said, more iterations or a better optimizer\n",
    "# would definitely help\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNIeQ0voFX23lmSKDLJydNj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
