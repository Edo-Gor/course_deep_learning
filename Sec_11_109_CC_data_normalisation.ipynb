{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxLsu_5_Ao80"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 11.109\n",
    "#    Code challenge 15: data normalisation\n",
    "\n",
    "#    1) Start drom code from video 11.108\n",
    "#    2) Use the same model architecture and metaparameters\n",
    "#    3) Try the three following type of data normalisation (min-max scaling\n",
    "#       between 0 and 1, and confirm by printing data range):\n",
    "#       I)   Normalise train and test data\n",
    "#       II)  Normalise train but not test data\n",
    "#       III) Normalise test but not train data\n",
    "#    4) Plot accuracy and loss for each of them\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFOW2iaUArr4"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_DNgJfDQKl2"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Split labels from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1751642714017,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "64VpgZccuiyC",
    "outputId": "be1d5908-eda8-474e-f1eb-737d5ee35b71"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Convert to tensor (float and integers)\n",
    "data_tensor   = torch.tensor(data).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "# Split data with scikitlearn (10% test data)\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "# Normalise train and/or test data separately (original range is (0,255))\n",
    "train_data = train_data / torch.max(train_data)\n",
    "test_data  = test_data  / torch.max(test_data)\n",
    "\n",
    "print( f'Range of train data: [ {train_data.min():.1f} , {train_data.max():.1f} ]' )\n",
    "print( f'Range of test data:  [ {test_data.min():.1f} , {test_data.max():.1f} ]' )\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO5LXN3D87TB"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.fc1    = nn.Linear( 64,32)\n",
    "            self.fc2    = nn.Linear( 32,32)\n",
    "            self.output = nn.Linear( 32,10)\n",
    "\n",
    "        # Forward propagation (log-softmax because NLLLoss instead of CrossEntropyLoss)\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = torch.log_softmax( self.output(x),axis=1 )\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create model instance\n",
    "    ANN = mnist_FFN()\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.NLLLoss()\n",
    "\n",
    "    # Optimizer (SGD to slow down learning for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1751641572795,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "1P-V22PH62Rx",
    "outputId": "c7a13f0a-62a8-46c7-cf80-7cc7ea7bca87"
   },
   "outputs": [],
   "source": [
    "# Test the model on one batch\n",
    "\n",
    "ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "X,y  = next(iter(train_loader))\n",
    "yHat = ANN(X)\n",
    "\n",
    "# Print log-softmax output (size should be batch_size by output nodes)\n",
    "print(yHat)\n",
    "print(yHat.shape)\n",
    "print()\n",
    "\n",
    "# Print probabilities\n",
    "print(torch.exp(yHat))\n",
    "print()\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fun(yHat,y)\n",
    "print('Loss: ')\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BPr1HkE8WXF"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    losses_trn = []\n",
    "    losses_tst = []\n",
    "    train_acc  = []\n",
    "    test_acc   = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses_trn.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "        loss = loss_fun(yHat,y)\n",
    "        losses_tst.append(loss.item())\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses_trn,losses_tst,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FEWUdEM-bTc"
   },
   "outputs": [],
   "source": [
    "# %% Run the training\n",
    "\n",
    "train_acc,test_acc,losses_trn,losses_tst,ANN = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "executionInfo": {
     "elapsed": 3198,
     "status": "ok",
     "timestamp": 1751642810970,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "2b603IZ0ujg7",
    "outputId": "707e8435-8b86-4af1-df88-b80bac98bd4b"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].plot(losses_trn,label='Train loss')\n",
    "ax[0].plot(losses_tst,label='Test loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(train_acc,label='Train accuracy')\n",
    "ax[1].plot(test_acc,label='Test accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%\\nNormalised train and test data')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure23_code_challenge_15.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure23_code_challenge_15.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1751642616852,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "XkOwcI7O2sWU",
    "outputId": "c8a8c7ed-85f4-40e0-bf39-36f95703c02e"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Try different normalizations, e.g., [-1 0] or [10 15]. How do these affect learning?\n",
    "\n",
    "# The hypothesis is that as long as the train and test data are normalised, the\n",
    "# results shouldn't change based on the normalisation range (i.e., [0,1] is just\n",
    "# a practical range). The output seems to support this hypothesis for the range\n",
    "# [-1,0], but for the range [10,15] the performance seems to be a bit affected\n",
    "# since the accuracy drops slightly; I admit I'm a bit puzzled by this.\n",
    "\n",
    "# Modify range of normalisation (normalise between 0 and 1 and then scale)\n",
    "data_tensor   = torch.tensor(data).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "a,b = -1,0\n",
    "train_data = train_data / torch.max(train_data)\n",
    "test_data  = test_data  / torch.max(test_data)\n",
    "train_data = train_data * (b-a) + a\n",
    "test_data  = test_data  * (b-a) + a\n",
    "print( f'Range of train data: [ {train_data.min():.1f} , {train_data.max():.1f} ]' )\n",
    "print( f'Range of test data:  [ {test_data.min():.1f} , {test_data.max():.1f} ]' )\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1751642926891,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "jQOtEpu25W7q",
    "outputId": "1e1f096e-cbfa-4da7-8c92-460aed90064e"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Correlate loss and accuracy over epochs, and visualize in a scatterplot. Do the loss and\n",
    "#    accuracy functions really provide different information, or do the two variables reflect\n",
    "#    the same underlying performance? (Note that your conclusion here is based exclusively on this\n",
    "#    dataset and this architecture!)\n",
    "\n",
    "# In this specific case, the correlation is nearly perfect, in the sense that as\n",
    "# losses decrease, accuracy increases according to a linear map; so one could\n",
    "# say that here the two variables provide basically the same information, even\n",
    "# though comnceptually they are very different (loss tells you whether the\n",
    "# model is learning, accuracy how good is the overall performance); also note\n",
    "# that this graph shows only data for a min-max normalisation [0,1] for both\n",
    "# train and test data\n",
    "\n",
    "# Compute correlations\n",
    "corr_train = np.corrcoef(losses_trn,train_acc)[0,1]\n",
    "corr_test  = np.corrcoef(losses_tst,test_acc)[0,1]\n",
    "print(f\"Correlation (Train loss vs. train accuracy): {corr_train:.3f}\")\n",
    "print(f\"Correlation (Test loss vs. test accuracy):  {corr_test:.3f}\")\n",
    "\n",
    "# Plotting\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].scatter(losses_trn,train_acc,color='royalblue',alpha=0.7)\n",
    "ax[0].set_title(f'Train loss vs. train accuracy\\n(r = {corr_train:.3f})')\n",
    "ax[0].set_xlabel('Train loss')\n",
    "ax[0].set_ylabel('Train Accuracy (%)')\n",
    "train_fit = np.polyfit(losses_trn,train_acc,1)\n",
    "x_vals    = np.linspace(min(losses_trn),max(losses_trn),100)\n",
    "y_vals    = train_fit[0] * x_vals + train_fit[1]\n",
    "ax[0].plot(x_vals,y_vals,linestyle='--',color='blue',linewidth=2,alpha=0.6)\n",
    "\n",
    "ax[1].scatter(losses_tst,test_acc,color='crimson',alpha=0.7)\n",
    "ax[1].set_title(f'Test loss vs. test accuracy\\n(r = {corr_test:.3f})')\n",
    "ax[1].set_xlabel('Test loss')\n",
    "ax[1].set_ylabel('Test Accuracy (%)')\n",
    "test_fit = np.polyfit(losses_tst,test_acc,1)\n",
    "x_vals   = np.linspace(min(losses_tst),max(losses_tst),100)\n",
    "y_vals   = test_fit[0] * x_vals + test_fit[1]\n",
    "ax[1].plot(x_vals,y_vals,linestyle='--',color='red',linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure28_code_challenge_15_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure28_code_challenge_15_extra2.png')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMC/Vvr6nF/1atdCyzxIcSI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
