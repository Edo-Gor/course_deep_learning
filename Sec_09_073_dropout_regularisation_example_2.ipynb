{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le5MsLuthBqv"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 9.73\n",
    "#    Dropout example 2\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty5MFy8KhV0e"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tjETYHmr_Py"
   },
   "outputs": [],
   "source": [
    "# %% Import Iris dataset\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Convert from pandas df to tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Species to numbers\n",
    "labels = torch.zeros(len(data),dtype=torch.long)\n",
    "labels[iris.species=='setosa']     = 0\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWzWytSwsvtT"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 16\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_f3o4TQou8tx"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "class model_class(nn.Module):\n",
    "    def __init__(self,dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear( 4,12)\n",
    "        self.hidden = nn.Linear(12,12)\n",
    "        self.output = nn.Linear(12,3 )\n",
    "\n",
    "        # Parameters\n",
    "        self.dr = dropout_rate\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # Input (switch off dropout during evaluation)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Hidden\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1746312313777,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "CQSZdwC6vSOi",
    "outputId": "4c80dd9f-0218-45e8-d440-5bc6f657f7ca"
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "\n",
    "tmp_net  = model_class(dropout_rate=0.25)\n",
    "tmp_data = torch.randn((10,4))\n",
    "\n",
    "yHat = tmp_net(tmp_data)\n",
    "yHat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UhjG6nKv9-B"
   },
   "outputs": [],
   "source": [
    "# %% Function to create model instance\n",
    "\n",
    "def gen_model(dropout_rate):\n",
    "\n",
    "    # Model instance, loss, optimizer\n",
    "    ANN       = model_class(dropout_rate)\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.005)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWrKTi6wwvYa"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Global parameters\n",
    "num_epochs = 500\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Accuracies initialise\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Switch learning on\n",
    "        ANN.train()\n",
    "\n",
    "        # Loop over training data batches\n",
    "        batch_acc = []\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Training accuracy for current batch\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()).item() )\n",
    "\n",
    "        # Get average accuracy over epochs\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "        X,y         = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append( 100*torch.mean((pred_labels==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4Mjre4Y0x_Z"
   },
   "outputs": [],
   "source": [
    "# %% Test model\n",
    "\n",
    "dropoutRate            = 0.0\n",
    "ANN,loss_fun,optimizer = gen_model(dropoutRate)\n",
    "train_acc,test_acc     = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1746312337365,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "C0cV3Zrt1mcA",
    "outputId": "6a091e97-3abe-407c-becc-d924067bc32d"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(train_acc,'s-')\n",
    "plt.plot(test_acc,'o-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.title(f'Dropout rate = {dropoutRate}')\n",
    "\n",
    "plt.savefig('figure12_dropout_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure12_dropout_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00Mw6opy7Uiw"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment on dropout rates\n",
    "\n",
    "dropoutRates = np.arange(10)/10\n",
    "results      = np.zeros((len(dropoutRates),2))\n",
    "\n",
    "for drop_i in range(len(dropoutRates)):\n",
    "\n",
    "    # Generate and train the model\n",
    "    ANN,loss_fun,optimizer = gen_model(dropoutRates[drop_i])\n",
    "    train_acc,test_acc     = train_model()\n",
    "\n",
    "    # Store accuracies\n",
    "    results[drop_i,0] = np.mean(train_acc[-50:])\n",
    "    results[drop_i,1] = np.mean(test_acc[-50:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1746312456234,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "D_YOLmDO7WNa",
    "outputId": "2f05d84b-8ec7-41d3-f083-0328ade9ad88"
   },
   "outputs": [],
   "source": [
    "# plot the experiment results\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(dropoutRates,results,'o-')\n",
    "ax[0].set_xlabel('Dropout proportion')\n",
    "ax[0].set_ylabel('Average accuracy')\n",
    "ax[0].legend(['Train','Test'])\n",
    "\n",
    "ax[1].plot(dropoutRates,-np.diff(results,axis=1),'o-')\n",
    "ax[1].plot([0,.9],[0,0],'k--')\n",
    "ax[1].set_xlabel('Dropout proportion')\n",
    "ax[1].set_ylabel('Train-test difference (acc%)')\n",
    "\n",
    "plt.savefig('figure13_dropout_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure13_dropout_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRsZVOm9KOAI"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Remove the ReLU nonlinearity from the network. Does that change the effect of dropout proportion on performance?\n",
    "\n",
    "# Interesting case, with a linear model the training accuracy seems to decrease with a somewhat quadratic pattern,\n",
    "# while the test accuracy stays constant, no matter the dropout proportion. Not sure about how to explain to myself\n",
    "# this discrepancy. Sure the linear model boils down to a single layer, no matter the complexity of the original\n",
    "# network, but I would have expected the training data and the test data to behave similarly (?).\n",
    "\n",
    "class model_class(nn.Module):\n",
    "    def __init__(self,dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear( 4,12)\n",
    "        self.hidden = nn.Linear(12,12)\n",
    "        self.output = nn.Linear(12,3 )\n",
    "\n",
    "        # Parameters\n",
    "        self.dr = dropout_rate\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # Input (switch off dropout during evaluation)\n",
    "        x = self.input(x)\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Hidden\n",
    "        x = self.hidden(x)\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2T1OAr2K5hF"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    I mentioned that dropout doesn't necessarily improve performance for shallow models. What happens if you increase\n",
    "#    the complexity of this model, for example by adding several additional (and wider) hidden layers?\n",
    "\n",
    "# Here I tried two options, (1) keep the same depth and make the model broader, and (2) increase the depth\n",
    "# rather than the width (see code below). In both cases, making the model more complex seems to help a bit\n",
    "# with the performance and the dropout regularisation; the performance with both the training and test data\n",
    "# is \"pushed up\" a bit and stays higher for longer before collapsing for really high values of dropout rate.\n",
    "# So yes, as mentioned in the lectures, dropout regularisation benefits from more complex models (among\n",
    "# other things).\n",
    "\n",
    "# Wider model\n",
    "class model_class(nn.Module):\n",
    "    def __init__(self,dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear(  4,128)\n",
    "        self.hidden = nn.Linear(128,128)\n",
    "        self.output = nn.Linear(128,3 )\n",
    "\n",
    "        # Parameters\n",
    "        self.dr = dropout_rate\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # Input (switch off dropout during evaluation)\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Hidden\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Output\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Deeper model\n",
    "class model_class_deep(nn.Module):\n",
    "    def __init__(self,dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input    = nn.Linear( 4,12)\n",
    "        self.hidden1  = nn.Linear(12,12)\n",
    "        self.hidden2  = nn.Linear(12,12)\n",
    "        self.hidden3  = nn.Linear(12,12)\n",
    "        self.hidden4  = nn.Linear(12,12)\n",
    "        self.output   = nn.Linear(12,3 )\n",
    "\n",
    "        self.dr = dropout_rate\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Input layer\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Hidden layer 1\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Hidden layer 2\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Hidden layer 3\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Hidden layer 4\n",
    "        x = F.relu(self.hidden4(x))\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuRC7mF5DTbAkzenx4Sdfw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
