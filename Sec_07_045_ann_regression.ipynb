{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnGhWxHNZ_T3"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 7.45\n",
    "#    ANN for regression\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vdkDRf-bDse"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import copy\n",
    "\n",
    "from google.colab                     import files\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1741613567991,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "teoXt8uX-W86",
    "outputId": "cd0d5ace-faa8-4cf0-daf6-08dd2c956140"
   },
   "outputs": [],
   "source": [
    "# %% Create data\n",
    "\n",
    "n = 30\n",
    "x = torch.randn(n,1)\n",
    "y = x + torch.randn(n,1)/2\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x,y,'s')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Some data')\n",
    "\n",
    "plt.savefig('figure1_ann_regression.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure1_ann_regression.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1741613572730,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "bJ2PzEzfoq3Y",
    "outputId": "8defd409-0077-47af-951e-3254648bb5c6"
   },
   "outputs": [],
   "source": [
    "# %% Build the model\n",
    "\n",
    "ANNreg = nn.Sequential(\n",
    "            nn.Linear(1,1),   # input layer (num inputs, num outputs)\n",
    "            nn.ReLU(),        # activation function\n",
    "            nn.Linear(1,1)    # output layer (num inputs, num outputs)\n",
    "            )\n",
    "\n",
    "ANNreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ0LCQKRpons"
   },
   "outputs": [],
   "source": [
    "# %% Training parameters\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Loss function\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "# Optimizer (i.e. the flavour of gradient to implement; here stocastic gradient descent)\n",
    "optimizer = torch.optim.SGD(ANNreg.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ekwetS8qcYe"
   },
   "outputs": [],
   "source": [
    "# %% Train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 500\n",
    "losses = torch.zeros(num_epochs)\n",
    "\n",
    "## Training\n",
    "for epoch_i in range(num_epochs):\n",
    "\n",
    "    # Forward propagation\n",
    "    yHat = ANNreg(x)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fun(yHat,y)\n",
    "    losses[epoch_i] = loss\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1741613595619,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "ExzraD1Yr7oi",
    "outputId": "c5a5710e-55cc-495e-e59e-e846bb62f5a3"
   },
   "outputs": [],
   "source": [
    "# %% Show losses\n",
    "\n",
    "# Final forward pass\n",
    "predictions = ANNreg(x)\n",
    "\n",
    "# Final loss (MSE)\n",
    "testloss    = (predictions-y).pow(2).mean()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n",
    "plt.plot(num_epochs,testloss.detach(),'ro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Losses')\n",
    "plt.title(f'Final loss = {round(testloss.item(),4)}')\n",
    "\n",
    "plt.savefig('figure2_ann_regression.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure2_ann_regression.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1741613603100,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "R4hUQkFtxJco",
    "outputId": "33231f01-0385-4138-93ac-3f8cb4652186"
   },
   "outputs": [],
   "source": [
    "# %% Show data\n",
    "\n",
    "plt.plot(x,y,'bo',label='Real data')\n",
    "plt.plot(x,predictions.detach(),'rs',label='Predictions')\n",
    "plt.title(f'Data-prediction correlation: r = {np.round(np.corrcoef(y.T,predictions.detach().T)[0,1],2)}')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('figure3_ann_regression.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure3_ann_regression.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPMhh-Ddynnh"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    How much data is \"enough\"? Try different values of N and see how low the loss gets.\n",
    "#    Do you still get low loss (\"low\" is subjective, but let's say loss<.25) with N=10? N=5?\n",
    "\n",
    "# In statistics the sample size is always an important factor; in the case of a regression, a low\n",
    "# sample size increases the risk that the regression coeffiecient is heavily influenced by noise\n",
    "# and/or outliers. In deep learning, I guess a similar consideration is also valid, since fewer\n",
    "# data points make the error landscape more 'rugged'; a further consideration is that DL models\n",
    "# include some randomness in the computation of the weights, while a linear regression has a closed\n",
    "# form solution that guarantees we are getting the best possible solution for the given data.\n",
    "# Bottom line, more data is better, but given a fortunate starting point and low noise, it's still\n",
    "# not impossible to get good results even with small samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKV2_f5NzXg3"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Does your conclusion above depend on the amount of noise in the data? Try changing the noise level\n",
    "#    by changing the division (\"/2\") when creating y as x+randn.\n",
    "\n",
    "# Similar considerations to the point above. More noise is in a certain sense similar to less data points,\n",
    "# in the sense that both these factors tend to increase the variance, and thus the MSE. For example, training\n",
    "# the same model with n = 30 but with a noise of : y = x + torch.randn(n,1) produces a lower performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1741560095073,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "duQt62UtzXMS",
    "outputId": "4c564cf6-4b09-4f19-eb3a-ed0905745df6"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    Notice that the model doesn't always work well. Put the original code (that is, N=30 and /2 noise)\n",
    "#    into a function or a for-loop and repeat the training 100 times (each time using a fresh model instance).\n",
    "#    Then count the number of times the model had a loss>.25.\n",
    "\n",
    "# Create data\n",
    "n = 30\n",
    "x = torch.randn(n,1)\n",
    "y = x + torch.randn(n,1)/2\n",
    "\n",
    "# Repeat training 100 times\n",
    "testloss = torch.zeros(100)\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    # Build the model\n",
    "    ANNreg = nn.Sequential(\n",
    "                nn.Linear(1,1),   # input layer (num inputs, num outputs)\n",
    "                nn.ReLU(),        # activation function\n",
    "                nn.Linear(1,1)    # output layer (num inputs, num outputs)\n",
    "                )\n",
    "\n",
    "    # Training parameters\n",
    "    learning_rate = 0.05\n",
    "    loss_fun = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(ANNreg.parameters(),lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 500\n",
    "    losses = torch.zeros(num_epochs)\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        yHat = ANNreg(x)\n",
    "\n",
    "        loss = loss_fun(yHat,y)\n",
    "        losses[epoch_i] = loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Final forward pass and loss\n",
    "    predictions = ANNreg(x)\n",
    "    testloss[i] = (predictions-y).pow(2).mean()\n",
    "\n",
    "low_loss_count = (testloss < 0.25).sum().item()\n",
    "\n",
    "print(f'Proportion of losses < 0.25: {low_loss_count/len(testloss)}')\n",
    "\n",
    "# Indeed, as mentioned above, the stocasticity inherent to DL model does not guarantee\n",
    "# to get good approximation of our data, especially for small datasets where the loss function\n",
    "# might be affected by the presence of poor local minima (an event whose probability should\n",
    "# decrease for higher dimentional datasets). This is coherent with the fact that the solution\n",
    "# is either quite good or totally bad, no middle ground\n",
    "\n",
    "# See also histogram\n",
    "plt.hist(testloss.detach().numpy(),bins=100,edgecolor='black',alpha=0.7)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of loss values')\n",
    "\n",
    "plt.savefig('figure13_ann_regression_extra3.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure13_ann_regression_extra3.png')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqW23Ftk8/AO4n5eV3KaRX",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
