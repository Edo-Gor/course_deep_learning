{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dm1OulQmGS7Y"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 11.113\n",
    "#    Scrambled MNIST\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VRnO_4QGiWI"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJokslogtW1t"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Split labels from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# Normalise data (original range is (0,255))\n",
    "data_norm = data / np.max(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "executionInfo": {
     "elapsed": 2518,
     "status": "ok",
     "timestamp": 1751912581977,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "9lGYio47HASC",
    "outputId": "deb0a2f6-2934-4372-9aed-c63fc8b284b3"
   },
   "outputs": [],
   "source": [
    "# %% Scrambled data\n",
    "\n",
    "permute   = np.random.permutation(data.shape[1])\n",
    "scrambled = data_norm[:,permute]\n",
    "\n",
    "# show a few random digits\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,axs = plt.subplots(3,4,figsize=(1.5*6*phi,6))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "\n",
    "  randimg2show = np.random.randint(0,high=data.shape[0])\n",
    "\n",
    "  img = np.reshape(scrambled[randimg2show,:],(28,28))\n",
    "  ax.imshow(img,cmap='gray')\n",
    "  ax.set_title('The number %i'%labels[randimg2show])\n",
    "\n",
    "plt.suptitle('The scrambled data',fontsize=14)\n",
    "plt.tight_layout(rect=[0,0,1,.95])\n",
    "\n",
    "plt.savefig('figure57_scrambled_mnist.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure57_scrambled_mnist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5IiMkI-v6EI"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Convert to tensor (float and integers) the scrambled data\n",
    "data_tensor   = torch.tensor(scrambled).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "# Split data with scikitlearn (10% test data)\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goF1yTSeGp08"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.fc1    = nn.Linear( 64,32)\n",
    "            self.fc2    = nn.Linear( 32,32)\n",
    "            self.output = nn.Linear( 32,10)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create model instance\n",
    "    ANN = mnist_FFN()\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (SGD to slow down learning for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L1y1mi6JN5_"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    losses_trn = []\n",
    "    losses_tst = []\n",
    "    train_acc  = []\n",
    "    test_acc   = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses_trn.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "        loss = loss_fun(yHat,y)\n",
    "        losses_tst.append(loss.item())\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses_trn,losses_tst,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBhga_LjJbv8"
   },
   "outputs": [],
   "source": [
    "# %% Fit the model\n",
    "\n",
    "train_acc,test_acc,losses_trn,losses_tst,ANN = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1751913448269,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "bXCLxM_wJa8G",
    "outputId": "1a7f9c2a-d16e-4eb6-a764-fec8ac24f0f3"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].plot(losses_trn,label='Train loss')\n",
    "ax[0].plot(losses_tst,label='Test loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_acc,label='Train accuracy')\n",
    "ax[1].plot(test_acc,label='Test accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%\\nNormalised train and test data')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure58_scrambled_mnist.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure58_scrambled_mnist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYfr-T8nJ8lq"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    We scrambled all images using the same permuted index. What would happen if each image is uniquely\n",
    "#    randomly scrambled? Rewrite the scrambling code so that each individual stimulus has a unique,\n",
    "#    random permuted index. What do the results show and why is that the case?\n",
    "\n",
    "# So, in this case the performace collapses. It makes sense because the model\n",
    "# can learn patterns, even though not spatial patterns in this case, and the\n",
    "# shuffling with one index basically preserves the statistical structure of the\n",
    "# digits, despite looking unrecognaisable to an human eye. This is already\n",
    "# interesting beacuse it shows how these models do not \"recognise\" digits, they\n",
    "# simply classify images based on their statistics, assigning them to different\n",
    "# classes also based on the nuber of units in the output layers (i.e., the model\n",
    "# also works because we know in advance we are classifying 10 numbers); and this\n",
    "# is indeed possibe even when the images are shuffled, but consistently. If,\n",
    "# however, the images are \"inconsistently\" scrambled, the model cannot learn to\n",
    "# classify the images, because there is no statistical structure to build upon.\n",
    "\n",
    "# Unique scambling (preallocate and run)\n",
    "scrambled = np.zeros_like(data_norm)\n",
    "\n",
    "for i in range(data_norm.shape[0]):\n",
    "    permute      = np.random.permutation(data_norm.shape[1])\n",
    "    scrambled[i] = data_norm[i,permute]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPMAWBYWrAKuVQeowwlznmR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
