{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMgnUGzxK6KY"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 10.93\n",
    "#    Code challenge 11: predict sugar\n",
    "\n",
    "#    1) Start drom code from video 10.083\n",
    "#    2) Predict 'residual sugar' instead of quality\n",
    "#    3) Use only one batch size (unless you want to explore)\n",
    "#    4) Plot train/test losses and model prediction vs. observation, along with\n",
    "#       correlation coefficients for train and test\n",
    "#    5) Produce a correlation matrix of the model features and interpret them\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve2hqLuTLPez"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1748721043520,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "uB-rMtcjW5YF",
    "outputId": "3909ef04-cfab-42e4-8362-ca3a4c9772c1"
   },
   "outputs": [],
   "source": [
    "# %% Load and prepare data\n",
    "\n",
    "# Load\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url,sep=';')\n",
    "\n",
    "# Remove some outliers (see lec. 82 for why)\n",
    "data = data[data['total sulfur dioxide']<200]\n",
    "\n",
    "# Z-score all the variables\n",
    "cols2zscore = data.keys()\n",
    "\n",
    "for col in cols2zscore:\n",
    "    mean_val  = np.mean(data[col])\n",
    "    std_val   = np.std(data[col])\n",
    "    data[col] = (data[col] - mean_val) / std_val\n",
    "\n",
    "# Convert from pandas dataframe to PyTorch tensor (remove sugar before\n",
    "# generating the data tensor)\n",
    "cols2zscore = cols2zscore.drop('residual sugar')\n",
    "data_t      = torch.tensor( data[cols2zscore].values ).float()\n",
    "labels      = torch.tensor( data['residual sugar'].values ).float()\n",
    "\n",
    "print(f'Data shape: {data_t.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "# Labels need to be multidimentional for PyTorch, not an array, and need to be long integers too\n",
    "labels = labels[:,None]\n",
    "print(f'Proper labels shape: {labels.shape}')\n",
    "\n",
    "# Reminder of what the data are actually like\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRbJ74cbW5YG"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_t,labels,test_size=0.1)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "# > the train_loader is moved inside the train_model() function to allow a parametric test of the batch size\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7dZG1GYQm0n"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "class model_class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear(11,32)\n",
    "        self.hid1   = nn.Linear(32,32)\n",
    "        self.hid2   = nn.Linear(32,32)\n",
    "        self.output = nn.Linear(32,1)\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hid1(x))\n",
    "        x = F.relu(self.hid2(x))\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMe8OYxxXE4c"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_fun = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    # Initialise losses\n",
    "    train_losses = []\n",
    "    test_losses  = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Switch training mode on\n",
    "        ANN.train()\n",
    "\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track losses (no accuracy for continous vars)\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average losses from batch\n",
    "        train_losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        ANN.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_loss = []\n",
    "\n",
    "            for X,y in test_loader:\n",
    "\n",
    "                yHat_test = ANN(X)\n",
    "                test_loss = loss_fun(yHat_test,y)\n",
    "                batch_loss.append(test_loss.item())\n",
    "\n",
    "            test_losses.append(np.mean(batch_loss))\n",
    "\n",
    "    # Final predictions on all training and test data\n",
    "    ANN.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Train data\n",
    "        train_preds   = []\n",
    "        train_targets = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            train_preds.append(ANN(X))\n",
    "            train_targets.append(y)\n",
    "\n",
    "        train_preds   = torch.cat(train_preds,dim=0)\n",
    "        train_targets = torch.cat(train_targets,dim=0)\n",
    "\n",
    "        # Test data\n",
    "        test_preds = []\n",
    "        test_targets = []\n",
    "\n",
    "        for X,y in test_loader:\n",
    "\n",
    "            test_preds.append(ANN(X))\n",
    "            test_targets.append(y)\n",
    "\n",
    "        test_preds   = torch.cat(test_preds,dim=0)\n",
    "        test_targets = torch.cat(test_targets,dim=0)\n",
    "\n",
    "    # Function output\n",
    "    return train_losses,test_losses, train_preds,train_targets, test_preds,test_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 490153,
     "status": "ok",
     "timestamp": 1748723179788,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "QxQXr3HVgP6y",
    "outputId": "898d30f8-2362-406a-e70b-1662dfa47810"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over mini-batches size (track time as well)\n",
    "\n",
    "# Takes about 8 mins\n",
    "batch_size_exp = np.arange(3,8)\n",
    "\n",
    "num_exps         = len(batch_size_exp)\n",
    "test_losses_all  = np.zeros((num_epochs,num_exps))\n",
    "train_losses_all = np.zeros((num_epochs,num_exps))\n",
    "\n",
    "train_preds_all   = []\n",
    "train_targets_all = []\n",
    "test_preds_all    = []\n",
    "test_targets_all  = []\n",
    "\n",
    "elapsed_time = np.zeros((len(batch_size_exp),1))\n",
    "\n",
    "for i,exp_i in enumerate(batch_size_exp):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_size   = int(2**exp_i)\n",
    "        train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "        ANN = model_class()\n",
    "        train_losses,test_losses, train_preds,train_targets, test_preds,test_targets = train_model()\n",
    "\n",
    "        train_losses_all[:,i] = train_losses\n",
    "        test_losses_all[:,i]  = test_losses\n",
    "\n",
    "        train_preds_all.append(train_preds.numpy())\n",
    "        train_targets_all.append(train_targets.numpy())\n",
    "\n",
    "        test_preds_all.append(test_preds.numpy())\n",
    "        test_targets_all.append(test_targets.numpy())\n",
    "\n",
    "        elapsed_time[i] = time.time() - start_time\n",
    "        print(f\"Batch size 2^{exp_i} = {batch_size} completed in {elapsed_time[i, 0]:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1748726718430,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "WUFOZMyL-fUF",
    "outputId": "a9f6428a-f6ff-4aa7-f366-dadb8911ecea"
   },
   "outputs": [],
   "source": [
    "test_preds_all = np.array(test_preds_all)\n",
    "test_preds_all.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6108,
     "status": "ok",
     "timestamp": 1748727379232,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "fg9b-fjehQ6U",
    "outputId": "5a7694b8-31f9-4bc4-fd07-f3b2a39bc2c5"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "\n",
    "for i, exp_i in enumerate(batch_size_exp):\n",
    "\n",
    "    batch_size = 2**exp_i\n",
    "\n",
    "    fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "    # Training and test losses\n",
    "    ax[0].plot(train_losses_all[:,i],label='Train loss')\n",
    "    ax[0].plot(test_losses_all[:,i],label='Test loss')\n",
    "    ax[0].set_title('Loss curves')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('MSE loss')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    # True vs predicted values (and get correlation coefficients)\n",
    "    train_preds   = train_preds_all[i].squeeze()\n",
    "    train_targets = train_targets_all[i].squeeze()\n",
    "    r_train       = np.corrcoef(train_preds,train_targets)[0,1]\n",
    "\n",
    "    test_preds   = test_preds_all[i].squeeze()\n",
    "    test_targets = test_targets_all[i].squeeze()\n",
    "    r_test       = np.corrcoef(test_preds, test_targets)[0,1]\n",
    "\n",
    "    ax[1].plot(train_targets_all[i],train_preds_all[i],'o',label=f'Train (r={r_train:.3f})',alpha=0.75)\n",
    "    ax[1].plot(test_targets_all[i],test_preds_all[i],'x',label=f'Train (r={r_test:.3f})',alpha=0.75)\n",
    "    ax[1].plot([min(train_targets_all[i]),max(train_targets_all[i])],\n",
    "               [min(train_targets_all[i]),max(train_targets_all[i])],\n",
    "               'k--', label='Identity')\n",
    "    ax[1].set_title('True vs. predicted values')\n",
    "    ax[1].set_xlabel('True values')\n",
    "    ax[1].set_ylabel('Predicted values')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    fig.suptitle(f'Batch Size: {batch_size}',fontsize=14)\n",
    "    fig.tight_layout(rect=[0,0.03,1,0.95])\n",
    "\n",
    "    filename = f'figure{49+i}_code_challenge_11.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    files.download(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1748728297943,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "DIt73Fj_ThiH",
    "outputId": "b741bed6-bdc6-4bee-9423-7d4c98790d40"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "# Notice how the correlation patterns are not obvious, menaing that that there are\n",
    "# quite some subtle relationship between wine quality and the listed features (beside\n",
    "# the quite strong correlation with alcohol eheheh)\n",
    "\n",
    "# Visualise covariance matrix of data features\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "plt.imshow(np.corrcoef(data.T),vmin=-1,vmax=1,cmap='jet')\n",
    "plt.xticks(range(len(data.keys())),labels=data.keys(),rotation=90)\n",
    "plt.yticks(range(len(data.keys())),labels=data.keys())\n",
    "plt.colorbar()\n",
    "plt.title('Data correlation matrix')\n",
    "\n",
    "plt.savefig('figure54_code_challenge_11.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure54_code_challenge_11.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOpnIQ9bRxWu"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Why did the output node not have a sigmoid activation function? It is possible to train this model using a sigmoid\n",
    "#    function on the output?\n",
    "\n",
    "# I would say that we don't use a sigmoid because we are dealing with a continous\n",
    "# predictor, in other words, it's not a classification task (which a sigmoid is\n",
    "# useful for)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfM6mAXVR5Dp"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Let's say you don't need to know the *exact* sugar amount, only the approximate amount. You could then label each\n",
    "#    wine as being in one of three bins, according to the amount of sugar. What would you need to change in the model?\n",
    "\n",
    "# In this case one might categorise the sugar variable, and then build a classification\n",
    "# model similar to the one built for quality; see code in cells below.\n",
    "# main changes I've implemented:\n",
    "# > Data preprocessing and categorisation, I opted for a 3 quintile split of the sugar feature, after normalisation (i.e. 1/3, 1/3, and 1/3);\n",
    "# > Model class now requires 3 output nodes;\n",
    "# > Loss function needs to be nn.CrossEntropyLoss() for multioutput categorisation;\n",
    "# > Adapt accuracy computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "executionInfo": {
     "elapsed": 1810,
     "status": "ok",
     "timestamp": 1748730540720,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "xMvNNGgdYO13",
    "outputId": "9774215d-1ede-421d-f1d5-80bb40d7f1bc"
   },
   "outputs": [],
   "source": [
    "# %% Load and prepare data\n",
    "\n",
    "# Load\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url,sep=';')\n",
    "\n",
    "# Remove some outliers (see lec. 82 for why)\n",
    "data = data[data['total sulfur dioxide']<200]\n",
    "\n",
    "# Z-score all the variables\n",
    "cols2zscore = data.keys()\n",
    "\n",
    "for col in cols2zscore:\n",
    "    mean_val  = np.mean(data[col])\n",
    "    std_val   = np.std(data[col])\n",
    "    data[col] = (data[col] - mean_val) / std_val\n",
    "\n",
    "# Categorise residual sugar (IQRs)\n",
    "q33 = data['residual sugar'].quantile(0.33)\n",
    "q66 = data['residual sugar'].quantile(0.66)\n",
    "\n",
    "data.loc[:,'categorical_residual_sugar'] = 0                              # low sugar\n",
    "data.loc[data['residual sugar'] > q33, 'categorical_residual_sugar'] = 1  # medium sugar\n",
    "data.loc[data['residual sugar'] > q66, 'categorical_residual_sugar'] = 2  # high sugar\n",
    "\n",
    "cols2zscore = data.keys()\n",
    "\n",
    "# Convert from pandas dataframe to PyTorch tensor\n",
    "cols2zscore = cols2zscore.drop('categorical_residual_sugar')\n",
    "cols2zscore = cols2zscore.drop('residual sugar')\n",
    "data_t      = torch.tensor( data[cols2zscore].values ).float()\n",
    "labels      = torch.tensor( data['categorical_residual_sugar'].values ).long()\n",
    "\n",
    "print(f'Data shape: {data_t.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "# Labels need to be multidimentional for PyTorch, not an array, and need to be long integers too\n",
    "#labels = labels[:,None]\n",
    "print(f'Proper labels shape: {labels.shape}')\n",
    "\n",
    "# Reminder of what the data are actually like\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXCS5KlQYO15"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_t,labels,test_size=0.1)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "# > the train_loader is moved inside the train_model() function to allow a parametric test of the batch size\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wPq-gFZYO15"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "class model_class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear(11,32)\n",
    "        self.hid1   = nn.Linear(32,32)\n",
    "        self.hid2   = nn.Linear(32,32)\n",
    "        self.output = nn.Linear(32,3)\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hid1(x))\n",
    "        x = F.relu(self.hid2(x))\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjbuFYEhYO16"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    # Initialise losses\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Switch training mode on\n",
    "        ANN.train()\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            predicted_class = torch.argmax(yHat,dim=1)\n",
    "            batch_acc.append( 100*torch.mean((predicted_class==y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "        predicted_class = torch.argmax(yHat,dim=1)\n",
    "        test_acc.append( 100*torch.mean((predicted_class==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 490959,
     "status": "ok",
     "timestamp": 1748731867319,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "L-DVr_6XYO16",
    "outputId": "be6debe8-7459-47cb-874b-2f09def38e27"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over mini-batches size (track time as well)\n",
    "\n",
    "# Takes about 8 mins\n",
    "batch_size_exp = np.arange(3,8)\n",
    "\n",
    "train_acc    = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "test_acc     = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "losses       = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "elapsed_time = np.zeros((len(batch_size_exp),1))\n",
    "\n",
    "for i,exp_i in enumerate(batch_size_exp):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_size   = int(2**exp_i)\n",
    "        train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "        ANN = model_class()\n",
    "        train_acc[:,i],test_acc[:,i],losses[:,i] = train_model()\n",
    "\n",
    "        elapsed_time[i] = time.time() - start_time\n",
    "        print(f\"Batch size 2^{exp_i} = {batch_size} completed in {elapsed_time[i, 0]:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCrsVATpeR7X"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 929,
     "status": "ok",
     "timestamp": 1748732246800,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "JeCACIv1YO17",
    "outputId": "d6aa886f-eaca-4adf-9cf2-547db3909a46"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(batch_size_exp)))\n",
    "for i in range(len(batch_size_exp)):\n",
    "    ax[0].plot(smooth_adaptive(train_acc[:,i],40),color=cmaps[i])\n",
    "    ax[1].plot(smooth_adaptive(test_acc[:,i],40),color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# Make the legend easier to read\n",
    "leglabels = [2**int(i) for i in batch_size_exp]\n",
    "\n",
    "# Common features\n",
    "for i in range(2):\n",
    "    ax[i].legend(leglabels)\n",
    "    ax[i].set_xlabel('Epoch')\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].set_ylim([41,101])\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.savefig('figure55_code_challenge_11_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure55_code_challenge_11_extra2.png')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNjUbVbFZq98UVMn+dWAe7r",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
