{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUEtJeVwpRzY"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 12.119\n",
    "#    Code challenge 19: unbalanced data\n",
    "\n",
    "#    1) Start from code from video 10.092 (wine dataset)\n",
    "#    2) Use a leaky ReLU, Adam with initial lr = 0.001, and 500 epochs\n",
    "#    3) Create a function exporting train/test dataloaders with a specified\n",
    "#       quality threshold for binarising bad and good wine\n",
    "#    4) Train the model using thresholds of 4.5/8, 5.5/8, and 6.5/8\n",
    "#    5) Plot a 3x3 subplot matrix with losses, avg accuracy, and per category\n",
    "#       accuracy on each line\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btGdImNLp9qL"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiba_nWdyCrV"
   },
   "outputs": [],
   "source": [
    "# %% Function to get data and flexibly split them into two categories\n",
    "\n",
    "def gen_data(data,split_threshold):\n",
    "\n",
    "    # Remove some outliers (see lec. 82 for why)\n",
    "    data = data[data['total sulfur dioxide']<200].copy()\n",
    "\n",
    "    # Z-score all the variables but quality\n",
    "    cols2zscore = data.keys()\n",
    "    cols2zscore = cols2zscore.drop('quality')\n",
    "\n",
    "    for col in cols2zscore:\n",
    "        mean_val      = np.mean(data[col])\n",
    "        std_val       = np.std(data[col])\n",
    "        data.loc[:,col] = (data[col] - mean_val) / std_val\n",
    "\n",
    "    # Binarise quality according to input threshold\n",
    "    data.loc[:,'boolean_quality'] = 0\n",
    "    data.loc[data['quality']>split_threshold, 'boolean_quality'] = 1\n",
    "    data.loc[data['quality']<split_threshold, 'boolean_quality'] = 0 # Implicit but here for clarity\n",
    "\n",
    "    # Convert from pandas dataframe to PyTorch tensor\n",
    "    data_t = torch.tensor( data[cols2zscore].values ).float()\n",
    "    labels = torch.tensor( data['boolean_quality'].values ).float()\n",
    "\n",
    "    # Labels need to be multidimentional for PyTorch, not an array, and need to be long integers too\n",
    "    labels = labels[:,None]\n",
    "\n",
    "    # Split with scikitlearn\n",
    "    train_data,test_data,train_labels,test_labels = train_test_split(data_t,labels,test_size=0.1)\n",
    "\n",
    "    # Convert into PyTorch datasets\n",
    "    train_data = TensorDataset(train_data,train_labels)\n",
    "    test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "    # Convert into DataLoader objects\n",
    "    batch_size   = 32\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "    test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "    return train_loader,test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "an7IwYWBp-p1"
   },
   "outputs": [],
   "source": [
    "# %% Test data function\n",
    "\n",
    "# Load\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url,sep=';')\n",
    "\n",
    "# Call function (input data and threshold as a value between 1 and 8)\n",
    "train_loader,test_loader = gen_data(data,6.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASRoH0Dup-lr"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "# Optional arg to switch activation function\n",
    "class ANN_wine(nn.Module):\n",
    "    def __init__(self,act_fun):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear(11,32)\n",
    "        self.hid1   = nn.Linear(32,32)\n",
    "        self.hid2   = nn.Linear(32,32)\n",
    "        self.output = nn.Linear(32,1)\n",
    "\n",
    "        # Activation function\n",
    "        self.act_fun = act_fun\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self,x):\n",
    "\n",
    "        act_fun = getattr(torch.nn,self.act_fun)()\n",
    "        x = act_fun(self.input(x))\n",
    "        x = act_fun(self.hid1(x))\n",
    "        x = act_fun(self.hid2(x))\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rXGn_kvp-ef"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Loss function and optimizer (vary lr to highlight differences in activation functions)\n",
    "    loss_fun  = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.001)\n",
    "\n",
    "    # Initialise losses\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Switch training mode on\n",
    "        ANN.train()\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss (with batchnorm arg)\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357169,
     "status": "ok",
     "timestamp": 1753628078442,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "myT0GmnY0QuU",
    "outputId": "69629e24-f2c3-417e-c9e3-8d0428a8ec70"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment on data split thresholds\n",
    "\n",
    "# Data and parameters\n",
    "url  = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url,sep=';')\n",
    "\n",
    "thresholds = [4.5,5.5,6.5]\n",
    "\n",
    "# Preallocate vars\n",
    "train_acc  = np.zeros((len(thresholds),num_epochs))\n",
    "test_acc   = np.zeros((len(thresholds),num_epochs))\n",
    "losses     = np.zeros((len(thresholds),num_epochs))\n",
    "\n",
    "test_acc_0 = np.zeros(len(thresholds))\n",
    "test_acc_1 = np.zeros(len(thresholds))\n",
    "n_samples  = np.zeros((len(thresholds),2))\n",
    "\n",
    "# Run the experiment! (it takes ~5 mins)\n",
    "for i,threshold in enumerate(thresholds):\n",
    "\n",
    "    # Get data\n",
    "    train_loader,test_loader = gen_data(data,threshold)\n",
    "\n",
    "    # Run model\n",
    "    ANN = ANN_wine('LeakyReLU')\n",
    "    train_acc[i,:],test_acc[i,:],losses[i,:],ANN = train_model()\n",
    "\n",
    "    # Get accuracies by category\n",
    "    ANN.eval()\n",
    "    X_test, y_test = next(iter(test_loader))\n",
    "    with torch.no_grad():\n",
    "        yHat  = ANN(X_test)\n",
    "        preds = (yHat > 0).float().flatten()\n",
    "\n",
    "    # Flatten labels\n",
    "    y_true = y_test.flatten()\n",
    "\n",
    "    # Loop through both classes for accuracies (0 and 1)\n",
    "    for label in [0,1]:\n",
    "        idx = y_true == label\n",
    "        n   = idx.sum().item()\n",
    "        n_samples[i,label] = n\n",
    "        if n > 0:\n",
    "            acc = 100 * torch.mean((preds[idx] == y_true[idx]).float()).item()\n",
    "            if label == 0:\n",
    "                test_acc_0[i] = acc\n",
    "            else:\n",
    "                test_acc_1[i] = acc\n",
    "            print(f'Accuracy for class {label} and threshold {threshold}: {acc:.2f}% ({n} samples)')\n",
    "        else:\n",
    "            print(f'No samples for class {label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEmF9EHY1sEx"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "executionInfo": {
     "elapsed": 1895,
     "status": "ok",
     "timestamp": 1753628103766,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "LmdpnJBDOcPY",
    "outputId": "ba56e4eb-7d0c-4e75-bb0b-f526f8f05a33"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(3,3,figsize=(7*phi,7))\n",
    "\n",
    "for i,threshold in enumerate(thresholds):\n",
    "\n",
    "    # Column 0 (losses)\n",
    "    ax[i,0].plot(smooth_adaptive(losses[i,:],25),color='firebrick')\n",
    "    ax[i,0].set_title(f'Loss (data threshold={threshold})')\n",
    "    ax[i,0].set_xlabel('Epoch')\n",
    "    ax[i,0].set_ylabel('Loss')\n",
    "\n",
    "    # Column 1 (accuracies)\n",
    "    ax[i,1].plot(smooth_adaptive(train_acc[i,:],25),label='Train',color='steelblue')\n",
    "    ax[i,1].plot(smooth_adaptive(test_acc[i,:],25),label='Test',color='darkorange')\n",
    "    ax[i,1].set_title(f'Accuracy (data threshold={threshold})')\n",
    "    ax[i,1].set_xlabel('Epoch')\n",
    "    ax[i,1].set_ylabel('Accuracy (%)')\n",
    "    ax[i,1].set_ylim([45,105])\n",
    "    ax[i,1].legend()\n",
    "\n",
    "    # Column 2 (accuracy by class)\n",
    "    accs   = [test_acc_0[i],test_acc_1[i]]\n",
    "    labels = ['Low qual. wines','High qual. wines']\n",
    "    counts = n_samples[i,:]\n",
    "\n",
    "    bars = ax[i,2].bar(labels,accs)\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax[i,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                      f'n={int(count)}',ha='center',va='bottom')\n",
    "\n",
    "    ax[i,2].set_ylim([0,115])\n",
    "    ax[i,2].set_ylabel('Accuracy (%)')\n",
    "    ax[i,2].set_title(f'Class-wise acc. (data threshold={threshold})')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure8_code_challenge_19.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure8_code_challenge_19.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ky_yNLNOcMb"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    L2 regularization is supposed to help minimize over-training. Try adding an L2 regularizer and see if that\n",
    "#    helps reduce the bias due to unbalanced N.\n",
    "\n",
    "# Easily done by adding weight_decay=0.01 to the optimizer line. It doesn't seem\n",
    "# to change a lot, if anything the first example, with only N=5 samples in one\n",
    "# of the categories, shows nicely how the overall accuray on test data (~97%)\n",
    "# can be quite misleading! A bit like the cats and boats example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n0UZGDAB5sI"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Does the unbalanced design get better (that is, less unbalanced) if the train/test split is 80/20 instead of 90/10?\n",
    "#    Try it and find out! Note that you don't need to train models for this question; you simply need to modify the data\n",
    "#    splitting function and then compute the proportions of the two categories.\n",
    "\n",
    "# Trying this without L2 regularisation for comparability with the results from\n",
    "# original experiment. And no chaging to 20% test data doesn't seem to improve\n",
    "# or change anything that much; most likely the unbalance is still too strong\n",
    "# for the lower and upper splits.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNtBb35pIYAnyhqUa5gYiW2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
