{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajZQZPTAL35r"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 12.121\n",
    "#    Data oversampling in MNIST\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH8Ro002LsPZ"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvGQyapPQ9Tn"
   },
   "outputs": [],
   "source": [
    "# %% Function to get the data\n",
    "\n",
    "# Load data\n",
    "data_all = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Function\n",
    "def get_dataset(n,double_data=False):\n",
    "\n",
    "    # Remove labels (i.e., numbers IDs) from dataset, and select only n data\n",
    "    labels = data_all[:n,0]\n",
    "    data   = data_all[:n,1:]\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    data_norm = data / np.max(data)\n",
    "\n",
    "    # Make an exact copy of all the data (note: if done here it will generate\n",
    "    # problems with the devset, because there will be copies of items in both\n",
    "    # train and dev sets)\n",
    "    #if double_data==True:\n",
    "    #    data_norm = np.concatenate((data_norm,data_norm),axis=0)\n",
    "    #    labels    = np.concatenate((labels,labels),axis=0)\n",
    "\n",
    "    # Covert to tensor\n",
    "    data_T   = torch.tensor(data_norm).float()\n",
    "    labels_T = torch.tensor(labels).long()\n",
    "\n",
    "    # Split data with scikitlearn\n",
    "    train_data,test_data, train_labels,test_labels = train_test_split(data_T,labels_T,test_size=0.1)\n",
    "\n",
    "    # Make an exact copy of the train data after splitting, to avoid\n",
    "    # above-mentioned issue\n",
    "    if double_data==True:\n",
    "        train_data   = torch.cat((train_data,train_data),axis=0)\n",
    "        train_labels = torch.cat((train_labels,train_labels),axis=0)\n",
    "\n",
    "    # PyTorch datasets\n",
    "    train_data = TensorDataset(train_data,train_labels)\n",
    "    test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "    # DataLoader objects\n",
    "    batch_size   = 20\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "    return train_loader,test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1753649072708,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "S9WYpp8rVq--",
    "outputId": "1bed4f9b-6f33-46f5-cfd5-964d08d67b17"
   },
   "outputs": [],
   "source": [
    "# %% Test the data function\n",
    "\n",
    "# Do double data\n",
    "r,t = get_dataset(200,False)\n",
    "print(r.dataset.tensors[0].shape)\n",
    "print(t.dataset.tensors[0].shape)\n",
    "\n",
    "# Do not double data\n",
    "r,t = get_dataset(200,True)\n",
    "print(r.dataset.tensors[0].shape)\n",
    "print(t.dataset.tensors[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lChDHV4DWomm"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.hid1   = nn.Linear(64,32)\n",
    "            self.hid2   = nn.Linear(32,32)\n",
    "            self.output = nn.Linear(32,10)\n",
    "\n",
    "        def forward(self,x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid1(x))\n",
    "            x = F.relu(self.hid2(x))\n",
    "\n",
    "            return self.output(x)\n",
    "\n",
    "    # Model instance, loss function, and optimizer\n",
    "    ANN       = model()\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hVHxMwyXo3A"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Epochs (few to keep some varaibility in performace) and fresh model instance\n",
    "    num_epochs = 50\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    # Preallocate vars\n",
    "    losses    = torch.zeros(num_epochs)\n",
    "    train_acc = torch.zeros(num_epochs)\n",
    "    test_acc  = torch.zeros(num_epochs)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training data batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward pass, backpropagation, and optimizer step\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        losses[epoch_i]    = np.mean(batch_loss).item()\n",
    "        train_acc[epoch_i] = np.mean(batch_acc).item()\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "            test_acc[epoch_i] = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 18818,
     "status": "ok",
     "timestamp": 1753649106237,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "XX81RDzwXpon",
    "outputId": "e147b626-ca5b-4a7e-df11-9d62d6d51a43"
   },
   "outputs": [],
   "source": [
    "# %% Test the whole setting\n",
    "\n",
    "train_loader,test_loader      = get_dataset(5000,False)\n",
    "train_acc,test_acc,losses,ANN = train_model()\n",
    "\n",
    "# plot the results\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(train_acc,label='Train')\n",
    "ax[1].plot(test_acc,label='Test')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([20,102])\n",
    "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure11_data_oversampling_mnist.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure11_data_oversampling_mnist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6fDrUciXpkY"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment on amount of data\n",
    "\n",
    "# Parameters and preallocated vars\n",
    "sample_sizes   = np.arange(500,4001,500)\n",
    "results_single = torch.zeros((len(sample_sizes),3))\n",
    "results_double = torch.zeros((len(sample_sizes),3))\n",
    "\n",
    "# Run the experiment (takes ~3 mins)\n",
    "for i,sample_size in enumerate(sample_sizes):\n",
    "\n",
    "    # Non-doubled data\n",
    "    train_loader,test_loader      = get_dataset(sample_size,False)\n",
    "    train_acc,test_acc,losses,ANN = train_model()\n",
    "\n",
    "    # Get results\n",
    "    results_single[i,0] = torch.mean(train_acc[-5:]).item()\n",
    "    results_single[i,1] = torch.mean(test_acc[-5:]).item()\n",
    "    results_single[i,2] = torch.mean(losses[-5:]).item()\n",
    "\n",
    "    # Doubled data\n",
    "    train_loader,test_loader      = get_dataset(sample_size,True)\n",
    "    train_acc,test_acc,losses,ANN = train_model()\n",
    "\n",
    "    # Get results\n",
    "    results_double[i,0] = torch.mean(train_acc[-5:]).item()\n",
    "    results_double[i,1] = torch.mean(test_acc[-5:]).item()\n",
    "    results_double[i,2] = torch.mean(losses[-5:]).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "executionInfo": {
     "elapsed": 1179,
     "status": "ok",
     "timestamp": 1753653380405,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "iDy5-IYHXphT",
    "outputId": "46b5b4ea-92e8-4c64-f999-650a87950833"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(1.5*6*phi,6))\n",
    "\n",
    "# Axes and title labels\n",
    "titles    = ['Train accuracy','Dev. set accuracy','Losses']\n",
    "yaxlabels = ['Accuracy','Accuracy','Losses']\n",
    "\n",
    "# Common features\n",
    "for i in range(3):\n",
    "\n",
    "    # Plot the lines\n",
    "    ax[i].plot(sample_sizes,results_single[:,i],'s-',label='Original')\n",
    "    ax[i].plot(sample_sizes,results_double[:,i],'s-',label='Doubled')\n",
    "\n",
    "    # Make it nicer\n",
    "    ax[i].set_ylabel(yaxlabels[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('Unique sample size')\n",
    "    ax[i].grid('on')\n",
    "\n",
    "    if i<2:\n",
    "        ax[i].set_ylim([20,102])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure12_data_oversampling_mnist.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure12_data_oversampling_mnist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duAYtqykjuIt"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Notice that we're using the \"test_dataset\" multiple times, which really means that it's the devset,\n",
    "#    aka hold-out set, and not a true TEST set. A real test set gets evaluated only once. Modify the code\n",
    "#    to create a test set, using images in dataFull that are not in dataNorm. Note that you don't need\n",
    "#    to re-run the entire experiment; you only need to train two models (and save their 'net' outputs), so that\n",
    "#    you can run the test data through (make sure to normalize the test data!). Then you can evaluate the test\n",
    "#    performance relative to train and devset from those two models.\n",
    "\n",
    "# A bit over-engineered here, but recoded a bit everything to avoid going mad\n",
    "# over the variable names. Now, I guess here the test set doesn't really give\n",
    "# insanely different results, because we didn't really twisted a lot of\n",
    "# parameters to optimise the training with the dev set (e.g., architecture,\n",
    "# regularisations, and any other metaparameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1753651459192,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "ATVI2LhjtI0X",
    "outputId": "0ba2065a-d3d8-4517-d0d4-1178ea48769c"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1 - continue ...\n",
    "#    Function to get the data (train, dev, test)\n",
    "\n",
    "# Load data\n",
    "data_all = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Function\n",
    "def get_dataset(n,double_data=False):\n",
    "\n",
    "    # Remove labels (i.e., numbers IDs) from dataset, and select only n data\n",
    "    labels = data_all[:n,0]\n",
    "    data   = data_all[:n,1:]\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    data_norm = data / np.max(data)\n",
    "\n",
    "    # Covert to tensor\n",
    "    data_T   = torch.tensor(data_norm).float()\n",
    "    labels_T = torch.tensor(labels).long()\n",
    "\n",
    "    # Split data with scikitlearn\n",
    "    train_data,dev_data, train_labels,dev_labels = train_test_split(data_T,labels_T,test_size=0.1)\n",
    "\n",
    "    # Make an exact copy of the train data after splitting, to avoid\n",
    "    # above-mentioned issue\n",
    "    if double_data==True:\n",
    "        train_data   = torch.cat((train_data,train_data),axis=0)\n",
    "        train_labels = torch.cat((train_labels,train_labels),axis=0)\n",
    "\n",
    "    # PyTorch datasets\n",
    "    train_data = TensorDataset(train_data,train_labels)\n",
    "    dev_data  = TensorDataset(dev_data,dev_labels)\n",
    "\n",
    "    # DataLoader objects\n",
    "    batch_size   = 20\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    dev_loader   = DataLoader(dev_data,batch_size=dev_data.tensors[0].shape[0])\n",
    "\n",
    "    # Get actual test set from unused rows\n",
    "    n_test         = dev_data.tensors[0].shape[0]\n",
    "    remaining_data = data_all[n:]\n",
    "    if remaining_data.shape[0] < n_test:\n",
    "        raise ValueError(\"Not enough data for an actual test set of this size.\")\n",
    "\n",
    "    test_labels = remaining_data[:n_test,0]\n",
    "    test_data   = remaining_data[:n_test,1:] / np.max(data_all)\n",
    "\n",
    "    test_data  = torch.tensor(test_data).float()\n",
    "    test_labels = torch.tensor(test_labels).long()\n",
    "\n",
    "    test_data  = TensorDataset(test_data,test_labels)\n",
    "    test_loader = DataLoader(test_data,batch_size=n_test)\n",
    "\n",
    "    return train_loader,dev_loader,test_loader\n",
    "\n",
    "# Test the data function\n",
    "\n",
    "# Do double data\n",
    "r,d,t = get_dataset(200,False)\n",
    "print(r.dataset.tensors[0].shape)\n",
    "print(d.dataset.tensors[0].shape)\n",
    "print(t.dataset.tensors[0].shape)\n",
    "\n",
    "# Do not double data\n",
    "r,d,t = get_dataset(200,True)\n",
    "print(r.dataset.tensors[0].shape)\n",
    "print(d.dataset.tensors[0].shape)\n",
    "print(t.dataset.tensors[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cbry5Xjktt5N"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1 - continue ...\n",
    "#    Model class\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    class model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.hid1   = nn.Linear(64,32)\n",
    "            self.hid2   = nn.Linear(32,32)\n",
    "            self.output = nn.Linear(32,10)\n",
    "\n",
    "        def forward(self,x):\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.hid1(x))\n",
    "            x = F.relu(self.hid2(x))\n",
    "\n",
    "            return self.output(x)\n",
    "\n",
    "    # Model instance, loss function, and optimizer\n",
    "    ANN       = model()\n",
    "    loss_fun  = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JUe8e4Iu5Hd"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1 - continue ...\n",
    "#    Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Epochs (few to keep some varaibility in performace) and fresh model instance\n",
    "    num_epochs = 50\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    # Preallocate vars\n",
    "    losses    = torch.zeros(num_epochs)\n",
    "    train_acc = torch.zeros(num_epochs)\n",
    "    dev_acc   = torch.zeros(num_epochs)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training data batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward pass, backpropagation, and optimizer step\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "            batch_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        losses[epoch_i]    = np.mean(batch_loss).item()\n",
    "        train_acc[epoch_i] = np.mean(batch_acc).item()\n",
    "\n",
    "        # Dev set accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(dev_loader))\n",
    "            yHat = ANN(X)\n",
    "            dev_acc[epoch_i] = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,dev_acc,losses,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1753651922308,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "4gJ7B-DNtt0Q",
    "outputId": "3e9ef28b-d0a6-43a2-c8f5-a33eba374b06"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1 - continue ...\n",
    "#    Parametric experiment on amount of data\n",
    "\n",
    "# Parameters and preallocated vars\n",
    "sample_sizes   = np.arange(500,4001,500)\n",
    "results_single = torch.zeros((len(sample_sizes),4))\n",
    "results_double = torch.zeros((len(sample_sizes),4))\n",
    "\n",
    "# Run the experiment (takes ~3 mins)\n",
    "for i,sample_size in enumerate(sample_sizes):\n",
    "\n",
    "    # Non-doubled data\n",
    "    train_loader,dev_loader,test_loader = get_dataset(sample_size,False)\n",
    "    train_acc,dev_acc,losses,ANN        = train_model()\n",
    "\n",
    "    # Evaluate on real test set\n",
    "    ANN.eval()\n",
    "    X_test,y_test = next(iter(test_loader))\n",
    "    with torch.no_grad():\n",
    "        yHat_test = ANN(X_test)\n",
    "        test_acc = 100*torch.mean((yHat_test.argmax(1)==y_test).float())\n",
    "\n",
    "    # Get results\n",
    "    results_single[i,0] = torch.mean(train_acc[-5:]).item()\n",
    "    results_single[i,1] = torch.mean(dev_acc[-5:]).item()\n",
    "    results_single[i,2] = torch.mean(losses[-5:]).item()\n",
    "    results_single[i,3] = (test_acc).item()\n",
    "\n",
    "\n",
    "    # Doubled data\n",
    "    train_loader,dev_loader,test_loader = get_dataset(sample_size,True)\n",
    "    train_acc,dev_acc,losses,ANN        = train_model()\n",
    "\n",
    "    # Evaluate on real test set\n",
    "    ANN.eval()\n",
    "    X_test,y_test = next(iter(test_loader))\n",
    "    with torch.no_grad():\n",
    "        yHat_test = ANN(X_test)\n",
    "        test_acc = 100*torch.mean((yHat_test.argmax(1)==y_test).float())\n",
    "\n",
    "    # Get results\n",
    "    results_double[i,0] = torch.mean(train_acc[-5:]).item()\n",
    "    results_double[i,1] = torch.mean(dev_acc[-5:]).item()\n",
    "    results_double[i,2] = torch.mean(losses[-5:]).item()\n",
    "    results_double[i,3] = (test_acc).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "executionInfo": {
     "elapsed": 1692,
     "status": "ok",
     "timestamp": 1753652911783,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "Gh0m7Fk8ttty",
    "outputId": "63aa70c7-78a0-4e7d-bcbb-3858fd848aaf"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1 - continue ...\n",
    "#    Plotting\n",
    "\n",
    "phi = (1 + np.sqrt(5)) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(1.5*6*phi,6))\n",
    "\n",
    "# Axes and title labels\n",
    "titles    = ['Train accuracy','Dev. and test set accuracy','Losses']\n",
    "yaxlabels = ['Accuracy','Accuracy','Losses']\n",
    "\n",
    "# Common features\n",
    "for i in range(3):\n",
    "\n",
    "    # Plot the train and dev set info\n",
    "    ax[i].plot(sample_sizes,results_single[:,i],'s-',label='Original')\n",
    "    ax[i].plot(sample_sizes,results_double[:,i],'s-',label='Doubled')\n",
    "\n",
    "    # Plot test accuracy only in the middle plot\n",
    "    if i == 1:\n",
    "        ax[i].plot(sample_sizes,results_single[:,3],'o:',color='tab:blue',label='Original (test)')\n",
    "        ax[i].plot(sample_sizes,results_double[:,3],'o:',color='tab:orange',label='Doubled (test)')\n",
    "\n",
    "    # Make it nicer\n",
    "    ax[i].set_ylabel(yaxlabels[i])\n",
    "    ax[i].set_title(titles[i])\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('Unique sample size')\n",
    "    ax[i].grid('on')\n",
    "\n",
    "    if i<2:\n",
    "        ax[i].set_ylim([20,102])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figure15_data_oversampling_mnist_extra1.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure15_data_oversampling_mnist_extra1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7TF_aMojt2_"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    We've previously discovered that Adam can outperform SGD on the MNIST dataset. I used SGD here on purpose\n",
    "#    to make performance worse (!) so we could test for effects of oversampling. Re-run the experiment using\n",
    "#    Adam to see whether you still get the same effects.\n",
    "\n",
    "# With Adam the model reaches a ceiling performance for basically any\n",
    "# configuration, a bit better for the doubled data on the dev set accuracy,\n",
    "# but it's probably due to the fact that we doubled the data before splitting\n",
    "# into train and dev sets, so some data in the train set are also in the dev\n",
    "# set, which is a problem. Indeed, if one double only the train data after the\n",
    "# split, the picture becomes more coherent and less suspicious\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjJmR2znr/rJJfNSJ78Q1+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
