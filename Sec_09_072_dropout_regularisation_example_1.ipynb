{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le5MsLuthBqv"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 9.72\n",
    "#    Dropout regularisation in practice\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty5MFy8KhV0e"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kh9NzoctsTH-"
   },
   "outputs": [],
   "source": [
    "# %% Some synthetic nonlinear data\n",
    "\n",
    "n_clust = 200\n",
    "\n",
    "th = np.linspace(0,4*np.pi,n_clust)\n",
    "r1 = 10\n",
    "r2 = 15\n",
    "\n",
    "# Data\n",
    "a = [ r1*np.cos(th) + np.random.randn(n_clust)*3,\n",
    "      r1*np.sin(th) + np.random.randn(n_clust)   ]\n",
    "b = [ r2*np.cos(th) + np.random.randn(n_clust),\n",
    "      r2*np.sin(th) + np.random.randn(n_clust)*3 ]\n",
    "\n",
    "data = np.hstack(( a,b )).T\n",
    "\n",
    "# Labels\n",
    "labels_np = np.vstack(( np.zeros((n_clust,1)),np.ones((n_clust,1)) ))\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data   = torch.tensor(data).float()\n",
    "labels = torch.tensor(labels_np).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 639,
     "status": "ok",
     "timestamp": 1746132749400,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "l05XbTVCulMN",
    "outputId": "7bafefdf-9e85-4fe9-d7d4-c4da70310ccd"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.plot(data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'s')\n",
    "plt.plot(data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'o')\n",
    "plt.title(\"Some data\")\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "\n",
    "plt.savefig('figure1_dropout_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure1_dropout_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRd226U2vD02"
   },
   "outputs": [],
   "source": [
    "# %% Split data into DataLoaders\n",
    "\n",
    "# Split data with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data,labels,test_size=0.2)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert to DataLoader objects\n",
    "batch_size   = 16\n",
    "#batch_size  = int(train_data.tensors[0].shape[0]/4) # but hard-coding is often better to avoid huge batches\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mpOlEyAxgbI"
   },
   "outputs": [],
   "source": [
    "# %% Create the model class\n",
    "\n",
    "class model_class(nn.Module):\n",
    "    def __init__(self,dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear(  2,128)\n",
    "        self.hidden = nn.Linear(128,128)\n",
    "        self.output = nn.Linear(128,1  )\n",
    "\n",
    "        # Parameters\n",
    "        self.dr = dropout_rate\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self,x):\n",
    "\n",
    "        # Pass through input layer\n",
    "        x = F.relu(self.input(x))\n",
    "        # Dropout after input layer (training=self.training means to turn the dropout off during evaluation mode)\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Pass through the hidden layer\n",
    "        x = F.relu(self.hidden(x))\n",
    "        # Dropout after hidden layer\n",
    "        x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "        # Pass through output layer (no dropoout here!)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1746132762574,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "nXv7CsPPz2DV",
    "outputId": "333e86cf-0233-4cf8-b21e-27e783d9e9e8"
   },
   "outputs": [],
   "source": [
    "# %% Quick test\n",
    "\n",
    "tmp_net  = model_class(0.25)\n",
    "\n",
    "tmp_data = torch.randn((10,2))\n",
    "yHat     = tmp_net(tmp_data)\n",
    "\n",
    "print(yHat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYbEBbyk0QWA"
   },
   "outputs": [],
   "source": [
    "# %% Function to create the model\n",
    "\n",
    "def gen_model(drop_rate):\n",
    "\n",
    "    # Grab an instance of the model class\n",
    "    ANN = model_class(drop_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.002)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeA3scLW05dC"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "# Note how here the model, the loss function, and the optimizer are inputs\n",
    "def train_model(ANN,loss_fun,optimizer):\n",
    "\n",
    "    # Preallocate accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Loop over epochs and batches\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Switch training mode on\n",
    "        ANN.train()\n",
    "\n",
    "        batch_acc= []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean(((yHat>0)==y).float()).item() )\n",
    "\n",
    "        # Average training accuracy of batches\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "\n",
    "        # Test accuracy (switch training off, extract X,y from dataloader, final pass)\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean(((yHat>0)==y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BquEqHUr4bkv"
   },
   "outputs": [],
   "source": [
    "# %% Function for 1D smoothing filter\n",
    "\n",
    "# Try k=1,5,20\n",
    "def smooth(x,k=20):\n",
    "    return np.convolve(x,np.ones(k)/k,mode='same')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8pKHmWOkMgu"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k=20):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n",
    "\n",
    "# Improved for edge effects - padding ('edge' repeat edge value)\n",
    "def smooth_padding(x,k=20):\n",
    "    pad_width = k // 2\n",
    "    padded    = np.pad(x,pad_width, mode='edge')\n",
    "    kernel    = np.ones(k)/k\n",
    "\n",
    "    return np.convolve(padded,kernel,mode='valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9bv10hu3lMy"
   },
   "outputs": [],
   "source": [
    "# %% Test the model\n",
    "\n",
    "drop_rate = 0\n",
    "\n",
    "ANN,loss_fun,optimizer = gen_model(drop_rate)\n",
    "train_acc,test_acc     = train_model(ANN,loss_fun,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1746133723627,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "ulHtLFuz4zEA",
    "outputId": "fb04b1e4-acd9-4388-9e2c-002e8f6f2c9f"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(smooth(train_acc),'s-')\n",
    "plt.plot(smooth(test_acc),'o-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim([20,100])\n",
    "plt.legend(['Train','Test'])\n",
    "plt.title(f'Dropout rate = {drop_rate}')\n",
    "\n",
    "plt.savefig('figure2_dropout_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure2_dropout_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1746133752354,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "Bfd6FmwEnkD3",
    "outputId": "7da274a8-1885-4332-cc37-6b68bc80aa13"
   },
   "outputs": [],
   "source": [
    "# Plotting with improved smoothing\n",
    "\n",
    "# Adaptive window\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(smooth_adaptive(train_acc),'s-')\n",
    "plt.plot(smooth_adaptive(test_acc),'o-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim([20,100])\n",
    "plt.legend(['Train','Test'])\n",
    "plt.title(f'Dropout rate = {drop_rate}')\n",
    "\n",
    "plt.savefig('figure10_dropout_regularisation_extra1.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure10_dropout_regularisation_extra1.png')\n",
    "\n",
    "# Padding\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(smooth_padding(train_acc),'s-')\n",
    "plt.plot(smooth_padding(test_acc),'o-')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim([20,100])\n",
    "plt.legend(['Train','Test'])\n",
    "plt.title(f'Dropout rate = {drop_rate}')\n",
    "\n",
    "plt.savefig('figure11_dropout_regularisation_extra1.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure11_dropout_regularisation_extra1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SKyzSzD5Kku"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over dropout rates\n",
    "\n",
    "drop_rates = np.arange(10)/10\n",
    "results    = np.zeros((len(drop_rates),2))\n",
    "\n",
    "for drop_i in range(len(drop_rates)):\n",
    "\n",
    "    # Generate and train model\n",
    "    ANN,loss_fun,optimizer = gen_model(drop_rates[drop_i])\n",
    "    train_acc,test_acc     = train_model(ANN,loss_fun,optimizer)\n",
    "\n",
    "    # Store accuracies from last 100 epochs\n",
    "    results[drop_i,0] = np.mean(train_acc[-100:])\n",
    "    results[drop_i,1] = np.mean(test_acc[-100:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1745873888896,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "P0-md2vt65JZ",
    "outputId": "47cda0fb-e38f-4359-ef45-cb7941fd5c3c"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "ax[0].plot(drop_rates,results,'o-')\n",
    "ax[0].set_xlabel('Dropout proportion')\n",
    "ax[0].set_ylabel('Average accuracy')\n",
    "ax[0].legend(['Train','Test'])\n",
    "\n",
    "ax[1].plot(drop_rates,-np.diff(results,axis=1),'o-')\n",
    "ax[1].plot([0,.9],[0,0],'k--')\n",
    "ax[1].set_xlabel('Dropout proportion')\n",
    "ax[1].set_ylabel('Train-test difference (in percent of acc)')\n",
    "\n",
    "plt.savefig('figure3_dropout_regularisation.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure3_dropout_regularisation.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAwqW4Yq71Se"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Explore the effects of the smoothing parameter ('k' in the smooth() function). How much smoothing is \"too much\"?\n",
    "#    Note that this is a subjective judgment; the goal here is to gain some familiarity with smoothing filters.\n",
    "\n",
    "# Indeed it's quite subjective but higher k values produce smoother graphs but larger edge effects (wider filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvZy9phF7-IT"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Dropout wasn't too impressive here. Perhaps it would be more helpful with a different number of nodes in the hidden\n",
    "#    layer? Try running the experiment again using half as many hidden nodes, and twice as many nodes. Tip: take screenshots\n",
    "#    of each result to compare the three runs.\n",
    "\n",
    "# Nope, modifying the width of the network doesn't really change anything\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1VgQaiI9/JjuRfmJO13Ro",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
