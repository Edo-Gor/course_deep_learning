{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxLsu_5_Ao80"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 11.110\n",
    "#    Distribution of weights pre- and post-learning\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFOW2iaUArr4"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKF0ZjocsPPb"
   },
   "outputs": [],
   "source": [
    "# %% Reminder\n",
    "\n",
    "# The whole purpose of deep learning is to find weights minimising the loss\n",
    "# function (i.e., the difference between what the model predicts and what the\n",
    "# reality of the data is).\n",
    "# This is why it can be useful to analyse or visualise the distribution of the\n",
    "# weights before and after training/model fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNn5YMHLtfVf"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Split labels from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# Normalise data (original range is (0,255))\n",
    "data_norm = data / np.max(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64VpgZccuiyC"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Convert to tensor (float and integers)\n",
    "data_tensor   = torch.tensor(data_norm).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "# Split data with scikitlearn (10% test data)\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO5LXN3D87TB"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model(drop_rate):\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self,dropout_rate):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.fc1    = nn.Linear( 64,32)\n",
    "            self.fc2    = nn.Linear( 32,32)\n",
    "            self.output = nn.Linear( 32,10)\n",
    "\n",
    "            # Dropout\n",
    "            self.dr = dropout_rate\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.dropout(x,p=self.dr,training=self.training)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x,p=self.dr,training=self.training)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create model instance\n",
    "    ANN = mnist_FFN(drop_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (SGD to slow down learning for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1751660237179,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "9pTeucB0unnV",
    "outputId": "4c9c8fe1-563f-40d2-a921-ef2d56e5dd9c"
   },
   "outputs": [],
   "source": [
    "# %% Explore the model\n",
    "\n",
    "# Template\n",
    "drop_rate   = 0\n",
    "example_net = gen_model(drop_rate)[0]\n",
    "\n",
    "# Summary\n",
    "print('Model summary:')\n",
    "print(example_net)\n",
    "print()\n",
    "\n",
    "# Explore one layer\n",
    "print('Summary of input layer:')\n",
    "print(vars(example_net.input))\n",
    "print()\n",
    "\n",
    "# Explore a weight matrix\n",
    "print('Input layer weights:')\n",
    "print(example_net.input.weight.shape)\n",
    "print(example_net.input.weight)\n",
    "print()\n",
    "\n",
    "# Extract the weights and make an histogram\n",
    "w = example_net.input.weight.detach().flatten()\n",
    "plt.hist(w,40)\n",
    "plt.xlabel('Weight value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of initialized input layer weights')\n",
    "\n",
    "plt.savefig('figure29_weights_distribution_pre_post_learning.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure29_weights_distribution_pre_post_learning.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwqKo0H8xOot"
   },
   "outputs": [],
   "source": [
    "# %% Function to create an histogram of all weights across all layers\n",
    "\n",
    "def weights_histogram(network):\n",
    "\n",
    "    # Initialise weight vector\n",
    "    W = np.array([])\n",
    "\n",
    "    # Concatenate each set of weights\n",
    "    for layer in network.parameters():\n",
    "        W = np.concatenate( (W,layer.detach().flatten().numpy()) )\n",
    "\n",
    "    # Compute histogram (note: hard-coded range)\n",
    "    hist_y,hist_x = np.histogram(W,bins=np.linspace(-.3,.3,101),density=True)\n",
    "    hist_x = (hist_x[1:]+hist_x[:-1])/2\n",
    "\n",
    "    return hist_x,hist_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1751664305684,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "4-2BWo8-yboo",
    "outputId": "8113cc72-7ca5-4047-a0cb-dc41a4ec2999"
   },
   "outputs": [],
   "source": [
    "# %% Test the histogram function\n",
    "\n",
    "hist_x,hist_y = weights_histogram(example_net)\n",
    "plt.plot(hist_x,hist_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHWf6BRNtkg4"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    drop_rate  = 0.25\n",
    "    ANN,loss_fun,optimizer = gen_model(drop_rate)\n",
    "\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    hist_x = None\n",
    "    hist_y = np.zeros((num_epochs,100))\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Get the weights distribution\n",
    "        hx,hy = weights_histogram(ANN)\n",
    "        hist_y[epoch_i,:] = hy\n",
    "        if hist_x is None:\n",
    "            hist_x = hx\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "            test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN,hist_x,hist_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FEWUdEM-bTc"
   },
   "outputs": [],
   "source": [
    "# %% Run the training\n",
    "\n",
    "train_acc,test_acc,losses,ANN,hist_x,hist_y = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1751660369267,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "OWhWN4BX-iDa",
    "outputId": "f641e586-d087-4e36-ffd0-e61b7f7b602d"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].plot(losses)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "\n",
    "ax[1].plot(train_acc,label='Train accuracy')\n",
    "ax[1].plot(test_acc,label='Test accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure33_weights_distribution_pre_post_learning.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure33_weights_distribution_pre_post_learning.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1751660473229,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "Kqjw1NKBt3Du",
    "outputId": "3d601fd9-29ab-4f32-a2ee-796691d06ee2"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "num_epochs = hist_y.shape[0]\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.2,.9,hist_y.shape[0]))\n",
    "for i in range(hist_y.shape[0]):\n",
    "    ax[0].plot(hist_x,hist_y[i,:],color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Histograms of weights\\n(brigther is later in training)')\n",
    "ax[0].set_xlabel('Weight value')\n",
    "ax[0].set_ylabel('Density')\n",
    "\n",
    "ax[1].imshow(hist_y,vmin=0,vmax=3,extent=[hist_x[0],hist_x[-1],0,num_epochs-1],aspect='auto',origin='lower',cmap='hot')\n",
    "ax[1].set_xlabel('Weight value')\n",
    "ax[1].set_ylabel('Training epoch')\n",
    "ax[1].set_title(\"Image of weight histograms\\n(note the similarity with Sauron's eye)\")\n",
    "\n",
    "plt.savefig('figure34_weights_distribution_pre_post_learning.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure34_weights_distribution_pre_post_learning.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1751664682007,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "DZ0oBASGAH6m",
    "outputId": "45ca6e35-787f-40c7-8c51-a40290c98f49"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Separate the distributions for input, hidden, and output layers.\n",
    "#    Are the learning-related changes similar across all layers?\n",
    "\n",
    "# The distribution of the weights gets less and less uniform as one goes through\n",
    "# the layers, and that is already evident even in the initialised weights. This\n",
    "# trend seems however clearer for the input layer than for the later layers. The\n",
    "# dropout regularisation (0 or 0.25) doesn't seem to change much.\n",
    "\n",
    "# Modified model\n",
    "def train_model():\n",
    "    num_epochs = 60\n",
    "    drop_rate  = 0\n",
    "    ANN,loss_fun,optimizer = gen_model(drop_rate)\n",
    "\n",
    "    losses    = []\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Define histogram bins\n",
    "    bin_edges   = np.linspace(-0.7,0.7,101)\n",
    "    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n",
    "\n",
    "    # Track histograms for each layer\n",
    "    histograms_per_layer = {\n",
    "        \"Input layer (784→64)\":   np.zeros((num_epochs,len(bin_centers))),\n",
    "        \"Hidden layer 1 (64→32)\": np.zeros((num_epochs,len(bin_centers))),\n",
    "        \"Hidden layer 2 (32→32)\": np.zeros((num_epochs,len(bin_centers))),\n",
    "        \"Output layer (32→10)\":   np.zeros((num_epochs,len(bin_centers))),\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Capture per-layer histograms\n",
    "        for name,layer in zip(histograms_per_layer.keys(),[ANN.input,ANN.fc1,ANN.fc2,ANN.output]):\n",
    "            w       = layer.weight.detach().flatten().numpy()\n",
    "            hist, _ = np.histogram(w,bins=bin_edges,density=True)\n",
    "            histograms_per_layer[name][epoch,:] = hist\n",
    "\n",
    "        # Training loop\n",
    "        batch_loss = []\n",
    "        batch_acc  = []\n",
    "\n",
    "        for X, y in train_loader:\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss.append(loss.item())\n",
    "            accuracy = 100*torch.mean((torch.argmax(yHat,axis=1)==y).float())\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses.append(np.mean(batch_loss))\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "            test_acc.append(100*torch.mean((torch.argmax(yHat,axis=1)==y).float()))\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses,ANN,histograms_per_layer,bin_centers\n",
    "\n",
    "# Define a plotting function\n",
    "def plot_layer_weights(histograms_per_layer,bin_centers):\n",
    "\n",
    "    num_layers = len(histograms_per_layer)\n",
    "\n",
    "    phi = ( 1 + np.sqrt(5) ) / 2\n",
    "    fig,axs = plt.subplots(2,2,figsize=(8*phi,8))\n",
    "\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for ax,(layer_name, hist_matrix) in zip(axs,histograms_per_layer.items()):\n",
    "        num_epochs = hist_matrix.shape[0]\n",
    "        cmaps      = plt.cm.plasma(np.linspace(.2,.9,num_epochs))\n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            ax.plot(bin_centers,hist_matrix[i,:],color=cmaps[i],linewidth=1)\n",
    "\n",
    "        ax.set_title(layer_name)\n",
    "        ax.set_xlabel(\"Weight value\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # If fewer than 4 layers, hide unused subplot(s)\n",
    "    for ax in axs[num_layers:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Weight histograms per layer over epochs\\n(brighter is later in training)\",fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('figure35_weights_distribution_pre_post_learning_extra1.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    files.download('figure35_weights_distribution_pre_post_learning_extra1.png')\n",
    "\n",
    "# Train model and call plotting function\n",
    "train_acc,test_acc,losses,ANN,hists,bin_centers = train_model()\n",
    "plot_layer_weights(hists,bin_centers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQqU7_IsAeJ0"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Re-run the code without data normalization.\n",
    "#    Does the scale of the data affect the scale of the weights?\n",
    "\n",
    "# No, it doesn't seem to matter too much for these data, which are most likely\n",
    "# already distributed in a fairly normalised way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chqzZlP4AeAv"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    Test how dropout regularization affects the weight distributions.\n",
    "\n",
    "# Again, given that the model was already performing quite well, the impact of\n",
    "# a 0.25 dropout regularisation doesn't seem too strong\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMEldI0ugg1NWBZcdZyNG2j",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
