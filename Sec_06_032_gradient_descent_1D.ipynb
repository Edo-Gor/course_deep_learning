{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8kn-VXY5NLJ"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 6.32\n",
    "#    Gradient descent in 1D\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsCrkZ3J5iJ4"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import copy\n",
    "\n",
    "from google.colab                     import files\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIl51Jyx5oGr"
   },
   "outputs": [],
   "source": [
    "# %% Define function\n",
    "\n",
    "# The function\n",
    "def fx(x):\n",
    "    return 3*x**2 - 3*x + 4\n",
    "\n",
    "def df(x):\n",
    "    return 6*x -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1740601987724,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "86EQpTrE8Ob1",
    "outputId": "bce1210c-8b4d-4e4e-cb7e-7c51243833a7"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "x = np.linspace(-2,2,2001)\n",
    "\n",
    "plt.plot(x,fx(x),x,df(x))\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend([\"f(x)\",\"f'(x)\"])\n",
    "plt.title('A function and its derivative')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1740604537083,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "Gbs_nl3t9rzp",
    "outputId": "63ba417c-f984-49e1-f6c9-5d98625647fa"
   },
   "outputs": [],
   "source": [
    "# %% Algorithm for gradient descent\n",
    "\n",
    "# 1) Random starting point (out of vector x)\n",
    "local_min = np.random.choice(x,1).item()\n",
    "\n",
    "print(f'Random starting local mininum: {local_min:.8f}')\n",
    "\n",
    "# 2) Learning parameters\n",
    "learning_rate   = .01\n",
    "training_epochs = 100\n",
    "\n",
    "# 3) Loop over epochs\n",
    "for i in range(training_epochs):\n",
    "    gradient  = df(local_min)\n",
    "    local_min = local_min - gradient*learning_rate\n",
    "\n",
    "print(f'Estimated local mininum: {local_min:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1740604704433,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "X-FA3NDyBHZT",
    "outputId": "14f9c109-eedc-4c74-b361-b79d2e4fa73f"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "plt.plot(x,fx(x),x,df(x))\n",
    "plt.plot(local_min,df(local_min),'ro')\n",
    "plt.plot(local_min,fx(local_min),'ro')\n",
    "\n",
    "plt.xlim(x[[0,-1]])\n",
    "plt.grid()\n",
    "plt.xlabel('')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend([\"f(x)\",\"f'(x)\",\"f(x) min\"])\n",
    "plt.suptitle('A function and its derivative')\n",
    "plt.title('Empirical local minimum: %s' %np.round(local_min,4))\n",
    "\n",
    "plt.savefig('figure1_gradient_descent_1d.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure1_gradient_descent_1d.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0oDC5TADBU9"
   },
   "outputs": [],
   "source": [
    "# %% Store gradient descent interations for visualisation\n",
    "\n",
    "local_min = np.random.choice(x,1).item()\n",
    "\n",
    "learning_rate   = .01\n",
    "training_epochs = 100\n",
    "model_params    = np.zeros((training_epochs,2))\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    gradient          = df(local_min)\n",
    "    local_min         = local_min - gradient*learning_rate\n",
    "    model_params[i,:] = local_min,gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1740604745843,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "ogwQx4plDj1f",
    "outputId": "55b2e301-d3d7-4622-aed2-0913267c1cf9"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].plot(model_params[:,i],'o-')\n",
    "    ax[i].set_xlabel('Iteration')\n",
    "    ax[i].set_title(f'Estimated minimum on last iteration: {local_min:.4f}')\n",
    "\n",
    "ax[0].set_ylabel('Local minimum')\n",
    "ax[1].set_ylabel('Derivative')\n",
    "\n",
    "plt.savefig('figure2_gradient_descent_1d.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure2_gradient_descent_1d.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 664,
     "status": "ok",
     "timestamp": 1740605396471,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "ip9r-4RcINHw",
    "outputId": "cafb873b-f213-4e6b-de63-de36e3cb5028"
   },
   "outputs": [],
   "source": [
    "# %% Explore effects of learning rate and training epochs\n",
    "\n",
    "# Gradient descent\n",
    "local_min = np.random.choice(x,1).item()\n",
    "\n",
    "learning_rate   = .001\n",
    "training_epochs = 1000\n",
    "model_params    = np.zeros((training_epochs,2))\n",
    "\n",
    "for i in range(training_epochs):\n",
    "    gradient          = df(local_min)\n",
    "    local_min         = local_min - gradient*learning_rate\n",
    "    model_params[i,:] = local_min,gradient\n",
    "\n",
    "# Plotting\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].plot(model_params[:,i],'o-')\n",
    "    ax[i].set_xlabel('Iteration')\n",
    "    ax[i].set_title(f'Estimated minimum on last iteration: {local_min:.4f}')\n",
    "\n",
    "ax[0].set_ylabel('Local minimum')\n",
    "ax[1].set_ylabel('Derivative')\n",
    "plt.suptitle(f'Learning rate = {learning_rate} ; Epochs (N) = {training_epochs}')\n",
    "\n",
    "plt.savefig('figure4_gradient_descent_1d.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure4_gradient_descent_1d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1740607018042,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "DK2OE3aZLGdT",
    "outputId": "9623e44e-c336-42d0-cfb3-c6e9a6cc77db"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Most often in DL, the model trains for a set number of iterations, which is what we do here. But there are other ways\n",
    "#    of defining how long the training lasts. Modify the code so that training ends when the derivative is smaller than\n",
    "#    some threshold, e.g., 0.1. Make sure your code is robust for negative derivatives.\n",
    "\n",
    "local_min = np.random.choice(x,1).item()\n",
    "\n",
    "learning_rate  = .01\n",
    "grad_threshold = 1e-4\n",
    "epoch_counter  = 0\n",
    "\n",
    "while True:\n",
    "    gradient       = df(local_min)\n",
    "    local_min      = local_min - gradient*learning_rate\n",
    "    epoch_counter += 1\n",
    "\n",
    "    if abs(gradient) < grad_threshold:\n",
    "        break\n",
    "    elif epoch_counter > 1e5:\n",
    "        break\n",
    "\n",
    "print(f'Estimated local mininum: {local_min:.8f}')\n",
    "print(f'Required {epoch_counter} iterations, for precision of {grad_threshold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXlPtbKqL0FG"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Does this change to the code produce a more accurate result? What if you change the stopping threshold?\n",
    "\n",
    "# Modify script in Ex. 1 to explore\n",
    "# Maybe but not necessarily; using a threshold would require to know the scale of the data, and the procedure is still\n",
    "# dependent on the random starting point; if anything, it can force an arbitrary upper level of precision (e.g.,\n",
    "# in case you decide that a certain level of precision is enough and you prefer to potentially save up on the\n",
    "# number of iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F60cz2drL1BZ"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    Can you think of any potential problems that might arise when the stopping criterion is based on the derivative\n",
    "#    instead of a specified number of training epochs?\n",
    "\n",
    "# As mentioned above, it would require to know at least how the data are roughly distributed and what is their range,\n",
    "# otherwise the choice of a threshold might be fatal (but to be fair, this might also be a problem when chosing a fixed\n",
    "# number of iterations with a too small learning rate)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNuH5Ga2PEtsfJ95HSYdV9y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
