{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le5MsLuthBqv"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 8.65\n",
    "#    Cross-validation - Scikitlearn\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty5MFy8KhV0e"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0AxLpZyQPBY"
   },
   "outputs": [],
   "source": [
    "# %% A tanget from the code\n",
    "\n",
    "# The 80/20 split we have been using so far is not a golden rule, in fact, it's rather\n",
    "# arbitrary. Ideally you want to use as many data as possible in the training while\n",
    "# still having enough data for the devset and the test set; what is \"enough\" is up\n",
    "# to the speculation of the user, and it depends on the nature and size of the dataset.\n",
    "# For larger dataset, for instance, one can also do more extreme splits such as 98/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC2vZd-3Q71j"
   },
   "outputs": [],
   "source": [
    "# %% Import Iris dataset\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "# Convert from pandas df to tensor\n",
    "data = torch.tensor(iris[iris.columns[0:4]].values).float()\n",
    "\n",
    "# Species to numbers\n",
    "labels = torch.zeros(len(data),dtype=torch.long)\n",
    "labels[iris.species=='setosa']     = 0\n",
    "labels[iris.species=='versicolor'] = 1\n",
    "labels[iris.species=='virginica']  = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS5UCmouSfNJ"
   },
   "outputs": [],
   "source": [
    "# %% How to use train_test_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1743929308860,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "iyjIhvtCRDHV",
    "outputId": "a2e6481f-f63d-4b16-c48b-d9230c2e19aa"
   },
   "outputs": [],
   "source": [
    "# Fake dataset\n",
    "\n",
    "fake_data   = np.tile( np.array([1,2,3,4]),(10,1) ) + np.tile( 10*np.arange(1,11),(4,1) ).T\n",
    "fake_labels = np.arange(10)>4\n",
    "\n",
    "print(fake_data)\n",
    "print()\n",
    "print(fake_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1743933165567,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "e24G0NZgSiy8",
    "outputId": "fb986e4e-60c9-466d-e677-d0677a5cc2f1"
   },
   "outputs": [],
   "source": [
    "# %% Use scikitlearn to split data\n",
    "\n",
    "# Note that the 3rd parameter can be specified as \"test_size\" or \"train_size\", be mindful\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(fake_data,fake_labels,test_size=0.2)\n",
    "\n",
    "# Print sizes and data; notice how the labels are not randomised within the test.\n",
    "# If you don't want the shuffle, specify shuffle=False as option, but you would do that\n",
    "# only if the data are already randomised\n",
    "print(f'Training data size: {train_data.shape}')\n",
    "print(f'Test data size: {test_data.shape}\\n')\n",
    "\n",
    "print(f'Training data: \\n{train_data}')\n",
    "print(f'Training data: \\n{test_data}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2GiV4TrTln1"
   },
   "outputs": [],
   "source": [
    "# %% Now back to the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmvRaF_DVEfD"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    # Architecture\n",
    "    ANNiris = nn.Sequential(\n",
    "                 nn.Linear(4,64),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Linear(64,64),\n",
    "                 nn.ReLU(),\n",
    "                 nn.Linear(64,3)\n",
    "                 )\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(ANNiris.parameters(),lr=0.01)\n",
    "\n",
    "    return ANNiris,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXBZHAwbWKhG"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Global parameters\n",
    "num_epochs = 200\n",
    "\n",
    "# Function\n",
    "def train_model(train_proportion):\n",
    "\n",
    "    # Initialise losses\n",
    "    losses    = torch.zeros(num_epochs)\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "\n",
    "    # Split train and test data\n",
    "    # 1) Split before entering iterations (otherwise it's overfitting!)\n",
    "    # 2) Specify train_size\n",
    "    x_train,x_test,y_train,y_test = train_test_split(data,labels,train_size=train_proportion)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        yHat = ANNiris(x_train)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fun(yHat,y_train)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Taining accuracy\n",
    "        train_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y_train).float()).item() )\n",
    "\n",
    "        # Test accuracy (final pass on test data)\n",
    "        pred_labels = torch.argmax(ANNiris(x_test),axis=1)\n",
    "        test_acc.append( 100*torch.mean((pred_labels==y_test).float()).item() )\n",
    "\n",
    "    return train_acc,test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x30_motLa6iO"
   },
   "outputs": [],
   "source": [
    "# %% Test the model once\n",
    "\n",
    "ANNiris,loss_fun,optimizer = gen_model()\n",
    "train_acc,test_acc         = train_model(0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1743932025363,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "_kUPRjnSbv_v",
    "outputId": "f23cf34b-14dd-4fca-9b53-56d76808e433"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.plot(train_acc,'o-',alpha=.8)\n",
    "plt.plot(test_acc,'s-',alpha=.8)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.title('Train/test example')\n",
    "\n",
    "plt.savefig('figure3_cross_validation_scikitlearn.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure3_cross_validation_scikitlearn.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5oZuFicc5hV"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment on training data proportion\n",
    "\n",
    "# Parameters\n",
    "train_prop_vec = np.linspace(.2,.95,15)\n",
    "all_train_acc  = np.zeros((len(train_prop_vec),num_epochs))\n",
    "all_test_acc   = np.zeros((len(train_prop_vec),num_epochs))\n",
    "\n",
    "# Loop over training\n",
    "for i in range(len(train_prop_vec)):\n",
    "\n",
    "    # Generate model\n",
    "    ANNiris,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    # Train it\n",
    "    train_acc,test_acc = train_model(train_prop_vec[i])\n",
    "\n",
    "    # Store results\n",
    "    all_train_acc[i,:] = train_acc\n",
    "    all_test_acc[i,:]  = test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1743933626436,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "aafGHBTreGAG",
    "outputId": "aa855b1b-c4b0-4606-a439-02614951ddba"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "ax[0].imshow(all_train_acc,aspect='auto',cmap='jet',\n",
    "             vmin=50,vmax=90, extent=[0,num_epochs,train_prop_vec[-1],train_prop_vec[0]])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Training size proportion')\n",
    "ax[0].set_title('Training accuracy')\n",
    "\n",
    "p = ax[1].imshow(all_test_acc,aspect='auto',cmap='jet',\n",
    "                 vmin=50,vmax=90, extent=[0,num_epochs,train_prop_vec[-1],train_prop_vec[0]])\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Training size proportion')\n",
    "ax[1].set_title('Test accuracy')\n",
    "fig.colorbar(p,ax=ax[1])\n",
    "\n",
    "plt.savefig('figure4_cross_validation_scikitlearn.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure4_cross_validation_scikitlearn.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6zjxCqEfZPk"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    The images above suggest that the training proportion doesn't really affect learning success (for this data and this\n",
    "#    model). Does increasing the number of epochs to 1000 change the conclusion? How about with a lr=.001?\n",
    "\n",
    "# As one would expect, increasing the iterations allows the model to learn for longer, and the accuracy reaches a ceil;\n",
    "# decreasing the step size, on the other hand, does not allow the model to reach an optimal solution before the training\n",
    "# iterations are over (assuming they are set back to 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1743933584854,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "dOI8SgmYfcil",
    "outputId": "eb74d6d6-c916-4966-8d5a-590a64cdf3be"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    According to the help doc for train_test_split(), the train_size input can be either a float between 0.0 and 1.0, or\n",
    "#    an int. Here we only used float inputs to indicate the proportion of the data used for training. Modify the code to\n",
    "#    specify the training size as an integer corresponding to the number of samples.\n",
    "\n",
    "# In this case, this can be easily achieved by setting a vector of absolute sample number\n",
    "# instead of the proportion vector, and then running the parametrix experiment as usual:\n",
    "\n",
    "train_abs_sample = np.linspace(30,data.shape[0]-8,15,dtype=int)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPCUf1dlUGTAVlVL8d9SuEG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
