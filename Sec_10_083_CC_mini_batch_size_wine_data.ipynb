{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqafqL5r0Xyr"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 10.83\n",
    "#    Code challenge 8: minibatch size in the wine dataset\n",
    "\n",
    "#    1) Run a mini-batch size parametric experiment by setting the batch size to 2^n, n=1,3,5,7,9\n",
    "#    2) Use the binarised wine quality feture and all 11 features to predict it\n",
    "#    3) Optional: track time needed to train the model with each batch size, and bar-plot it\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knsqpvot0jAb"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HyD9itgCWuk4"
   },
   "outputs": [],
   "source": [
    "# %% Solution 1\n",
    "#    Use a sequential model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 509,
     "status": "ok",
     "timestamp": 1748096171749,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "PmF7OdN2Piq7",
    "outputId": "0f273b13-d1f0-4a8a-8acf-03cbd6b5c348"
   },
   "outputs": [],
   "source": [
    "# %% Load and prepare data\n",
    "\n",
    "# Load\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url,sep=';')\n",
    "\n",
    "# Remove some outliers (see lec. 82 for why)\n",
    "data = data[data['total sulfur dioxide']<200]\n",
    "\n",
    "# Z-score all the variables but quality\n",
    "cols2zscore = data.keys()\n",
    "cols2zscore = cols2zscore.drop('quality')\n",
    "\n",
    "for col in cols2zscore:\n",
    "    mean_val  = np.mean(data[col])\n",
    "    std_val   = np.std(data[col])\n",
    "    data[col] = (data[col] - mean_val) / std_val\n",
    "\n",
    "# Binarise quality\n",
    "data.loc[:,'boolean_quality'] = 0\n",
    "data.loc[data['quality']>5, 'boolean_quality'] = 1\n",
    "data.loc[data['quality']<6, 'boolean_quality'] = 0 # Implicit but here for clarity\n",
    "\n",
    "# Convert from pandas dataframe to PyTorch tensor\n",
    "data_t = torch.tensor( data[cols2zscore].values ).float()\n",
    "labels = torch.tensor( data['boolean_quality'].values ).float()\n",
    "\n",
    "print(f'Data shape: {data_t.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "# Labels need to be multidimentional for PyTorch, not an array, and need to be long integers too\n",
    "labels = labels[:,None]\n",
    "labels = labels.long()\n",
    "print(f'Proper labels shape: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghSP7JLoSnEA"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_t,labels,test_size=0.1)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "# > the train_loader is moved inside the train_model() function to allow a parametric test of the batch size\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df2s-SeXv19U"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    # Architecture\n",
    "    ANN = nn.Sequential(\n",
    "             nn.Linear(11,32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32,32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32,32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32,2))\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOG_NVNztQwX"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model(ANN,loss_fun,optimizer):\n",
    "\n",
    "    # Initialise accuracies\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y.squeeze())\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            preds = torch.argmax(yHat, axis=1)\n",
    "            batch_acc.append(100 * torch.mean((preds == y.squeeze()).float()).item())\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        X,y = next(iter(test_loader))\n",
    "        pred_labels = torch.argmax(ANN(X),axis=1)\n",
    "        test_acc.append(  100*torch.mean((pred_labels==y.squeeze()).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1135607,
     "status": "ok",
     "timestamp": 1748097764933,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "FCpUg2W8TAck",
    "outputId": "e8a16ef8-882d-4991-954e-4ef07e7c4ff7"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over mini-batches size (track time as well)\n",
    "\n",
    "# Takes about 18 mins for batch size 2^n, n=1,3,5,7,9\n",
    "arr            = np.arange(1,10)\n",
    "batch_size_exp = arr[arr % 2==1]\n",
    "#batch_size_exp = np.arange(9,10)\n",
    "\n",
    "train_acc    = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "test_acc     = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "losses       = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "elapsed_time = np.zeros((len(batch_size_exp),1))\n",
    "\n",
    "for i,exp_i in enumerate(batch_size_exp):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_size   = int(2**exp_i)\n",
    "        train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "        ANN,loss_fun,optimizer = gen_model()\n",
    "        train_acc[:,i],test_acc[:,i],losses[:,i] = train_model(ANN,loss_fun,optimizer)\n",
    "\n",
    "        elapsed_time[i] = time.time() - start_time\n",
    "        print(f\"Batch size 2^{exp_i} = {batch_size} completed in {elapsed_time[i, 0]:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsaxJNsWXLZx"
   },
   "outputs": [],
   "source": [
    "# %% Functions for 1D smoothing filter\n",
    "\n",
    "# Improved for edge effects - adaptive window\n",
    "def smooth_adaptive(x,k):\n",
    "    smoothed = np.zeros_like(x)\n",
    "    half_k   = k // 2\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        start       = max(0, i-half_k)\n",
    "        end         = min(len(x), i+half_k + 1)\n",
    "        smoothed[i] = np.mean(x[start:end])\n",
    "\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 776,
     "status": "ok",
     "timestamp": 1748098303131,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "i7yca2ngXMeJ",
    "outputId": "eee72e94-c840-4675-d130-e2df1e58f72c"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "# Interesting to see how the model does not go above a certain threshold; others\n",
    "# architecture might, but one must also consider that humans are variable, often\n",
    "# unpredictable, and inconsistent, thus if we develop models to predict human\n",
    "# behaviour, it is silly to expect the model to have really high accuracy (e.g.,\n",
    "# we migt like different wines, and we might like the same wine differently depending\n",
    "# on the context)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(batch_size_exp)))\n",
    "for i in range(len(batch_size_exp)):\n",
    "    ax[0].plot(smooth_adaptive(train_acc[:,i],20),color=cmaps[i])\n",
    "    ax[1].plot(smooth_adaptive(test_acc[:,i],20),color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# Make the legend easier to read\n",
    "leglabels = [2**int(i) for i in batch_size_exp]\n",
    "\n",
    "# Common features\n",
    "for i in range(2):\n",
    "    ax[i].legend(leglabels)\n",
    "    ax[i].set_xlabel('Epoch')\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].set_ylim([30,101])\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.savefig('figure8_code_challenge_8.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure8_code_challenge_8.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1748098327637,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "fWN29CwbamSa",
    "outputId": "ae924f99-0081-44b9-e628-de2975f14825"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "labels = [f\"{2**i}\" for i in batch_size_exp]\n",
    "plt.bar(labels,elapsed_time.squeeze())\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Training time per minibatch size')\n",
    "\n",
    "plt.savefig('figure9_code_challenge_8.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure9_code_challenge_8.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srGgxJ19W5FB"
   },
   "outputs": [],
   "source": [
    "# %% Solution 2\n",
    "#    Use a class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 507,
     "status": "ok",
     "timestamp": 1748091282826,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "uB-rMtcjW5YF",
    "outputId": "2a77bbdd-e72f-4a9f-8815-170e0cf3d383"
   },
   "outputs": [],
   "source": [
    "# %% Load and prepare data\n",
    "\n",
    "# Load\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = pd.read_csv(url,sep=';')\n",
    "\n",
    "# Remove some outliers (see lec. 82 for why)\n",
    "data = data[data['total sulfur dioxide']<200]\n",
    "\n",
    "# Z-score all the variables but quality\n",
    "cols2zscore = data.keys()\n",
    "cols2zscore = cols2zscore.drop('quality')\n",
    "\n",
    "for col in cols2zscore:\n",
    "    mean_val  = np.mean(data[col])\n",
    "    std_val   = np.std(data[col])\n",
    "    data[col] = (data[col] - mean_val) / std_val\n",
    "\n",
    "# Binarise quality\n",
    "data.loc[:,'boolean_quality'] = 0\n",
    "data.loc[data['quality']>5, 'boolean_quality'] = 1\n",
    "data.loc[data['quality']<6, 'boolean_quality'] = 0 # Implicit but here for clarity\n",
    "\n",
    "# Convert from pandas dataframe to PyTorch tensor\n",
    "data_t = torch.tensor( data[cols2zscore].values ).float()\n",
    "labels = torch.tensor( data['boolean_quality'].values ).float()\n",
    "\n",
    "print(f'Data shape: {data_t.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "# Labels need to be multidimentional for PyTorch, not an array, and need to be long integers too\n",
    "labels = labels[:,None]\n",
    "print(f'Proper labels shape: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRbJ74cbW5YG"
   },
   "outputs": [],
   "source": [
    "# %% Split into train and test data\n",
    "\n",
    "# Split with scikitlearn\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_t,labels,test_size=0.1)\n",
    "\n",
    "# Convert into PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "# > the train_loader is moved inside the train_model() function to allow a parametric test of the batch size\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7dZG1GYQm0n"
   },
   "outputs": [],
   "source": [
    "# %% Model class\n",
    "\n",
    "class model_class(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layers\n",
    "        self.input  = nn.Linear(11,32)\n",
    "        self.hid1   = nn.Linear(32,32)\n",
    "        self.hid2   = nn.Linear(32,32)\n",
    "        self.output = nn.Linear(32,1)\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.relu(self.input(x))\n",
    "        x = F.relu(self.hid1(x))\n",
    "        x = F.relu(self.hid2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMe8OYxxXE4c"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 1000\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_fun = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    # Initialise losses\n",
    "    train_acc = []\n",
    "    test_acc  = []\n",
    "    losses    = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Switch training mode on\n",
    "        ANN.train()\n",
    "\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Only now do backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Batch training accuracy\n",
    "            batch_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Average accuracy from batch\n",
    "        train_acc.append(np.mean(batch_acc))\n",
    "        losses.append(np.mean(batch_loss))\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "        X,y = next(iter(test_loader))\n",
    "        with torch.no_grad():\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean(((yHat>0) == y).float()).item() )\n",
    "\n",
    "    # Function output\n",
    "    return train_acc,test_acc,losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1173183,
     "status": "ok",
     "timestamp": 1748096017265,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "QxQXr3HVgP6y",
    "outputId": "b4274987-4dc7-4a5b-ddd9-ea902a9e8e3e"
   },
   "outputs": [],
   "source": [
    "# %% Parametric experiment over mini-batches size (track time as well)\n",
    "\n",
    "# Takes about 17 mins for batch size 2^n, n=1,3,5,7,9\n",
    "arr            = np.arange(1,10)\n",
    "batch_size_exp = arr[arr % 2==1]\n",
    "\n",
    "train_acc    = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "test_acc     = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "losses       = np.zeros((num_epochs,len(batch_size_exp)))\n",
    "elapsed_time = np.zeros((len(batch_size_exp),1))\n",
    "\n",
    "for i,exp_i in enumerate(batch_size_exp):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        batch_size   = int(2**exp_i)\n",
    "        train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "        ANN = model_class()\n",
    "        train_acc[:,i],test_acc[:,i],losses[:,i] = train_model()\n",
    "\n",
    "        elapsed_time[i] = time.time() - start_time\n",
    "        print(f\"Batch size 2^{exp_i} = {batch_size} completed in {elapsed_time[i, 0]:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1748096044317,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "fg9b-fjehQ6U",
    "outputId": "2cd09c70-d3f4-4706-e332-d3ce84a875f8"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(17,7))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(batch_size_exp)))\n",
    "for i in range(len(batch_size_exp)):\n",
    "    ax[0].plot(smooth_adaptive(train_acc[:,i],20),color=cmaps[i])\n",
    "    ax[1].plot(smooth_adaptive(test_acc[:,i],20),color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].set_title('Test accuracy')\n",
    "\n",
    "# Make the legend easier to read\n",
    "leglabels = [2**int(i) for i in batch_size_exp]\n",
    "\n",
    "# Common features\n",
    "for i in range(2):\n",
    "    ax[i].legend(leglabels)\n",
    "    ax[i].set_xlabel('Epoch')\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].set_ylim([41,101])\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.savefig('figure10_code_challenge_8.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure10_code_challenge_8.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1748096053197,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "_1GT1tZBkeBT",
    "outputId": "7fe2c0e2-916a-42dc-cd64-3d9f0df31066"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig = plt.figure(figsize=(6*phi,6))\n",
    "\n",
    "labels = [f\"{2**i}\" for i in batch_size_exp]\n",
    "plt.bar(labels,elapsed_time.squeeze())\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.title('Training time per minibatch size')\n",
    "\n",
    "plt.savefig('figure11_code_challenge_8.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure11_code_challenge_8.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OD-cbOfNyuEx"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    There is another regularization technique called \"early stopping,\" which simply means to stop training the model\n",
    "#    earlier than the number of epochs you specified. Early stopping is used when the test accuracy starts to decrease\n",
    "#    with increased training. Do you think that early stopping would be beneficial here? How many epochs would you train?\n",
    "\n",
    "# In thise case the test accuracy reaches a plateau after a while, which is why an early\n",
    "# stopping might not be an optimal choice; also, if implemented, there should be some tolerance\n",
    "# to avoid small random fluctuations from stopping the training (consider that the plot are smoothed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1748099107550,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "dhynfTHXy2ql",
    "outputId": "c7f27d5c-3e0d-46a7-ca46-7b0a720ac49a"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    The training loop computes the losses, but those aren't plotted. Create an additional subplot to show the losses\n",
    "#    in a similar fashion as the accuracy. Does that plot provide any additional insights into the effects of minibatch\n",
    "#    size, beyond what we can already learn from the accuracy plots.\n",
    "\n",
    "# As one would expect from the training accuracy, the losses are higher for models\n",
    "# with larger batches sizes; still interesting, however, that the test accuracy reaches\n",
    "# approximately the same level despite the differences in training accuracy and losses\n",
    "\n",
    "# Plotting\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,3,figsize=(7*phi*1.5,7))\n",
    "\n",
    "cmaps = plt.cm.plasma(np.linspace(.1,.9,len(batch_size_exp)))\n",
    "for i in range(len(batch_size_exp)):\n",
    "    ax[0].plot(smooth_adaptive(train_acc[:,i],20),color=cmaps[i])\n",
    "    ax[1].plot(smooth_adaptive(test_acc[:,i],20),color=cmaps[i])\n",
    "    ax[2].plot(smooth_adaptive(losses[:,i],20),color=cmaps[i])\n",
    "\n",
    "ax[0].set_title('Train accuracy')\n",
    "ax[1].set_title('Test accuracy')\n",
    "ax[2].set_title('Losses')\n",
    "\n",
    "ax[0].set_ylim([41,101])\n",
    "ax[1].set_ylim([41,101])\n",
    "\n",
    "# Make the legend easier to read\n",
    "leglabels = [2**int(i) for i in batch_size_exp]\n",
    "\n",
    "# Common features\n",
    "for i in range(3):\n",
    "    ax[i].legend(leglabels)\n",
    "    ax[i].set_xlabel('Epoch')\n",
    "    ax[i].set_ylabel('Accuracy (%)')\n",
    "    ax[i].grid()\n",
    "\n",
    "plt.savefig('figure12_code_challenge_8_extra2.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure12_code_challenge_8_extra2.png')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNEJ/b4IDu9HNhi7v2htpCR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
