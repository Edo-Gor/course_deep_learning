{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrDdSPhwsuh2"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 7.53\n",
    "#    Code challenge 5: more qwerties\n",
    "\n",
    "#    1) Integrate code from binary and multioutput classification\n",
    "#    2) Make 3 groups of data and classifiy them with an ANN\n",
    "#    3) Try with a 2-4-3 architecture\n",
    "#    4) Plot losses and accuracy over epochs, and classification probabilities\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oY-2Pgi5trTX"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import copy\n",
    "\n",
    "from google.colab                     import files\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2megqQSt2Yk"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "nClust = 100\n",
    "blur   = 1\n",
    "\n",
    "A = [1, 1]\n",
    "B = [5, 1]\n",
    "C = [3,-2]\n",
    "\n",
    "a = [ A[0]+np.random.randn(nClust)*blur, A[1]+np.random.randn(nClust)*blur ]\n",
    "b = [ B[0]+np.random.randn(nClust)*blur, B[1]+np.random.randn(nClust)*blur ]\n",
    "c = [ C[0]+np.random.randn(nClust)*blur, C[1]+np.random.randn(nClust)*blur ]\n",
    "\n",
    "# True labels\n",
    "labels_np = np.vstack(( np.zeros((nClust,1)), np.ones((nClust,1)), 2*np.ones((nClust,1)) ))\n",
    "\n",
    "# Concatenate\n",
    "data_np = np.hstack((a,b,c)).T\n",
    "\n",
    "# Convert into torch tensor\n",
    "data   = torch.tensor(data_np).float()\n",
    "labels = (torch.tensor(labels_np,dtype=torch.long))\n",
    "labels = labels.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1742416749804,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "OdmGzeB7t_Ie",
    "outputId": "9bf4caa7-9113-46ae-b133-8b7ff410c1cc"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "plt.plot( data[np.where(labels==0)[0],0],data[np.where(labels==0)[0],1],'s' )\n",
    "plt.plot( data[np.where(labels==1)[0],0],data[np.where(labels==1)[0],1],'s' )\n",
    "plt.plot( data[np.where(labels==2)[0],0],data[np.where(labels==2)[0],1],'s' )\n",
    "plt.title('Some data')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "\n",
    "plt.savefig('figure67_code_challenge_5.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure67_code_challenge_5.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuHWI_Pzzbgs"
   },
   "outputs": [],
   "source": [
    "# %% Build the model\n",
    "\n",
    "# Architecture\n",
    "ANNclassify = nn.Sequential(\n",
    "                 nn.Linear(2,4),    # input layer\n",
    "                 nn.ReLU(),         # a.f.\n",
    "                 nn.Linear(4,3),    # output layer\n",
    "                 nn.Softmax(dim=1), # final activation unit\n",
    "                 )\n",
    "\n",
    "# Loss function (includes [Log]Softmax)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(ANNclassify.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8373,
     "status": "ok",
     "timestamp": 1742417084723,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "zkgN-PzV0fnR",
    "outputId": "fd06842b-f208-4ecc-d299-8e65d3997356"
   },
   "outputs": [],
   "source": [
    "# %% Train the model\n",
    "\n",
    "num_epochs  = 10000\n",
    "losses      = torch.zeros(num_epochs)\n",
    "ongoing_acc = []\n",
    "\n",
    "for epoch_i in range(num_epochs):\n",
    "\n",
    "    # Forward propagation\n",
    "    yHat = ANNclassify(data)\n",
    "\n",
    "    # Loss\n",
    "    loss = loss_fun(yHat,labels)\n",
    "    losses[epoch_i] = loss\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accuracy over iteration (argmax takes the index with the highest val in yHat rows)\n",
    "    matches     = torch.argmax(yHat,axis=1) == labels  # booleans\n",
    "    matches_num = matches.float()                      # booleans2numeric\n",
    "    acc_perc    = 100*torch.mean(matches_num)          # percent\n",
    "    ongoing_acc.append(acc_perc)                       # append to list\n",
    "\n",
    "# Final forward pass\n",
    "predictions = ANNclassify(data)\n",
    "\n",
    "pred_labels = torch.argmax(predictions,axis=1)\n",
    "tot_acc     = 100*torch.mean((pred_labels == labels).float())\n",
    "\n",
    "print(f'Final accuracy = {tot_acc.item():.4f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1742417113141,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "MCXWiMyu4HLf",
    "outputId": "8893d595-9f0a-469f-ad55-2980e58b95e8"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax[0].plot(losses.detach())\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_title('Losses over epochs')\n",
    "\n",
    "ax[1].plot(ongoing_acc)\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_title('Accuracy over epochs')\n",
    "\n",
    "plt.savefig('figure68_code_challenge_5.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure68_code_challenge_5.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1742417873755,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -60
    },
    "id": "FzNc66b-8Dgt",
    "outputId": "c64ff051-994a-49d9-969e-ef8e1bdf7dd9"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "sm = nn.Softmax(1)\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(sm(yHat.detach()),'s',markerfacecolor='none')\n",
    "plt.xlabel('Stimulus number')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend(['Group 1','Group 2','Group 3'],framealpha=0.75)\n",
    "plt.title('Classification probabilities')\n",
    "\n",
    "plt.savefig('figure69_code_challenge_5.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure69_code_challenge_5.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCNDKPue-F2Y"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    Does the model always do well? Re-run the entire notebook multiple times and see if it always reaches high accuracy\n",
    "#    (e.g., >90%). What do you think would be ways to improve the performance stability of the model?\n",
    "\n",
    "# Not super sure about this point, the model I set up tends to run reliably between 92-94%, and 10000 epochs (as I\n",
    "# noticed from the graphs at the beginning), seem way too many (e.g., see accuracy evolution plot). However,\n",
    "# I then realised that at first I run the model without the additional explicit Softmax pass in the end, see next\n",
    "# exploration for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQC2fzb0-MGz"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    You'll learn in the section \"Metaparameters\" that CrossEntropyLoss computes log-softmax internally. Does that mean\n",
    "#    that the Softmax() layer in the model needs to be there? Does it hurt or help? If you remove that final layer, what\n",
    "#    would change and what would be the same in the rest of the notebook?\n",
    "#    (Note about this problem: If it feels too advanced, then revisit this problem after the \"Metaparameters\" section.)\n",
    "\n",
    "# As mentioned above, running the model without the extra Softmax actually makes the learning a bit faster; adding the extra\n",
    "# explicit activation function makes the accuracy to increase more slowly. Also, the loss does not decrease as much as in\n",
    "# the above example. I'm not sure why though...maybe in the latter  case we are actually applying the softmax twice to the\n",
    "# output layer?\n",
    "# As for the rest of the code, the only modification needed without the extra softamx is that you need to apply it\n",
    "# when plotting the classification probabilities.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO8+hMPpdbLH2yJR0HZvbWB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
