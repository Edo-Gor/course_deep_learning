{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYQ479r7ObtJ"
   },
   "outputs": [],
   "source": [
    "# %% Deep learning - Section 11.114\n",
    "#    Shifted MNIST\n",
    "\n",
    "# This code pertains a deep learning course provided by Mike X. Cohen on Udemy:\n",
    "#   > https://www.udemy.com/course/deeplearning_x\n",
    "# The \"base\" code in this repository is adapted (with very minor modifications)\n",
    "# from code developed by the course instructor (Mike X. Cohen), while the\n",
    "# \"exercises\" and the \"code challenges\" contain more original solutions and\n",
    "# creative input from my side. If you are interested in DL (and if you are\n",
    "# reading this statement, chances are that you are), go check out the course, it\n",
    "# is singularly good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BjjgNkyPG4Q"
   },
   "outputs": [],
   "source": [
    "# %% Libraries and modules\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import seaborn             as sns\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas              as pd\n",
    "import scipy.stats         as stats\n",
    "import time\n",
    "\n",
    "from torch.utils.data                 import DataLoader,TensorDataset\n",
    "from sklearn.model_selection          import train_test_split\n",
    "from google.colab                     import files\n",
    "from torchsummary                     import summary\n",
    "from IPython                          import display\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDos2O8XPNbu"
   },
   "outputs": [],
   "source": [
    "# %% Data\n",
    "\n",
    "# Load data\n",
    "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
    "\n",
    "# Split labels from data\n",
    "labels = data[:,0]\n",
    "data   = data[:,1:]\n",
    "\n",
    "# Normalise data (original range is (0,255))\n",
    "data_norm = data / np.max(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkJo6fB9PRf3"
   },
   "outputs": [],
   "source": [
    "# %% A note\n",
    "\n",
    "# We know from the previous video that shifting an image (or scrambling it)\n",
    "# will not affect the classification, if we do it consistently and for both\n",
    "# train and test data.\n",
    "# Here we will leave the train data as they are, and only shift ('roll') the\n",
    "# test data by a few pixels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFPjVhWAQpr8"
   },
   "outputs": [],
   "source": [
    "# %% Create train and test datasets\n",
    "\n",
    "# Convert to tensor (float and integers) the scrambled data\n",
    "data_tensor   = torch.tensor(data_norm).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "# Split data with scikitlearn (10% test data)\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "# Convert into DataLoader objects\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1751918048064,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "hlmm_V4WQuez",
    "outputId": "d16e0235-f662-4d97-8886-f9b6554b0e57"
   },
   "outputs": [],
   "source": [
    "# %% How to shift a vectorized image\n",
    "\n",
    "# Grab one image and reshape to 2D\n",
    "tmp = test_loader.dataset.tensors[0][0,:]\n",
    "tmp = tmp.reshape(28,28)\n",
    "\n",
    "# Shift the image (pytorch calls it \"rolling\"), dim=0 for vertical shift\n",
    "tmpS = torch.roll(tmp,8,dims=1)\n",
    "\n",
    "# Plot\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].imshow(tmp, cmap='gray')\n",
    "ax[0].set_title('Original')\n",
    "\n",
    "ax[1].imshow(tmpS, cmap='gray')\n",
    "ax[1].set_title('Shifted (rolled)')\n",
    "\n",
    "plt.savefig('figure60_shifted_mnist.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure60_shifted_mnist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHMT-V0gQubL"
   },
   "outputs": [],
   "source": [
    "# %% Now shift all images in the test set\n",
    "# Note: run the previous cell again to confirm the shifting\n",
    "\n",
    "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
    "\n",
    "  # Get the image\n",
    "  img = test_loader.dataset.tensors[0][i,:]\n",
    "\n",
    "  # Reshape and roll by max. 10 pixels, dim=0 for vertical shift\n",
    "  randroll = np.random.randint(-10,11)\n",
    "  img      = torch.roll( img.reshape(28,28),randroll,dims=1 )\n",
    "\n",
    "  # Re-vectorize and put back into the matrix\n",
    "  test_loader.dataset.tensors[0][i,:] = img.reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goF1yTSeGp08"
   },
   "outputs": [],
   "source": [
    "# %% Function to generate the model\n",
    "\n",
    "def gen_model():\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.fc1    = nn.Linear( 64,32)\n",
    "            self.fc2    = nn.Linear( 32,32)\n",
    "            self.output = nn.Linear( 32,10)\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create model instance\n",
    "    ANN = mnist_FFN()\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer (SGD to slow down learning for illustration purpose)\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L1y1mi6JN5_"
   },
   "outputs": [],
   "source": [
    "# %% Function to train the model\n",
    "\n",
    "def train_model():\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model()\n",
    "\n",
    "    losses_trn = []\n",
    "    losses_tst = []\n",
    "    train_acc  = []\n",
    "    test_acc   = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses_trn.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "        loss = loss_fun(yHat,y)\n",
    "        losses_tst.append(loss.item())\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses_trn,losses_tst,ANN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBhga_LjJbv8"
   },
   "outputs": [],
   "source": [
    "# %% Fit the model\n",
    "\n",
    "train_acc,test_acc,losses_trn,losses_tst,ANN = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1751918187013,
     "user": {
      "displayName": "Edoardo Gornetti",
      "userId": "05048424707797899325"
     },
     "user_tz": -120
    },
    "id": "bXCLxM_wJa8G",
    "outputId": "428140ea-3d0b-40c7-a9f2-8f7c4aaa496b"
   },
   "outputs": [],
   "source": [
    "# %% Plotting\n",
    "\n",
    "phi = ( 1 + np.sqrt(5) ) / 2\n",
    "fig,ax = plt.subplots(1,2,figsize=(1.5*6*phi,6))\n",
    "\n",
    "ax[0].plot(losses_trn,label='Train loss')\n",
    "ax[0].plot(losses_tst,label='Test loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_ylim([0,3])\n",
    "ax[0].set_title('Model loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(train_acc,label='Train accuracy')\n",
    "ax[1].plot(test_acc,label='Test accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy (%)')\n",
    "ax[1].set_ylim([10,100])\n",
    "ax[1].set_title(f'Final model test accuracy: {test_acc[-1]:.2f}%\\nNormalised train and test data')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figure62_shifted_mnist.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "files.download('figure62_shifted_mnist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seV-Nh7dQt9o"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 1\n",
    "#    You saw that shifting by max-10 pixels has negative effects on model generalization. What do you think will happen\n",
    "#    if you shift the images by 4 pixels? 3? 2? 1? Try it and find out!\n",
    "\n",
    "# The hypothesis is that if the logics hold, then no matter the amount of\n",
    "# shifting, the performance should decrease; however, it's true that the digits\n",
    "# have a certain \"thikness\", and that for example shifting by 1 or 2 pixels\n",
    "# might not compromise the performace as much as a shifting that is larger that\n",
    "# that the average width of a digit (what I mean is that if you overlap an\n",
    "# original image to an image shifted by one pixel, the ovelap will still look\n",
    "# a lot like to the original image).\n",
    "#\n",
    "# And OMG, the prediction was accurate ! Almost can't belive it.\n",
    "\n",
    "# Reset loaders\n",
    "data_tensor   = torch.tensor(data_norm).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "# Shift all images in the test set by a fixed amount\n",
    "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
    "\n",
    "  img = test_loader.dataset.tensors[0][i,:]\n",
    "  n   = 1\n",
    "  img = torch.roll( img.reshape(28,28),n,dims=1 )\n",
    "  test_loader.dataset.tensors[0][i,:] = img.reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYBn3dMZQuU7"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 2\n",
    "#    Dropout regularization helps make representations more distributed. Can that help with the decrease in learning?\n",
    "#    Try this by testing the model on 2-pixel shifted images with and without 50% dropout.\n",
    "\n",
    "# Surprisingly and yet not surprisingly, dropout doesn't help. The training\n",
    "# accuracy goes down but that expected, the test accuracy however, stays the\n",
    "# same. This is kind of expected because only the test data are shifted\n",
    "\n",
    "# Reset loaders\n",
    "data_tensor   = torch.tensor(data_norm).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "# Shift all images in the test set by a fixed amount\n",
    "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
    "\n",
    "  img = test_loader.dataset.tensors[0][i,:]\n",
    "  n   = 2\n",
    "  img = torch.roll( img.reshape(28,28),n,dims=1 )\n",
    "  test_loader.dataset.tensors[0][i,:] = img.reshape(1,-1)\n",
    "\n",
    "# Function to generate the model\n",
    "def gen_model(drop_rate):\n",
    "\n",
    "    class mnist_FFN(nn.Module):\n",
    "        def __init__(self,dropout_rate):\n",
    "            super().__init__()\n",
    "\n",
    "            # Architecture\n",
    "            self.input  = nn.Linear(784,64)\n",
    "            self.fc1    = nn.Linear( 64,32)\n",
    "            self.fc2    = nn.Linear( 32,32)\n",
    "            self.output = nn.Linear( 32,10)\n",
    "\n",
    "            # Dropout\n",
    "            self.dr = dropout_rate\n",
    "\n",
    "        # Forward propagation\n",
    "        def forward(self,x):\n",
    "\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.dropout(x,p=self.dr,training=self.training)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.dropout(x,p=self.dr,training=self.training)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.dropout(x,p=self.dr,training=self.training)\n",
    "\n",
    "            x = self.output(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create model instance\n",
    "    ANN = mnist_FFN(drop_rate)\n",
    "\n",
    "    # Loss function\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(ANN.parameters(),lr=0.01)\n",
    "\n",
    "    return ANN,loss_fun,optimizer\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(dropout_rate):\n",
    "\n",
    "    # Parameters, model instance, inizialise vars\n",
    "    num_epochs = 60\n",
    "    ANN,loss_fun,optimizer = gen_model(dropout_rate)\n",
    "\n",
    "    losses_trn = []\n",
    "    losses_tst = []\n",
    "    train_acc  = []\n",
    "    test_acc   = []\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch_i in range(num_epochs):\n",
    "\n",
    "        # Loop over training batches\n",
    "        batch_acc  = []\n",
    "        batch_loss = []\n",
    "\n",
    "        for X,y in train_loader:\n",
    "\n",
    "            # Forward propagation and loss\n",
    "            yHat = ANN(X)\n",
    "            loss = loss_fun(yHat,y)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Loss and accuracy from this batch\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            matches     = torch.argmax(yHat,axis=1) == y\n",
    "            matches_num = matches.float()\n",
    "            accuracy    = 100 * torch.mean(matches_num)\n",
    "            batch_acc.append(accuracy)\n",
    "\n",
    "        losses_trn.append( np.mean(batch_loss) )\n",
    "        train_acc.append( np.mean(batch_acc) )\n",
    "\n",
    "        # Test accuracy\n",
    "        ANN.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            X,y = next(iter(test_loader))\n",
    "            yHat = ANN(X)\n",
    "        test_acc.append( 100*torch.mean((torch.argmax(yHat,axis=1)==y).float()) )\n",
    "        loss = loss_fun(yHat,y)\n",
    "        losses_tst.append(loss.item())\n",
    "\n",
    "        ANN.train()\n",
    "\n",
    "    return train_acc,test_acc,losses_trn,losses_tst,ANN\n",
    "\n",
    "# Fit the model\n",
    "drop_rate = 0\n",
    "train_acc,test_acc,losses_trn,losses_tst,ANN = train_model(drop_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ObtvOmWTRmH"
   },
   "outputs": [],
   "source": [
    "# %% Exercise 3\n",
    "#    Continue exploring the torch.roll() function. Make sure you know how to shift up and down, not just left/right.\n",
    "\n",
    "# For that it's just enough to change the dims parameter to 0. And just for\n",
    "# reference, the results are the same.\n",
    "\n",
    "# Reset loaders\n",
    "data_tensor   = torch.tensor(data_norm).float()\n",
    "labels_tensor = torch.tensor(labels).long()\n",
    "\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(data_tensor,labels_tensor,test_size=0.1)\n",
    "\n",
    "train_data = TensorDataset(train_data,train_labels)\n",
    "test_data  = TensorDataset(test_data,test_labels)\n",
    "\n",
    "batch_size   = 32\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "# Shift all images in the test set, dim=0 for vertical shift\n",
    "for i in range(test_loader.dataset.tensors[0].shape[0]):\n",
    "\n",
    "  img = test_loader.dataset.tensors[0][i,:]\n",
    "  n   = np.random.randint(-10,11)\n",
    "  img = torch.roll( img.reshape(28,28),n,dims=0 )\n",
    "  test_loader.dataset.tensors[0][i,:] = img.reshape(1,-1)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3R95oJoVWVHvPV15B2sBT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
